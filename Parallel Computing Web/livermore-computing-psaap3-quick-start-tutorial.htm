<!DOCTYPE html>
<html lang="en" dir="ltr"
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:dc="http://purl.org/dc/terms/"
  xmlns:foaf="http://xmlns.com/foaf/0.1/"
  xmlns:og="http://ogp.me/ns#"
  xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
  xmlns:sioc="http://rdfs.org/sioc/ns#"
  xmlns:sioct="http://rdfs.org/sioc/types#"
  xmlns:skos="http://www.w3.org/2004/02/skos/core#"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#">
<head>
<meta charset="utf-8" http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="Generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="/training/tutorials/livermore-computing-psaap3-quick-start-tutorial" />
<link rel="shortlink" href="/node/800" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
<link rel="shortcut icon" href="https://hpc.llnl.gov/sites/all/themes/tid/favicon.ico" type="image/vnd.microsoft.icon" />
<title>Livermore Computing PSAAP3 Quick Start Tutorial | High Performance Computing</title>
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_kShW4RPmRstZ3SpIC-ZvVGNFVAi0WEMuCnI0ZkYIaFw.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_bq48Es_JAifg3RQWKsTF9oq1S79uSN2WHxC3KV06fK0.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_vAm-LJc0tkC-w_c6v7Ekq0bW26Pzl31HvPM6kbvK-pc.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_ca6tstDbY9-H23Ty8uKiDyFQLT1AZftZKldhbTPPnm8.css" media="all" />
<!--[if lt IE 9]><script src="/sites/all/themes/tid/js/html5.js"></script><![endif]-->
</head>
<body class="html not-front not-logged-in no-sidebars page-node page-node- page-node-800 node-type-user-portal-one-column-page">
  <div aria="contentinfo"><noscript><img src="https://analytics.llnl.gov/piwik.php?idsite=149" class="no-border" alt="" /></noscript></div>
    <div id="page">
	<div class="unclassified"></div>
	<div class="headertop">
					<div id="skip-nav" role="navigation" aria-labelledby="skip-nav" class="reveal">
  			<a href="#main-content">Skip to main content</a>
			</div>
					</div>
        <div class="headerwrapbg">
                        <div class="headerwrap-portal">
                <div id="masthead" class="site-header container" role="banner">
                    <div class="row">
                        <div class="llnl-logo col-sm-3">
                            <a href="https://www.llnl.gov" target="_blank" title="Lawrence Livermore National Laboratory">
                                <img src="/sites/all/themes/tid/images/llnl-tab-portal.png" alt="LLNL Home" />
                            </a>
                        </div>
                        <div id="logo" class="site-branding col-sm-4">
                                                            <div id="site-logo">
                                        <!--High Performance Computing<br />Livermore Computing Center-->
                                        																					<a href="/user-portal" class="text-dark" title="Livermore Computing Center High Performance Computing">
                                            <img src="/sites/all/themes/tid/images/hpc.png" alt="Portal Home" />
																					</a>
																				
                                </div>
                                                    </div>
                        <div class="col-sm-5">
                            <div id="top-search">
															<div class="input-group">
																	<form class="navbar-form navbar-search navbar-right" action="/training/tutorials/livermore-computing-psaap3-quick-start-tutorial" method="post" id="search-block-form" accept-charset="UTF-8"><div><div class="container-inline">
      <div class="element-invisible">Search form</div>
    <div class="form-item form-type-textfield form-item-search-block-form">
  <label class="element-invisible" for="edit-search-block-form--2">Search </label>
 <input title="Enter the terms you wish to search for." type="text" id="edit-search-block-form--2" name="search_block_form" value="" size="15" maxlength="128" class="form-text" />
</div>
<div class="form-actions form-wrapper" id="edit-actions"><input type="submit" id="edit-submit" name="op" value="" class="form-submit" /></div><input type="hidden" name="form_build_id" value="form-Px7n9UGtur-61YU8FCIaVMA0YoNWwakdGvZ7fClTPUA" />
<input type="hidden" name="form_id" value="search_block_form" />
</div>
</div></form>                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div id="mainnav">
                    <div class="container">
                        <div class="row">
                            <nav id="Menu" aria-label="Mobile Menu" class="mobilenavi col-md-12"></nav>
                            <nav id="navigation" aria-label="Main Menu">
                                <div id="main-menu" class="main-menu-portal">
                                    <ul class="menu"><li class="first collapsed"><a href="/user-portal">Portal</a></li>
<li class="expanded"><a href="/accounts">Accounts</a><ul class="menu"><li class="first leaf"><a href="/accounts/new-account-setup">New Account Setup</a></li>
<li class="leaf"><a href="/accounts/idm-account-management">IdM Account Management</a></li>
<li class="leaf"><a href="https://hpc.llnl.gov/manuals/access-lc-systems" title="">Access to LC Systems</a></li>
<li class="leaf"><a href="/accounts/computer-coordinator-roles">Computer Coordinator Roles</a></li>
<li class="collapsed"><a href="/accounts/forms">Forms</a></li>
<li class="collapsed"><a href="/accounts/policies">Policies</a></li>
<li class="last leaf"><a href="/accounts/mailing-lists">Mailing Lists</a></li>
</ul></li>
<li class="expanded"><a href="/banks-jobs">Banks &amp; Jobs</a><ul class="menu"><li class="first leaf"><a href="/banks-jobs/allocations">Allocations</a></li>
<li class="expanded"><a href="/banks-jobs/running-jobs">Running Jobs</a><ul class="menu"><li class="first leaf"><a href="/banks-jobs/running-jobs/batch-system-primer">Batch System Primer</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/lsf-user-manual">LSF User Manual</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/lsf-quick-start-guide">LSF Quick Start Guide</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/lsf-commands">LSF Commands</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/slurm-user-manual" title="Guide to using the Slurm Workload/Resource Manager">Slurm User Manual</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/slurm-quick-start-guide">Slurm Quick Start Guide</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/slurm-commands">Slurm Commands</a></li>
<li class="leaf"><a href="/training/tutorials/slurm-and-moab">Slurm and Moab</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/batch-system-commands">Batch System Cross-Reference</a></li>
<li class="last leaf"><a href="/banks-jobs/running-jobs/slurm-srun-versus-ibm-csm-jsrun">Slurm srun versus IBM CSM jsrun</a></li>
</ul></li>
<li class="leaf"><a href="https://hpc.llnl.gov/accounts/forms/asc-dat" title="">ASC DAT Request</a></li>
<li class="last leaf"><a href="https://hpc.llnl.gov/accounts/forms/mic-dat" title="">M&amp;IC DAT Request</a></li>
</ul></li>
<li class="expanded"><a href="/hardware">Hardware</a><ul class="menu"><li class="first collapsed"><a href="/hardware/archival-storage-hardware">Archival Storage Hardware</a></li>
<li class="collapsed"><a href="/hardware/platforms">Compute Platforms</a></li>
<li class="leaf"><a href="/hardware/compute-platforms-gpus">Compute Platforms with GPUs</a></li>
<li class="collapsed"><a href="/hardware/file-systems">File Systems</a></li>
<li class="leaf"><a href="/hardware/testbeds">Testbeds</a></li>
<li class="collapsed"><a href="/hardware/zones">Zones (aka &quot;The Enclave&quot;)</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/lorenz/mylc/mylc.cgi" title="">MyLC (Lorenz)</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi?" title="">CZ Compute Platform Status</a></li>
<li class="leaf"><a href="https://rzlc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" title="">RZ Compute System Status</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/fsstatus/fsstatus.cgi" title="">CZ File System Status</a></li>
<li class="last leaf"><a href="https://rzlc.llnl.gov/fsstatus/fsstatus.cgi" title="">RZ File System Status</a></li>
</ul></li>
<li class="expanded"><a href="/services">Services</a><ul class="menu"><li class="first collapsed"><a href="/services/green-data-oasis">Green Data Oasis (GDO)</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/lorenz/mylc/mylc.cgi" title="">MyLC (Lorenz)</a></li>
<li class="last leaf"><a href="/services/visualization-services">Visualization Services</a></li>
</ul></li>
<li class="expanded"><a href="/software">Software</a><ul class="menu"><li class="first leaf"><a href="/software/archival-storage-software">Archival Storage Software</a></li>
<li class="collapsed"><a href="/software/data-management-tools-projects">Data Management Tools</a></li>
<li class="collapsed"><a href="/software/development-environment-software">Development Environment Software</a></li>
<li class="leaf"><a href="/software/mathematical-software">Mathematical Software</a></li>
<li class="leaf"><a href="/software/modules-and-software-packaging">Modules and Software Packaging</a></li>
<li class="collapsed"><a href="/software/visualization-software">Visualization Software</a></li>
<li class="last leaf"><a href="https://computing.llnl.gov/projects/radiuss" title="">RADIUSS</a></li>
</ul></li>
<li class="last expanded active-trail"><a href="/training" class="active-trail">Training</a><ul class="menu"><li class="first expanded active-trail"><a href="/training/tutorials" class="active-trail">Tutorials</a><ul class="menu"><li class="first leaf"><a href="/training/tutorials/introduction-parallel-computing-tutorial">Introduction to Parallel Computing Tutorial</a></li>
<li class="leaf"><a href="/training/tutorials/llnl-covid-19-hpc-resource-guide">LLNL Covid-19 HPC Resource Guide for New Livermore Computing Users</a></li>
<li class="leaf"><a href="/training/tutorials/using-lcs-sierra-system">Using LC&#039;s Sierra System</a></li>
<li class="leaf active-trail"><a href="/training/tutorials/livermore-computing-psaap3-quick-start-tutorial" class="active-trail active">Livermore Computing PSAAP3 Quick Start Tutorial</a></li>
<li class="leaf"><a href="https://hpc.llnl.gov/sites/default/files/PSAAP-alliance-quickguide.docx" title="">PSAAP Alliance Quick Guide</a></li>
<li class="leaf"><a href="/training/tutorials/linux-tutorial-exercises">Linux Tutorial Exercise One</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-linux-commodity-clusters-overview-part-one">Livermore Computing Linux Clusters Overview Part One</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-linux-commodity-clusters-overview-part-two">Livermore Computing Linux Clusters Overview Part Two</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-resources-and-environment">Livermore Computing Resources and Environment</a></li>
<li class="leaf"><a href="/training/tutorials/slurm-and-moab-exercise">Slurm and Moab Exercise</a></li>
<li class="leaf"><a href="/training/tutorials/slurm-and-moab">Slurm and Moab Tutorial</a></li>
<li class="leaf"><a href="/training/tutorials/totalview-part-2-common-functions">TotalView Part 2:  Common Functions</a></li>
<li class="leaf"><a href="/training/tutorials/totalview-part-3-debugging-parallel-programs">TotalView Part 3: Debugging Parallel Programs</a></li>
<li class="leaf"><a href="/training/tutorials/totalview-tutorial">TotalView Tutorial</a></li>
<li class="leaf"><a href="/training/tutorials/evaluation-form">Tutorial Evaluation Form</a></li>
<li class="leaf"><a href="/training/tutorials/srun-auto-affinity">srun --auto-affinity</a></li>
<li class="last leaf"><a href="/training/tutorials/srun-multi-prog">srun --multi-prog</a></li>
</ul></li>
<li class="collapsed"><a href="/training/documentation">Documentation &amp; User Manuals</a></li>
<li class="leaf"><a href="/training/technical-bulletins-catalog">Technical Bulletins Catalog</a></li>
<li class="collapsed"><a href="/training/workshop-schedule">Training Events</a></li>
<li class="last leaf"><a href="/training/user-meeting-presentations-archive">User Meeting Presentation Archive</a></li>
</ul></li>
</ul>                                                                            <div id="pagetoggle" class="btn-group btn-toggle pull-right" style="margin-right: 15px;">
                                            <a href="/" class="btn btn-default gs">General Site</a>
                                            <a href="/user-portal" class="btn btn-primary up active">User Portal</a>
                                        </div>
                                                                    </div>
                            </nav>
                        </div>
                    </div>
                </div>
            </div>
        </div>
            </div>
		<div id="main-content" class="l2content">
        <div class="container">
    		<div class="row">
        		                <div id="primary" class="content-area col-sm-12">
					                                        <section id="content" role="nav" class="clearfix col-sm-12">

                                                                                    <div id="breadcrumbs">
                                    <h2 class="element-invisible">breadcrumb menu</h2><nav class="breadcrumb" aria-label="breadcrumb-navigation"><a href="/">Home</a> » <a href="/training">Training</a> » <a href="/training/tutorials">Tutorials</a> » Livermore Computing PSAAP3 Quick Start Tutorial</nav>                                </div>
                                                    
                                            </section>
                  <main>

                                              <div id="content_top">
                                <div class="region region-content-top">
  <div id="block-print-ui-print-links" class="block block-print-ui">

    
    
  
  <div class="content">
    <span class="print_html"><a href="https://hpc.llnl.gov/print/800" title="Display a printer-friendly version of this page." class="print-page" onclick="window.open(this.href); return false" rel="nofollow">Printer-friendly</a></span>  </div>
  
</div> <!-- /.block --></div>
 <!-- /.region -->
                            </div>
                        
                        <div id="content-wrap">
                                                                                                                <div class="region region-content">
  <div id="block-system-main" class="block block-system">

    
    
  
  <div class="content">
    

<div  about="/training/tutorials/livermore-computing-psaap3-quick-start-tutorial" typeof="sioc:Item foaf:Document" class="node node-user-portal-one-column-page node-full view-mode-full">
    <div class="row">
    <div class="col-sm-12 ">
      <div class="field field-name-title field-type-ds field-label-hidden"><div class="field-items"><div class="field-item even" property="dc:title"><h1 class="title">Livermore Computing PSAAP3 Quick Start Tutorial</h1></div></div></div><div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even" property="content:encoded"><p><a name="TOC" id="TOC"></a></p>
<h2>Part 1: Resources and Environment</h2>
<ol><li><a href="#Resources">Resources</a>
<ol><li><a href="#Hardware">PSAAP 3 Systems Summary</a></li>
<li><a href="#IntelSystems">Intel Xeon Systems</a></li>
<li><a href="#coral-systems">CORAL: LLNL Sierra Systems</a></li>
</ol></li>
<li><a href="#Requesting-Accounts">Requesting Accounts—Sarape</a></li>
<li><a href="#lc-systems">Accessing LC Systems</a>
<ol><li><a href="#Passwords">Passwords, Authentication and OTP Tokens</a></li>
<li><a href="#methods">SSH and Access Methods</a></li>
<li><a href="#login">Where to Login</a></li>
<li><a href="#remote-access">VPN Remote Access Service</a></li>
</ol></li>
<li><a href="#LC-Hotline">Getting Help from LC's HPC Hotline</a></li>
<li><a href="#file-systems">File Systems</a>
<ol><li><a href="#home-directories">Home Directories and Login Files</a></li>
<li><a href="#usr-workspace">/usr/workspace File Systems</a></li>
<li><a href="#temp-file-systems">Temporary File Systems</a></li>
<li><a href="#parallel-file-systems">Parallel File Systems</a></li>
<li><a href="#archival-hpss">Archival HPSS Storage</a></li>
<li><a href="#usr-gapps">/usr/gapps, /usr/gdata File Systems</a></li>
<li><a href="#quotas">Quotas</a></li>
<li><a href="#purge-policies">Purge Policies</a></li>
<li><a href="#backups">Backups</a></li>
<li><a href="#file-transfer">File Transfer and Sharing</a></li>
</ol></li>
<li><a href="#sys-config-info">System Configuration and Status Information</a></li>
<li><a href="#development-environment">Software Overview</a></li>
<li><a href="#compilers">Compilers</a></li>
<li><a href="#debuggers">Debuggers</a></li>
<li><a href="#performance-analysis">Performance Analysis Tools</a></li>
</ol><h2>Part 2: Running Jobs</h2>
<ol><li><a href="#BasicConcepts">Basic Concepts</a>
<ol><li><a href="#Jobs">Jobs</a></li>
<li><a href="#QueueLimits">Queues and Queue Limits</a></li>
<li><a href="#Banks">Banks</a></li>
</ol></li>
<li><a href="#BasicFunctions">Basic Functions</a>
<ol><li><a href="#JobScript">Building a Job Script</a></li>
<li><a href="#Submit">Submitting Jobs</a></li>
<li><a href="#BanksUsage">Banks and Usage Information</a></li>
<li><a href="#OutputFiles">Output Files</a></li>
<li><a href="#Guesstimate">Estimating when your job will start</a></li>
</ol></li>
<li><a href="#ParallelJobs">Parallel Jobs and the srun Command</a></li>
<li><a href="#MultipleJobs">Running Multiple Jobs From a Single Job Script</a></li>
<li><a href="#Interactive">Interactive Jobs</a>
<ol><li><a href="#interactSLURM">Interactive Jobs - SLURM</a></li>
<li><a href="#interactLSF">Interactive Jobs - LSF</a></li>
</ol></li>
<li><a href="#moreinfo">More Information - LC Documentation and Tutorials</a></li>
</ol><h2><a id="Resources" name="Resources"></a>Resources</h2>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1919" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/lassen-hpc-png">lassen-hpc.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/lassen-hpc.png"><img alt="Lassen Supercomputer" height="158" width="260" style="height: 158px; width: 260px;" class="media-element file-default" data-delta="161" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/lassen-hpc-260x158.png" /></a>  </div>

  
</div>
</div> <br />Lassen</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-2133" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/img-3726-jpg">IMG_3726.JPG</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/IMG_3726.JPG"><img alt="pascal supercomputer" height="158" width="237" style="height: 158px; width: 237px;" class="media-element file-default" data-delta="162" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/IMG_3726-237x158.JPG" /></a>  </div>

  
</div>
</div><br />Pascal</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-2034" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/quartz-llnl-png">quartz-LLNL.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/quartz-LLNL.png"><img alt="Quartz" height="158" width="236" style="width: 236px; height: 158px;" class="media-element file-default" data-delta="163" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/quartz-LLNL-236x158.png" /></a>  </div>

  
</div>
</div><br />Quartz</div>
<h3><a id="Hardware" name="Hardware"></a>PSAAP3 Systems Summary</h3>
<table class="table table-striped table-bordered"><tr><th scope="col">Cluster</th>
<th scope="col">Architecture</th>
<th scope="col">Clock Speed (CHz)</th>
<th scope="col">Nodes GPUs</th>
<th scope="col">Cores / Node / GPU</th>
<th scope="col">Cores Total</th>
<th scope="col">Memory / Node (GB)</th>
<th scope="col">Memory Total (GB)</th>
<th scope="col">TFLOPS Peak</th>
<th scope="col">Switch</th>
</tr><tr><td><strong>lassen</strong></td>
<td>IBM Power9<br />NVIDIA Tesla V100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>774<br />774*4</td>
<td>44<br />5120</td>
<td>34,056<br />15,851,520</td>
<td>256<br />16*4</td>
<td>198,144<br />49,536</td>
<td>22,508</td>
<td>IB EDR</td>
</tr><tr><td><strong>pascal</strong></td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>171<br />163*2</td>
<td>36<br />3484</td>
<td>6,156<br />1,135,784</td>
<td>256<br />16*2</td>
<td>18,176<br />5,216</td>
<td>206.8<br />1,727.8</td>
<td>Omni-Path</td>
</tr><tr><td><strong>quartz</strong></td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>3,072</td>
<td>36</td>
<td>110,592</td>
<td>128</td>
<td>393,216</td>
<td>3,715.9</td>
<td>Omni-Path</td>
</tr></table><h2><a id="IntelSystems" name="IntelSystems"></a>Intel Xeon Systems</h2>
<ul><li>The majority of LC's systems are Intel Xeon based Linux clusters, and include the following processor architectures:
<ul><li>Intel Xeon 18-core E5-2695 v4 (Broadwell)</li>
<li>Intel Xeon 8-core E5-2670 (Sandy Bridge - TLCC2) w/without NVIDIA GPUs</li>
<li>Intel Xeon 12-core E5-2695 v2 (Ivy Bridge)</li>
</ul></li>
<li>Mix of resources:
<ul><li>8, 12, and 18 core processors</li>
<li>OCF and SCF</li>
<li>ASC, M&amp;IC, VIZ</li>
<li>Capacity, Grand Challenge, visualization, testbed</li>
<li>Several GPU enabled clusters</li>
</ul></li>
<li>64-bit architecture</li>
<li>TOSS operating system stack</li>
<li>InfiniBand and Intel Omni-Path interconnects</li>
<li>Hyper-threading enabled (2 threads/core)</li>
<li>Vector/SIMD operations</li>
<li>For detailed hardware information, please see the "Additional Information" references below.</li>
</ul><h4>Additional Information</h4>
<ul><li>Linux Clusters Tutorial: <a href="http://computing.llnl.gov/tutorials/linux_clusters" target="_blank">computing.llnl.gov/tutorials/linux_clusters</a></li>
<li>Usage Information for LC's new CTS-1 systems: <a href="https://lc.llnl.gov/confluence/display/TCE/TCE+Home" target="_blank">lc.llnl.gov/confluence/display/TCE/TCE+Home</a></li>
<li>Reference list of Intel Xeon Processors: <a href="http://en.wikipedia.org/wiki/List_of_Intel_Xeon_microprocessors" target="_blank">en.wikipedia.org/wiki/List_of_Intel_Xeon_microprocessors</a></li>
</ul><h3><a name="coral-systems" id="coral-systems"></a>CORAL: LLNL Sierra Systems</h3>
<p> </p>
<ul><li>Sierra is a classified, 125-petaflop, IBM Power Systems AC922 hybrid architecture system comprised of IBM POWER9 nodes with NVIDIA Volta GPUs. Sierra is a Tri-lab resource sited at LLNL.</li>
<li>Unclassified Sierra systems are similar, but smaller, and include:
<ul><li><strong>Lassen</strong> - a 20-petaflop system located on LC's CZ zone</li>
<li><strong>rzansel</strong> - a 1.5-petaflop system is located on LC's RZ zone</li>
</ul></li>
<li>IBM Power Systems AC922 Server: Hybrid architecture using IBM POWER9 processors and NVIDIA Volta GPUs.</li>
<li>IBM POWER9 processors (compute nodes):
<ul><li>2 per node (dual-socket)</li>
<li>22 cores/socket; 44 cores per node</li>
<li>4 SMT threads per core; 176 SMT threads per node</li>
<li>Clock: due to adaptive power management options, the clock speed can vary depending upon the system load. At LC speeds can vary from approximately 2.3 - 3.8 GHz. LC can also set the clock to a specific speed regardless of workload.</li>
</ul></li>
<li>NVIDIA GPUs:
<ul><li>4 NVIDIA Tesla V100 (Volta) GPUs per compute, login, launch node</li>
<li>5120 CUDA cores per GPU; 20,480 per node</li>
</ul></li>
<li>Memory:
<ul><li>256 GB DDR4 per compute node</li>
<li>16 GB HBM2 (High Bandwidth Memory 2) per GPU; 900 GB/s peak bandwidth</li>
</ul></li>
<li>NVLINK 2.0:
<ul><li>Interconnect for GPU-GPU and CPU-GPU shared memory</li>
<li>6 links per GPU with 300 GB/s total bandwidth</li>
</ul></li>
<li>NVRAM: 1.6 TB NVMe PCIe SSD per compute node</li>
<li>Network:
<ul><li>Mellanox 100 Gb/s Enhanced Data Rate (EDR) InfiniBand</li>
<li>One dual-port 100 Gb/s EDR Mellanox adapter per node</li>
</ul></li>
<li>Parallel File System: IBM Spectrum Scale (GPFS)</li>
<li>Batch System: IBM Spectrum LSF</li>
<li>Water (warm) cooled compute nodes</li>
<li>Additional information:
<ul><li>Tutorial: <a href="/training/tutorials/using-lcs-sierra-system">hpc.llnl.gov/training/tutorials/using-lcs-sierra-system</a></li>
<li>User Guide: <a href="https://lc.llnl.gov/confluence/display/SIERRA/Sierra+Systems" target="_blank">lc.llnl.gov/confluence/display/SIERRA/Sierra+Systems</a> (LC internal wiki)</li>
</ul></li>
</ul><h2><a id="Requesting-Accounts" name="Requesting-Accounts"></a>Requesting Accounts—Sarape</h2>
<p>All PSAAP accounts for Tri-lab compute resources must be requested through the web-based SARAPE account request system at <a href="http://sarape.sandia.gov" target="_blank">http://sarape.sandia.gov</a>.  <span class="note-red">Note</span>SARAPE requires login. Participating PSAAP centers will have a processing agent who will have login access. Your university processing agent will assist you in submitting the initial request. Most fields are self-explanatory, but some useful hints are provided below:</p>
<ul><li>Step 2: Requestor's Manager's Info: this pertains to your PSAAP center's designated SARAPE processing agent - that is, the person within your center who authorizes your center's account requests.</li>
<li>Step 3: Select the Lab where you would like an account and the available machines will appear. If you mouse-over the blue “i” icon next to the machine name, additional information about that machine will appear.</li>
<li>Step 4: Additional information: For the required "Justification" box, please indicate the name of your PSAAP center and what your work will pertain to.</li>
</ul><p>Following completion of the form, your request will be reviewed by your center's SARAPE processing agent, and if approved, it will then be sent to the Lab(s) where you requested an account.  The review and approval processes at each Lab vary, but averages about one week for US citizens.  Non-US citizens are required to submit additional paperwork, initiated after the SARAPE request is received at the host site, and requires additional approvals.  Processing time for these also varies, between one and three months.  </p>
<h3>Passwords/Tokens</h3>
<p>Each laboratory has its own tokens and/or passwords for access to its resources. If you have accounts at multiple sites you will receive them separately. After accounts are approved and issued, each laboratory has its own process for providing/sending your password and/or token.</p>
<h3>Training</h3>
<p>All laboratories require users to take online cyber security training on an annual basis. If this required training is not completed, machine accounts will be deactivated. </p>
<h3>Account Reauthorization</h3>
<p>All three laboratories require annual reauthorization of existing accounts. You will be notified via email regarding this process when your account renewal date approaches.</p>
<h2><a id="lc-systems" name="lc-systems"></a>Accessing LC Systems</h2>
<h3><a id="Passwords" name="Passwords"></a>Passwords, Authentication and OTP Tokens</h3>
<h4>One-time Passwords (OTP)</h4>
<p></p><div class="media media-element-container media-default"><div id="file-1394" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/cz-token-png">cz-token.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/cz-token_2.png"><img alt="CZ Token" height="100" width="180" style="height: 100px; width: 180px;" class="media-element file-default" data-delta="1" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/cz-token_2-180x100.png" /></a>  </div>

  
</div>
</div>
<ul><li>Single-use passwords are mandatory on all LC machines: classified and unclassified.</li>
<li>Based upon a "two factor" authentication:
<ul><li>static, 4-8 character alphanumeric PIN for every user</li>
<li>6-digit random number generated by an RSA SecureID token device</li>
</ul></li>
<li>OTP authentication is also used for other services:
<ul><li>Access to (some) internal LC web pages</li>
<li>Remote Access Services such as VPN (discussed later)</li>
</ul></li>
</ul><h4>Problems?</h4>
<ul><li>Under certain circumstances, an OTP server and your token may get out of sync. In such cases it is necessary to enter two consecutive token codes so the server can resynchronize itself. You may also need/want to change your PIN.
<ul><li>Both of these actions can be performed via the OTP web pages: <a href="https://otp.llnl.gov" target="_blank">otp.llnl.gov</a></li>
</ul></li>
<li>Contact the LC Hotline if problems persist, or for other token related issues/questions: (925) 422-4533 <a href="mailto:lc-support@llnl.gov">lc-support@llnl.gov</a></li>
</ul><h3><a name="methods" id="methods"></a>SSH and Access Methods</h3>
<h4>SSH Required</h4>
<ul><li>Secure Shell (SSH) is required for access to all LC systems.</li>
<li>The main advantages of SSH are:
<ul><li>No clear text password goes over network</li>
<li>The data stream is encrypted</li>
<li>Use of RSA/DSA authentication between LC clusters</li>
</ul></li>
<li>Mac and Linux users:
<ul><li>SSH is included on Mac and Linux platforms</li>
<li>Can simply be used from a terminal window command line. Examples:</li>
</ul></li>
</ul><pre>ssh joeuser@quartz.llnl.gov
ssh -l joeuser sierra.llnl.gov</pre><p> </p>
<ul><li>Windows 10 provides an OpenSSH SSH client, which can be used from a Command Prompt window or PowerShell window. Note that you may need to specify the MAC (authentication) type. Examples:</li>
</ul><pre>ssh -m hmac-sha2-256 joeuser@quartz.llnl.gov
ssh -m hmac-sha2-512 -l joeuser sierra.llnl.gov
</pre><p>To avoid the need to enter a MAC type each time, simply create a <span class="fixed">C:\Users\joeuser\.ssh\config file</span> and add the following line to it:</p>
<pre>MACs hmac-sha2-256,hmac-sha2-512</pre><ul><li>May need to install an SSH app such as PuTTY. Searching the web will reveal other options.</li>
</ul><ul><li>All production CZ systems can be accessed directly from the Internet.</li>
<li>Simply use SSH (or for Windows, use your favorite SSH app) and connect to a cluster where you have an account.</li>
<li>Authenticate with your LC username and CZ PIN + RSA OTP token.</li>
</ul><h4>Web Page Access</h4>
<ul><li>The majority of LC's web pages at <a href="https://hpc.llnl.gov">hpc.llnl.gov</a> are publicly available over the Internet without the need for authentication.</li>
<li>Web pages located on LC's Confluence Wikis require authentication with an LC username and the relevant domain PIN + OTP token.</li>
<li>Likewise, web pages located on the MyLC portals require the appropriate LC authentication method.</li>
</ul><h4>More Information</h4>
<ul><li>SSH Guide for Livermore Computing (internal wiki): <a href="https://lc.llnl.gov/confluence/display/czconfdocs/SSH+Guide+for+Livermore+Computing" target="_blank">lc.llnl.gov/confluence/display/czconfdocs/SSH+Guide+for+Livermore+Computing</a></li>
<li>ssh man page</li>
</ul><h3><a name="login" id="login"></a>Where to Login</h3>
<h4>Login Nodes</h4>
<ul><li class="clear-floats"><span>LC clusters have specific nodes dedicated to user login sessions.</span></li>
<li>Login nodes are shared by multiple users.</li>
<li>LC provides a "generic" login alias (cluster login) for each cluster. The cluster login automatically rotates between available login nodes for load balancing purposes.</li>
<li>For example: <strong>quartz.llnl.gov</strong> is the cluster login alias - which could be any of the physical login nodes.</li>
<li>Users don't need to know (in most cases) the actual login node they are rotated onto - unless there are problems. Using the <span class="fixed">hostname </span>command will indicate the actual login node name for support purposes.</li>
<li>If the login node you are on is having problems, you can <span class="fixed">ssh</span> directly to another one. To find the list of available login nodes, use the command: <span class="fixed">nodeattr -c login</span></li>
</ul><h3><a name="remote-access" id="remote-access"></a>VPN Remote Access Service</h3>
<ul><li>Use of a Remote Access Service (usually VPN) is required if you are outside of the LLNL internal network, and wish to access:
<ul><li>Institutional network services (LITE, LTRAIN, email, etc.)</li>
<li>Livermore Computing Restricted Zone (RZ) compute resources</li>
</ul></li>
<li>Provided by the Cyber Security Program</li>
<li>Not required for access to LC OCF Collaboration Zone (CZ) machines</li>
<li>To request LLNL VPN access, download software and see setup instructions, go to: <a href="https://access.llnl.gov/vpn/" target="_blank">access.llnl.gov/vpn/</a>.</li>
<li>LLNL also offers a browser-based SSL VPN Web Portal:
<ul><li>The web portal should be used for Internet kiosks, such as at an airport or a conference, to access LLNL systems from off-site.</li>
<li>This service can be used for submitting your timecard or sending unencrypted email.</li>
<li>For details, see the link provided above.</li>
</ul></li>
</ul><h2><a id="LC-Hotline" name="LC-Hotline"></a>Getting Help from the LC Hotline (for HPC)</h2>
<p>We have dedicated teams of subject area experts available:</p>
<ul><li>LC Hotline: <a href="tel:925-422-4531">(925) 422-4531</a></li>
<li>Technical Consultants | <a href="mailto:lc-hotline@llnl.gov">lc-hotline@llnl.gov</a></li>
<li>Account Specialists | <a href="mailto:lc-support@llnl.gov">lc-support@llnl.gov</a></li>
<li>LC Operations (24/7) | <a href="mailto:mfopers@llnl.gov">mfopers@llnl.gov</a></li>
</ul><p>For more information: <a href="https://hpc.llnl.gov/about-us/contact-us">https://hpc.llnl.gov/about-us/contact-us</a></p>
<h2><a name="file-systems" id="file-systems"></a>File Systems</h2>
<h3><a name="home-directories" id="home-directories"></a>Home Directories and Login Files</h3>
<h4>Home Directories</h4>
<ul><li>LC user home directories are <strong>global</strong> to their network partition: 1 home directory system for the SCF, 1 for the OCF-CZ and 1 for the OCF-RZ.</li>
<li>Naming scheme: <span class="fixed">/g/g#/user_name</span>. Examples:
<ul><li><span class="fixed">/g/g15/joeuser</span></li>
<li><span>/g/g0/joestaff</span></li>
</ul></li>
<li>Backups:
<ul><li>Online: <span class="fixed">.snapshot</span> directories - twice daily</li>
<li>Daily incremental</li>
<li>Monthly</li>
<li>Bi-annual offsite disaster recovery</li>
<li>See the <a href="#backups">Backups</a> section for details</li>
</ul></li>
<li>NFS mounted:
<ul><li>Not recommended for parallel I/O</li>
</ul></li>
<li>Quota in effect - see the <a href="#quotas">Quotas</a> section for details.</li>
</ul><h4>LC's Login Files</h4>
<ul><li>Your login shell is established when your LC account is initially setup. The usual login shells are supported:<br /><span class="fixed">/bin/bash<br />/bin/csh<br />/bin/ksh<br />/bin/sh<br />/bin/tcsh<br />/bin/zsh</span></li>
<li>All LC users automatically receive a set of login files. These include:</li>
</ul><pre>.cshrc        .kshenv       .login        .profile
              .kshrc        .logout
.cshrc.linux  .kshrc.linux  .login.linux  .profile.linux</pre><ul><li>The files which are "sourced" when you login depends upon your shell.</li>
<li>Note for bash and zsh users: LC does not provide .bashrc, .bash_profile, .zprofile or .zshrc files at this time.</li>
</ul><h3><a name="usr-workspace" id="usr-workspace"></a>/usr/workspace File Systems</h3>
<ul><li>LC provides 2 terabytes of NFS mounted file space for each user and group.</li>
<li>Located under <span class="fixed">/usr/workspace/<em>username</em></span><em> </em>and <span class="fixed">/usr/workspace/<em>groupname</em></span></li>
<li><span class="fixed">/usr/workspace/<em>username</em></span> is accessible by the user only. <span class="fixed">/usr/workspace/<em>groupname</em></span> may be accessed by the group members.</li>
<li>Similar to home directory:
<ul><li>Cross mounted from appropriate clusters</li>
<li>Not purged</li>
<li>Includes <span class="fixed">.snapshot</span> directory for twice-daily online backups</li>
<li>Not intended for parallel I/O</li>
</ul></li>
<li>Different from home directory:
<ul><li>Not backed up</li>
</ul></li>
</ul><h3><a name="temp-file-systems" id="temp-file-systems"></a>Temporary File Systems</h3>
<ul><li><strong>/tmp<br />/usr/tmp<br />/var/tmp</strong>
<ul><li>Different names for the same <span class="fixed">/tmp</span> file system</li>
<li>Local to each individual node; very small compared to other temporary file systems</li>
<li><strong>Note:</strong>  These are virtual file systems - use the node's local memory, which may impact the amount of memory left for the job running on the node.</li>
<li>Faster than NFS</li>
<li>No quota, no backups</li>
<li>Purged between batch jobs</li>
</ul></li>
</ul><ul><li><strong>/p/lustre#</strong>
<ul><li>Lustre parallel file systems</li>
<li>Global temporary file systems - shared by all users</li>
<li>Very large, multi-petabyte in size - varies by file system</li>
<li>Available on most OCF systems</li>
<li>Quotas are in place for the /p/lustre# file systems</li>
<li>Not subject to purging</li>
<li>No backups</li>
</ul></li>
</ul><ul><li><strong>/p/gpfs#<br />/p/gscratch#</strong>
<ul><li>Large, temporary parallel file systems found on Sierra and CORAL EA systems</li>
<li>IBM Spectrum Scale product (formerly known as GPFS)</li>
<li>No quotas, subject to purging, no backups</li>
</ul></li>
</ul><h3><a name="parallel-file-systems" id="parallel-file-systems"></a>Parallel File Systems</h3>
<ul><li>In a typical cluster, most nodes are <strong><em>compute nodes</em></strong> where programs actually run. A subset of the system's nodes are dedicated to serve as<strong><em> I/O nodes</em></strong>. I/O nodes are also referred to as <em><strong>gateway nodes</strong></em>.</li>
<li>I/O nodes are the interface to disk resources. All I/O performed on compute nodes is routed to the I/O nodes over the internal switch network (such as InfiniBand).</li>
<li>The I/O nodes then send the I/O requests to storage servers over the SAN (Storage Area Network) which can be 10Gbit Ethernet or InfiniBand. The storage servers then perform the actual I/O to attached physical disk resources.</li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-877" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/sangif">SAN.gif</a></h2>
    
  
  <div class="content">
    <img alt="Storage Area Network" height="348" width="392" class="media-element file-default" data-delta="164" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/SAN.gif" /></div>

  
</div>
</div> <div class="media media-element-container media-default"><div id="file-1835" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/filestriping-gif">fileStriping.gif</a></h2>
    
  
  <div class="content">
    <img alt="File Diagram with colored striping" height="361" width="577" style="width: 577px; height: 361px;" class="media-element file-default" data-delta="165" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/fileStriping_0.gif" /></div>

  
</div>
</div>
<ul><li>Individual files are stored as a series of "blocks" that are striped across the disks of different storage servers. This permits concurrent access by a multi-task application when tasks read/write to different segments of a common file.</li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-1836" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/lustrefilesystem1-jpg">lustreFileSystem1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/lustreFileSystem1.jpg"><img alt="Lustre File System" height="236" width="360" style="height: 236px; width: 360px;" class="media-element file-default" data-delta="166" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/lustreFileSystem1-360x236.jpg" /></a>  </div>

  
</div>
</div>
<ul><li>Internally, file striping is set to a specific block size that is configurable. At LC, the most efficient use of parallel file systems is with large files. The use of many small files is not advised if performance is important.</li>
<li>Parallelism:
<ul><li>Simultaneous reads/writes to non-overlapping regions of the same file by multiple tasks</li>
<li>Concurrent reads and writes to different files by multiple tasks</li>
<li>Concurrent reads/writes supported by MPI I/O and also file-system API, for Lustre or GPFS.</li>
<li>I/O will be serial if tasks attempt to use the same stripe of a file simultaneously.</li>
</ul></li>
</ul><h4>Parallel File Systems—Lustre</h4>
<ul><li>Many of LC'sx86  Linux clusters use Lustre parallel file systems.</li>
<li>To the user,  appears as another mounted file system.</li>
<li>Naming scheme: <strong>/p/lustre#</strong> for Linux. For example:</li>
</ul><pre>% <span class="text-danger">bdf | grep lustre</span>
172.19.1.165@o2ib100:172.19.1.    4.9P   1.1P   3.9P  22%  /p/lustre3
172.19.3.1@o2ib600:172.19.3.2@     15P   3.8P    12P  25%  /p/lustre2
172.19.3.98@o2ib600:172.19.3.9     15P   622T    15P   5%  /p/lustre1
</pre><ul><li>LC's Lustre parallel file systems are usually mounted by more than one Linux cluster.</li>
<li>No backups</li>
<li><span class="fixed">/p/lustre#</span> enforces quotas and is NOT subject to purging</li>
<li>For additional information also see: <a href="http://wiki.lustre.org/" target="_blank">wiki.lustre.org</a></li>
</ul><h4>Parallel File Systems—IBM Spectrum Scale (GPFS)</h4>
<ul><li>LC's Sierra and CORAL EA systems use IBM's Spectrum Scale parallel file systems (formerly known as GPFS).</li>
<li>From a user perspective, they look and feel like Lustre parallel file systems on other LC clusters.</li>
<li>Naming scheme: <span class="fixed">/p/gpfs#</span> for Sierra clusters, and <span class="fixed">/p/gscratch#</span> for CORAL EA clusters. For example:</li>
</ul><pre>% <span class="text-danger">bdf | grep gpfs</span>
gpfs1                             140P    13P   127P   9%  /p/gpfs1
% <span class="text-danger">bdf | grep gscratch</span>
gpfs0                             1.3P   379T   915T  30%  /p/gscratchr</pre><ul><li>No backups, subject to purging, no quotas - currently.</li>
</ul><h4>LC Parallel File Systems Summary</h4>
<ul><li>Shows which clusters mount which parallel file systems. File system capacities are also shown.</li>
<li>As of Jan 2020. Subject to change.</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">OCF-CZ</th>
<th scope="col">lustre1<br />15 PB</th>
<th scope="col">lustre2<br />15 PB</th>
<th scope="col">lustre3<br />4.9 PB</th>
<th scope="col">gpfs1<br />24 PB</th>
</tr><tr><td><strong>borax</strong></td>
<td>X</td>
<td>X</td>
<td> </td>
<td> </td>
</tr><tr><td><strong>lassen</strong></td>
<td> </td>
<td> </td>
<td> </td>
<td>X</td>
</tr><tr><td><strong>oslic</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X - lassengpfs1</td>
</tr><tr><td><strong>pascal</strong></td>
<td>X</td>
<td>X</td>
<td> </td>
<td> </td>
</tr><tr><td><strong>quartz</strong></td>
<td>X</td>
<td>X</td>
<td> </td>
<td> </td>
</tr></table><h3><a name="archival-hpss" id="archival-hpss"></a>Archival HPSS Storage</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-880" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/hpssdiagramgif">hpssDiagram.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/hpssDiagram.gif"><img alt="HPSS Diagram" height="369" width="400" style="width: 400px; height: 369px;" class="media-element file-default" data-delta="2" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/hpssDiagram-400x369.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>High Performance Storage System (HPSS) archival storage is available on the OCF.
<ul><li>Provides disk/tape archive storage in the petabyte range. Both capacity and performance are continually increasing to keep up with the ever increasing user demand.</li>
<li>GigE connectivity to all production clusters</li>
</ul></li>
<li>Primary components:
<ul><li>Server machines</li>
<li>RAID disk cache</li>
<li>Magnetic tape libraries</li>
<li>Jumbo frame GigE network</li>
</ul></li>
<li>FTP client on LC production machines defaults to an enhanced parallel HPSS FTP client</li>
<li>No back up, no purge</li>
</ul><p><strong>Access Methods and Usage:</strong></p>
<ul><li>The HPSS system is named <strong>storage.llnl.gov</strong> on the OCF.</li>
<li>All LC users automatically receive an HPSS storage account with their regular production machine account.</li>
<li>Data Transfer Tools: The more commonly used ones are simply listed here and described in more detail in the <a href="#file-transfer">File Transfer and Sharing</a> section that follows later.
<ul><li>Hopper: <a href="/software/hopper">hpc.llnl.gov/software/hopper</a></li>
<li>FTP/PFTP: <a href="/manuals/ezstorage/ftp/">hpc.llnl.gov/manuals/ezstorage/ftp/</a></li>
<li>NFT: <a href="/manuals/ezstorage/nft">hpc.llnl.gov/manuals/ezstorage/nft</a></li>
<li>HTAR: <a href="/manuals/ezstorage/htar">hpc.llnl.gov/manuals/ezstorage/htar</a></li>
<li>Tri-lab high bandwidth file transfers over SecureNet (see the File Transfer and Sharing section)</li>
<li>Note: <span class="fixed">ssh/scp/sftp</span> to storage are not supported.</li>
</ul></li>
<li>Quotas:
<ul><li>Based on a user's annual growth in HPSS file space</li>
<li>OCF yearly growth quota (FY20): 300 TB</li>
<li>For details see Technical Bulletin 534 (<a href="/technical-bulletin-534-fy20-hpss-yearly-growth-quotas">/technical-bulletin-534-fy20-hpss-yearly-growth-quotas</a>)</li>
</ul></li>
<li>How much storage am I using? The <span class="fixed">aquota</span> command (which can only be run on oslic) provides this information. For example:</li>
</ul><pre>oslic5% <span class="text-danger">kinit</span>
<em>[authenticate here]</em>

oslic5% <span class="text-danger">aquota</span>
Welcome to HPSS Quota Server oslici.llnl.gov

aq&gt; <span class="text-danger">show allowance</span>

Pool Name                 Pool Manager  Allowance
------------------------  ------------  ---------------
lcreserve                 lc-hotline             0.0 B
default                   lc-hotline             1.5 TB
   Total                                         1.5 TB

From 10/01/2019 through 02/06/2020:
     5 files created.
     46.8 GB of data used. 3.12% of total.
     Avg. Per Month:  11.1 GB

Total Data:      85.1 TB
Total Files:     2127240

aq&gt;</pre><p>Usage notes:</p>
<ul><li>Currently, you must be logged into <span class="fixed">oslic</span> to use this command.</li>
<li>Use the aquota <span class="fixed">help </span>subcommand for additional options.</li>
<li>You may need to authenticate with the <span class="fixed">kinit </span>command first.</li>
</ul><h4>Additional Information</h4>
<ul><li>HPSS Home Page: <a href="http://www.hpss-collaboration.org/" target="_blank">www.hpss-collaboration.org</a></li>
</ul><h3><a name="usr-gapps" id="usr-gapps"></a>/usr/gapps, /usr/gdata File Systems</h3>
<ul><li>LC provides shared, collaborative, NFS file space for user developed and supported applications and data on LC systems:  <span class="fixed">/<strong>usr/gapps, /collab/usr/gapps/, /usr/gdata, /collab/usr/gdata</strong></span></li>
</ul><ul><li>Unlike your home directory, these file systems can be used (with approval) to share file space within a group or even the world.</li>
<li>For convenience, OCF-RZ users can use <span class="fixed">/collab/usr/gapps</span> and <span class="fixed">/collab/usr/gdata</span> to share files with OCF-CZ users.</li>
<li>Backups:
<ul><li>Online: <span class="fixed">.snapshot</span> directories.</li>
<li>Daily incremental</li>
<li>Monthly</li>
<li>Bi-annual offsite disaster recovery</li>
<li>See the <span>Backups</span> section for details.</li>
</ul></li>
<li>Never purged</li>
<li>Multiple architectures are handled through the $SYS_TYPE variable:
<ul><li>Every LC machine sets this environment variable to a specific string that matches its architecture. For example:</li>
</ul></li>
</ul><pre>toss_3_x86_64_ib
blueos_3_ppc64le_ib</pre><ul><li>Versions of code built for specific architectures are placed in subdirectories named to match $SYS_TYPE strings</li>
<li>User scripts can select the appropriate code versions based upon the $SYS_TYPE setting. For example: <span class="fixed">cd /usr/gapps/myApp/$SYS_TYPE/bin</span></li>
</ul><ul><li>Requesting a directory within <span class="fixed">/usr/gapps</span>: submit the <a href="/sites/default/files/usr_gapps.pdf">LC USR_GAPPS</a> form to create/change/delete a directory.</li>
<li>Sharing files and directories in your <span class="fixed">/usr/gapps</span> directory with a group:
<ul><li>Create and manage UNIX groups: <a href="https://lc-idm.llnl.gov/" target="_blank">lc-idm.llnl.gov</a></li>
<li>Then use UNIX permissions to permit group sharing</li>
</ul></li>
<li>For additional information see the <a href="/hardware/file-systems/usr-gapps-file-system">/usr/gapps web page</a></li>
</ul><h3><a name="quotas" id="quotas"></a>Quotas</h3>
<h4>Home Directories</h4>
<ul><li>The default home directory quota is 24 GB per user.</li>
<li>Other file systems can have quotas. The /p/lustre# file systems, in particular, have a 3-tier quota system:</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">Quotas</th>
<th scope="col">Tier 1 (default)</th>
<th scope="col">Tier 2</th>
<th scope="col">Tier 3</th>
</tr><tr><td>space</td>
<td>20 TB</td>
<td>50 TB</td>
<td>over 50 TB</td>
</tr><tr><td>inodes</td>
<td>1 M</td>
<td>10 M</td>
<td>over 10 M</td>
</tr><tr><td>process</td>
<td>automatic</td>
<td>email LC Hotline</td>
<td>email LC Hotline</td>
</tr><tr><td>duration</td>
<td>system life</td>
<td>system life</td>
<td>up to 1 year</td>
</tr></table><ul><li>Lustre quotas are discussed in detail at: <a href="https://myconfluence.llnl.gov/display/RAM/LustreX" target="_blank">myconfluence.llnl.gov/display/RAM/LustreX</a></li>
<li>To check usage and limits: <span class="fixed">quota -v</span></li>
<li>Example:</li>
</ul><pre>% <span class="text-danger">quota -v</span>

Disk quotas for joeuser:
Filesystem     used   quota  limit    timeleft  files  quota  limit    timeleft
<span class="text-danger">/g/g0          1.1G   24.0G  24.0G              3.9K   n/a    n/a</span>     
/g/g10         -0-    24.0G  24.0G              -0-    n/a    n/a     
...   
/g/g92         -0-    24.0G  24.0G              -0-    n/a    n/a     
/g/g99         -0-    24.0G  24.0G              -0-    n/a    n/a     
/usr/gapps     -0-    n/a    n/a                -0-    n/a    n/a     
/collab/usr/gapps
               -0-    n/a    n/a                -0-    n/a    n/a     
/usr/give      -0-    25.0G  25.0G              -0-    n/a    n/a     
/usr/global    -0-    32.0G  32.0G              -0-    n/a    n/a     
/collab/usr/global
               -0-    32.0G  32.0G              -0-    n/a    n/a     
/usr/workspace/wsa
               -0-    n/a    n/a                -0-    n/a    n/a     
/usr/workspace/wsb
               -0-    n/a    n/a                -0-    n/a    n/a         
/p/lustre1     24.5K  18.0T  20.0T              0.0K   900.0K 1.0M    
/p/lustre2     24.5K  18.0T  20.0T              0.0K   900.0K 1.0M</pre><ul><li>Requests for additional disk space should be directed to the LC Hotline through your computer coordinator or PI.</li>
<li>To view usage by user per file system, see the log files located in <span class="fixed">/usr/global/docs/filerUsageInfo</span></li>
<li><strong>Exceeding quota:</strong>
<ul><li>Warning appears in login messages if usage over 90% quota</li>
<li>Heed quota warnings - risk of data loss if quota is exceeded!</li>
</ul></li>
</ul><h4>Other File Systems</h4>
<ul><li>HPSS archival storage: quotas in effect as discussed in <a href="#archival-hpss">Archival HPSS Storage</a> section.</li>
</ul><h3><a name="purge-policies" id="purge-policies"></a>Purge Policies</h3>
<ul><li>When file systems become full, performance can be significantly degraded. Because of this, LC maintains policies for purging temporary file systems.</li>
<li>The following <strong><em>temporary</em></strong> file systems are subject to purging:</li>
</ul><pre>/tmp
/usr/tmp
/var/tmp
/nfs/tmp#
/p/lscratch#</pre><ul><li>The <strong>/p/lustre#</strong> temporary file systems are NOT subject to purging since they enforce quotas. Likewise for the <span class="fixed">/p/gpfs#</span> file systems once quotas are implemented for them.</li>
<li>When are files purged?
<ul><li><span class="fixed">/tmp, /var/tmp, /usr/tmp:</span> node-local temporary file space is purged daily and/or in between batch jobs.</li>
<li><span class="fixed">/p/lscratch#:</span> as needed</li>
</ul></li>
<li>Files in temporary file systems are <strong>not</strong> backed up</li>
<li><strong>Don't forget:</strong> <span class="fixed">tmp</span> or <span class="fixed">scratch</span> in the name means <strong>temporary!</strong></li>
</ul><h3><a name="backups" id="backups"></a>Backups</h3>
<h4>Online .snapshot Directories</h4>
<ul><li>User home directories, <span class="fixed">/usr/workspace</span>, <span class="fixed">/usr/gapps</span> and <span class="fixed">/usr/gdata</span> have a special, online directory for regular, automatic backups.</li>
<li>"Hidden" <span class="fixed">.snapshot</span> subdirectory
<ul><li>You can <span class="fixed">cd .snapshot</span></li>
<li>Contains multiple subdirectories, each containing a full backup and a timestamp when the backup was created.</li>
<li><span class="fixed">.snapshot</span> is read-only directory</li>
</ul></li>
</ul><h4>Livermore Computing System Backups</h4>
<ul><li>LC performs regular backups of the following file systems:
<ul><li><span class="fixed">/g/g#</span>: User home directories</li>
<li><span class="fixed">/usr/gapps, /usr/gdata</span>: User application and data directories</li>
<li><span class="fixed">/usr/local, /usr/global</span>: LC developed or maintained application directories</li>
<li>Atlassian Tools: Jira, Confluence, Bitbucket, etc.</li>
</ul></li>
<li>Daily backups of new or changed files</li>
<li>Full monthly backup of all files. Retained onsite for 6 months.</li>
<li>Disaster recovery backups:
<ul><li>Performed every 6 months</li>
<li>Data is stored offsite at the Nevada Test Facility</li>
<li>Retained for 2 years</li>
</ul></li>
<li>For detailed information on LC backups, see the internal wiki document located at: <a href="https://lc.llnl.gov/confluence/display/LCBackups/LC+Backups+Home" target="_blank">lc.llnl.gov/confluence/display/LCBackups/LC+Backups+Home</a> (requires authentication)</li>
</ul><p><span class="note-red">Note:</span>Temporary file systems are not backed up:</p>
<ul><li><span class="fixed">/tmp, /var/tmp, /usr/tmp</span></li>
<li><span class="fixed">/p/lustre#</span></li>
<li><span class="fixed">/p/gscratch#</span></li>
<li><span class="fixed">/p/gpfs#</span></li>
</ul><h4>Archival HPSS Storage</h4>
<ul><li>Users are responsible for backing up all other data they wish to preserve, particularly any files residing in temporary file systems.</li>
<li>The preferred location for these backups is the archival HPSS storage system.</li>
<li>See t<span class="text-info">he <a href="#archival-hpss">Archival HPSS Storage</a> s</span>ection for details.</li>
</ul><h3><a name="file-transfer" id="file-transfer"></a>File Transfer and Sharing</h3>
<h4>File Transfer Tools</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-894" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/hopper400pixjpeg">hopper400pix.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/hopper400pix.jpeg"><img alt="Hopper" height="205" width="320" style="height: 205px; width: 320px;" class="media-element file-default" data-delta="3" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/hopper400pix-320x205.jpeg" /></a>  </div>

  
</div>
</div><br /><div class="media media-element-container media-default"><div id="file-2071" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mylchomepage-jpg-0">MYLChomepage.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/MYLChomepage_0.jpg"><img alt="screen shot of My LC home page" height="206" width="320" style="height: 206px; width: 320px;" class="media-element file-default" data-delta="4" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/MYLChomepage_0-320x206.jpg" /></a>  </div>

  
</div>
</div><br /><div class="media media-element-container media-default"><div id="file-2069" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/trilabtransfers-gif">TrilabTransfers.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/TrilabTransfers_0.gif"><img alt="diagrams of LANL, LLNL, and SNL data transfers" height="232" width="320" style="height: 232px; width: 320px;" class="media-element file-default" data-delta="5" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/TrilabTransfers_0-320x232.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>There are a number of ways to transfer files - depending upon what you want to do.</li>
<li><strong>hopper</strong> - A powerful, interactive, cross-platform tool that allows users to transfer and manipulate files and directories by means of a graphical user interface. Users can connect to and manage resources using most of the major file transfer protocols, including FTP, SFTP, SSH, NFT, and HTAR. See the hopper web pages ( <a href="/software/hopper">hpc.llnl.gov/software/hopper</a>), hopper man page or use the <span class="fixed">hopper -readme</span> command for more information.</li>
<li><strong>ftp</strong> - Is available for file transfer between LC machines. The ftp client at LC is an optimized parallel ftp implementation. It can be used to transfer files with machines outside LLNL if the command originates from an LLNL machine and the foreign host will permit it. FTP to LC machines from outside LLNL is not permitted unless the user is connected via an appropriate Remote Access service such as OTS or VPN. Documentation is available via the ftp man page or the FTP Usage Guide (<a href="/manuals/ezstorage/ftp">hpc.llnl.gov/manuals/ezstorage/ftp</a>)</li>
<li><strong>scp</strong> - (secure copy) is available on all LC machines. Example:<br /><span class="fixed">scp thisfile user@host2:thatfile</span></li>
<li><strong>sftp</strong> - Performs ftp-like operations over encrypted ssh.</li>
<li><strong>MyLC</strong> - Livermore Computing's user portal provides a mechanism for transferring files to/from your desktop machine and your home directory on an LC machine. See the "utilities" tab. Available at <a href="https://mylc.llnl.gov/" target="_blank">mylc.llnl.gov</a></li>
<li><strong>nft</strong> - (Network File Transfer) is LC's utility for persistent file transfer with job tracking. This is a command line utility that assumes transfers with storage and has a specific syntax. Documentation is available via its man page or the NFT Reference Manual (<a href="/manuals/ezstorage/nft">hpc.llnl.gov/manuals/ezstorage/nft</a>).</li>
<li><strong>htar</strong> - Is highly optimized for creation of archive files directly into HPSS, without having to go through the intermediate step of first creating the archive file on local disk storage, and then copying the archive file to HPSS via some other process such as ftp. The program uses multiple threads and a sophisticated buffering scheme in order to package member files into in-memory buffers, while making use of the high-speed network striping capabilities of HPSS. Syntax resembles that of the UNIX tar command. Documentation is available via its man page or the HTAR Reference Manual (<a href="/manuals/ezstorage/htar">hpc.llnl.gov/manuals/ezstorage/htar</a>).</li>
<li><strong>hsi</strong> - Hierarchical Storage Interface. HSI is a utility that communicates with HPSS via a user- friendly interface that makes it easy to transfer files and manipulate files and directories using familiar UNIX-style commands. HSI supports recursion for most commands as well as CSH-style support for wildcard patterns and interactive command line and history mechanisms. Documentation is available via its man page.</li>
<li><strong>Tri-lab high bandwidth file transfers over SecureNet:</strong>
<ul><li>All three Labs support wrapper scripts for enhanced data transfer between sites - classified side only.</li>
<li>Three different protocols can be used: <span class="fixed">hsi</span>, <span class="fixed">htar </span>and <span class="fixed">pftp</span>.</li>
<li>Transfers can be from host to storage or host to host</li>
<li>Commands are given names that are self-explanatory - see the accompanying image at right.</li>
<li>At LLNL, these scripts should already be in your path</li>
<li>For additional information please see <a href="https://aces.sandia.gov/tri_lab_home.html#file_xfer" target="_blank">aces.sandia.gov/tri_lab_home.html#file_xfer</a> (requires authentication)</li>
</ul></li>
</ul><h3><a name="system-status" id="system-status"></a><span>System Status and Configuration Information</span></h3>
<div>
<ul><li>Before you attempt to run your parallel application, it is important to know a few details about the way the system is configured. This is especially true at LC where every system is configured differently and where things change frequently.</li>
<li>It is also useful to know the status of the machines you intend on using. Are they available or down for maintenance?</li>
<li>System configuration and status information for all LC systems is readily available from the <a href="http://hpc.llnl.gov">LC Homepage</a><strong> </strong>and the <a href="http://mylc.llnl.gov" target="_blank">MyLC Portal</a>.</li>
</ul><table><tr><td><div class="media media-element-container media-default"><div id="file-2070" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/lchomepage-jpg-0">LChomepage.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/LChomepage_0.jpg"><img alt="screen shot of LC home page" height="225" width="351" style="height: 225px; width: 351px;" class="media-element file-default" data-delta="156" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/LChomepage_0-351x225.jpg" /></a>  </div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-2071--2" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mylchomepage-jpg-0">MYLChomepage.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/MYLChomepage_0.jpg"><img alt="screen shot of My LC home page" height="225" width="349" style="height: 225px; width: 349px;" class="media-element file-default" data-delta="157" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/MYLChomepage_0-349x225.jpg" /></a>  </div>

  
</div>
</div></td>
</tr></table><h3><a id="sys-config-info" name="sys-config-info"></a>System Configuration and Status Information</h3>
<ul><li>LC Homepage:
<ul><li><a href="https://hpc.llnl.gov">hpc.llnl.gov</a> (User Portal toggle) ==&gt; Hardware ==&gt; Compute Platforms</li>
<li>Direct link: <a href="/hardware/platforms">hpc.llnl.gov/hardware/platforms</a></li>
<li>All production systems appear in a summary table showing basic hardware information.</li>
<li>Diving on a machine's name will take you to a page of detailed hardware and configuration information for that machine.</li>
</ul></li>
<li>MyLC Portal:
<ul><li><a href="https://mylc.llnl.gov" target="_blank">mylc.llnl.gov</a></li>
<li>Click on a machine name in the "machine status" portlet, or the "my accounts" portlet.</li>
<li>Then select the "details", "topology" and/or "job limits" tabs for detailed hardware and configuration information.</li>
</ul></li>
<li>LC Tutorials:
<ul><li>Located on the LC Homepage under the "Training" menu.</li>
<li>Direct link: <a href="/training/tutorials">hpc.llnl.gov/training/tutorials</a></li>
</ul></li>
<li>Systems Summary Tables:
<ul><li>Systems Summary Table: <a href="/hardware/platforms">hpc.llnl.gov/hardware/platforms</a>. Concise summary of basic hardware information for LC systems.</li>
<li>LC Systems Summary: <a href="/sites/default/files/LC-systems-summary.pdf">hpc.llnl.gov/sites/default/files/LC-systems-summary.pdf</a>. Even more concise 1-page summary of LC production systems.</li>
</ul></li>
</ul><p> </p>
<h4>System Status Information</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1840" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/systemstatusmenu-png-0">systemStatusMenu.png</a></h2>
    
  
  <div class="content">
    <img alt="Screenshot of System Status dialog on User Portal home page" height="323" width="500" style="width: 500px; height: 323px;" class="media-element file-default" data-delta="132" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/systemStatusMenu_0.png" /></div>

  
</div>
</div></div>
<ul><li>LC Homepage:
<ul><li><a href="https://hpc.llnl.gov">hpc.llnl.gov</a> (User Portal toggle) - just look on the man page for the System Status links (shown at right).</li>
<li>The same links appear under the <a href="/hardware">Hardware</a> menu.</li>
<li>Unclassified systems only</li>
</ul></li>
<li>MyLC Portal:
<ul><li><a href="https://mylc.llnl.gov" target="_blank">mylc.llnl.gov</a></li>
<li>Several portlets provide system status information:
<ul><li>machine status</li>
<li>login node status</li>
<li>scratch file system status</li>
<li>enclave status</li>
</ul></li>
</ul></li>
<li>Machine status email lists:
<ul><li>Provide the most timely status information for system maintenance, problems, and system changes/updates</li>
<li>ocf-status covers all machines on the OCF</li>
<li>Additionally, each machine has its own status list - for example: <strong><a href="mailto:sierra-status@llnl.gov">sierra-status@llnl.gov</a></strong></li>
</ul></li>
<li>Login banner &amp; news items - always displayed immediately after logging in
<ul><li>Login banner includes basic configuration information, announcements and news items. Example <a href="/sites/default/files/loginBanner.txt">login banner HERE.</a></li>
<li>News items (unread) appear at the bottom of the login banner. For usage, type <span class="fixed">news -h</span>.</li>
</ul></li>
<li>Direct links for systems and file systems status pages:</li>
</ul><table class="table table-bordered table-striped"><tr><th scope="col">Description</th>
<th scope="col">Network</th>
<th scope="col">Links</th>
</tr><tr><td colspan="1" rowspan="1">System status web pages</td>
<td>OCF CZ</td>
<td><a href="https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" target="_blank">lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi</a></td>
</tr><tr><td colspan="1" rowspan="1">File systems status web pages</td>
<td>OCF CZ</td>
<td><a href="https://lc.llnl.gov/fsstatus/fsstatus.cgi" target="_blank">lc.llnl.gov/fsstatus/fsstatus.cgi</a></td>
</tr></table><h4>Examples</h4>
<table class="table table-bordered"><tr><td><a href="/sites/default/files/machineStatus1_1.gif"><div class="media media-element-container media-default"><div id="file-899" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/machinestatus1gif-1">machineStatus1.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/machineStatus1_1.gif"><img alt="CZ Machine Status" height="354" width="400" style="width: 400px; height: 354px;" class="media-element file-default" data-delta="77" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/machineStatus1_1-400x354.gif" /></a>  </div>

  
</div>
</div></a><br />CZ Machine Status</td>
<td><a href="/sites/default/files/lorenz3.gif"><div class="media media-element-container media-default"><div id="file-1841" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/lorenz3-gif">lorenz3.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/lorenz3.gif"><img alt="My LC user interface" height="357" width="400" style="height: 357px; width: 400px;" class="media-element file-default" data-delta="133" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/lorenz3-400x357.gif" /></a>  </div>

  
</div>
</div></a><br />mylc.llnl.gov</td>
</tr></table><p> </p>
<h2><a id="development-environment" name="development-environment"></a>Software Overview</h2>
<h3><a name="deg" id="deg"></a>Development Environment Group (DEG)</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1843" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/degstaff-jpg">DEGstaff.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/DEGstaff.jpg"><img alt="Development Environment Group" height="303" width="500" style="height: 303px; width: 500px;" class="media-element file-default" data-delta="135" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/DEGstaff-500x303.jpg" /></a>  </div>

  
</div>
</div></div>
<ul><li>LC's Development Environment Group (DEG) provides a stable, usable, leading-edge parallel application development environment that enables users to improve the reliability and scalable performance of LLNL applications.</li>
<li>DEG installs and supports the following LC software:
<ul><li>Compilers and Preprocessors</li>
<li>Debuggers</li>
<li>Memory Tools</li>
<li>Profiling Tools</li>
<li>Tracing Tools</li>
<li>Performance Analysis</li>
<li>Correctness Tools</li>
<li>Utilities</li>
</ul></li>
<li>Additionally, DEG's mission includes:
<ul><li>Working to make computing tools reliable, scalable and to help users make effective use of these tools.</li>
<li>Partnering with its application development user community to identify user requirements and evaluate tool effectiveness.</li>
<li>Collaborating with vendors and other third party software developers to ensure a complete environment in the most cost effective way possible and meet the needs of today's code developers utilizing emerging technologies.</li>
</ul></li>
<li>DEG Home Page: <a href="http://computing.llnl.gov/livermore-computing/development-environment-group" target="_blank">computing.llnl.gov/livermore-computing/development-environment-group</a></li>
</ul><h3><a name="toss" id="toss"></a>TOSS Operating System</h3>
<ul><li>TOSS = <strong>T</strong>ri-Laboratory <strong>O</strong>perating <strong>S</strong>ystem <strong>S</strong>tack</li>
<li>Based on Red Hat Enterprise Linux (RHEL) with modifications to support targeted HPC hardware and cluster computing</li>
<li>Used by most LC (and Tri-lab) production Linux clusters:
<ul><li>For Blue Gene systems, the login nodes use TOSS, but the compute nodes run a special Linux-like Compute Node Kernel (CNK).</li>
<li>CORAL EA and Sierra clusters use a "TOSS-like" OS/software stack, called <em><strong>blueos </strong></em>by LC.</li>
</ul></li>
<li>The primary components of TOSS include:
<ul><li>RHEL kernel optimized for large scale cluster computing</li>
<li>OpenFabrics Enterprise Distribution InfiniBand software stack including MVAPICH and OpenMPI libraries</li>
<li>Slurm workload manager</li>
<li>Integrated Lustre and Panasas parallel file system software</li>
<li>Scalable cluster administration tools</li>
<li>Cluster monitoring tools</li>
<li>GNU, C, C++ and Fortran90 compilers (GNU, Intel, PGI)</li>
<li>Testing software framework for hardware and operating system validation</li>
</ul></li>
<li>See <a href="http://www.redhat.com/" target="_blank">Redhat's documentation</a> for details on the RHEL kernel.</li>
<li>Version information for LC's clusters:
<ul><li>TOSS: <span class="fixed">distro_version</span> or cat <span class="fixed">/etc/toss-release</span></li>
<li>Redhat: <span class="fixed">cat /etc/redhat-release</span></li>
</ul></li>
</ul><h3><a name="software-lists" id="software-lists"></a>Software Lists, Documentation, and Downloads</h3>
<p>The table below lists and provides links to the majority of software available through LC and related organizations.</p>
<table class="table table-striped table-bordered"><tr><th scope="col">Software Category</th>
<th scope="col">Description and More Information</th>
</tr><tr><td>Compilers</td>
<td>Lists which compilers are available for each LC system: <a href="/software/development-environment-software/compilers">hpc.llnl.gov/software/development-environment-software/compilers</a></td>
</tr><tr><td>Supported Software and Computing Tools</td>
<td>Development Environment Group supported software includes compilers, libraries, debugging, profiling, trace generation/visualization, performance analysis tools, correctness tools, and several utilities: <a href="/software/development-environment-software">hpc.llnl.gov/software/development-environment-software.</a></td>
</tr><tr><td>Graphics Software</td>
<td>Graphics Group supported software includes visualization tools, graphics libraries, and utilities for the plotting and conversion of data: <a href="/data-vis/vis-software">hpc.llnl.gov/data-vis/vis-software</a></td>
</tr><tr><td>Mathematical Software Overview</td>
<td>Lists and describes the primary mathematical libraries and interactive mathematical tools available on LC machines: <a href="/software/mathematical-software">hpc.llnl.gov/software/mathematical-software</a></td>
</tr><tr><td>LINMath</td>
<td>The Livermore Interactive Numerical Mathematical Software Access Utility, is a Web-based access utility for math library software. The LINMath Web site also has pointers to packages available from external sources: <a href="https://www-lc.llnl.gov/linmath/" target="_blank">www-lc.llnl.gov/linmath/</a></td>
</tr><tr><td>Center for Applied Scientific Computing (CASC) Software</td>
<td>A wide range of software available for download from LLNL's CASC. Includes mathematical software, language tools, PDE software frameworks, visualization, data analysis, program analysis, debugging, and benchmarks: <a href="https://computing.llnl.gov/hpc/software" target="_blank">computing.llnl.gov/hpc/software</a>, <a href="https://computing.llnl.gov/projects" target="_blank">computing.llnl.gov/projects</a></td>
</tr><tr><td>LLNL Software Portal</td>
<td>Lab-wide portal of software repositories: <a href="https://software.llnl.gov/" target="_blank">software.llnl.gov/</a></td>
</tr></table><h3><a name="modules" id="modules"></a>Modules</h3>
<ul><li>Most LC clusters support Lmod modules for software packaging:
<ul><li>Provide a convenient, uniform way to select among multiple versions of software installed on LC systems.</li>
<li>Many LC software applications require that you load a particular "package" in order to use the software.</li>
</ul></li>
<li>Using Modules:</li>
</ul><pre>List available modules:     module avail
Load a module:              module add|load <em>modulefile</em>
Unload a module:            module rm|unload <em>modulefile</em>
List loaded modules:        module list
Read module help info:      module
Display module contents:    module display|show <em>modulefile</em></pre><ul><li>For more information see:
<ul><li><a href="/software/modules-and-software-packaging">LC modules documentation</a></li>
<li><a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod" target="_blank">TACC documentation: LMOD: ENVIRONMENTAL MODULES SYSTEM</a></li>
<li>The module man page</li>
</ul></li>
</ul><h3><a name="atlassian" id="atlassian"></a>Atlassian Tools - Confluence, JIRA, etc.</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-902" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/atlassianjpeg">atlassian.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/atlassian.jpeg"><img alt="Atlassian tools" height="149" width="225" style="width: 225px; height: 149px;" class="media-element file-default" data-delta="80" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/atlassian-225x149.jpeg" /></a>  </div>

  
</div>
</div></div>
<ul><li>LC supports a suite of web-based collaboration tools from Atlassian:
<ul><li><strong>Confluence Wiki:</strong> used for documentation, collaboration, knowledge sharing, file sharing, mockups, diagrams... anything you can put on a webpage.</li>
<li><strong>JIRA:</strong> issue tracking and project management system</li>
<li><strong>Bitbucket:</strong> for git repository hosting. Similar to popular sites like GitHub and Bitbucket, but it is intended for internal use on intranets.</li>
<li><strong>Bamboo:</strong> a continuous integration and delivery tool that combines automated builds, tests, and releases in a single workflow.</li>
</ul></li>
<li>All three collaboration tools:
<ul><li>Are based on LC usernames / groups and are intended to foster collaboration between LC users working on HPC projects.</li>
<li>Require authentication with your LC username and RSA PIN + token</li>
<li>Have a User Guide for usage information</li>
</ul></li>
<li>Locations:</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">Network</th>
<th scope="col">Confluence Wiki</th>
<th scope="col">JIRA</th>
<th scope="col">Bitbucket</th>
</tr><tr><td>CZ</td>
<td><a href="https://lc.llnl.gov/confluence/" target="_blank">lc.llnl.gov/confluence/</a></td>
<td><a href="https://lc.llnl.gov/jira/" target="_blank">lc.llnl.gov/jira/</a></td>
<td><a href="https://lc.llnl.gov/stash/" target="_blank">lc.llnl.gov/bitbucket/</a></td>
</tr><tr><td>RZ</td>
<td><a href="https://rzlc.llnl.gov/confluence/" target="_blank">rzlc.llnl.gov/confluence/</a></td>
<td><a href="https://rzlc.llnl.gov/jira/" target="_blank">rzlc.llnl.gov/jira/</a></td>
<td><a href="https://rzlc.llnl.gov/stash/" target="_blank">rzlc.llnl.gov/bitbucket/</a></td>
</tr><tr><td>SCF</td>
<td>lc.llnl.gov/confluence/</td>
<td>lc.llnl.gov/jira/</td>
<td>lc.llnl.gov/bitbucket/</td>
</tr></table><h3><a name="spack" id="spack"></a>Spack Package Manager</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-2072" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/spack-png">spack.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/spack.png"><img alt="Spack logo" height="78" width="225" style="height: 78px; width: 225px;" class="media-element file-default" data-delta="158" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/spack-225x78.png" /></a>  </div>

  
</div>
</div></div>
<ul><li>Spack is a flexible package manager for HPC</li>
<li>Easy to download and install. For example:</li>
</ul><pre>% git clone https://github.com/spack/spack
% . spack/share/spack/setup-env.csh (or setup-env.sh)</pre><ul><li>There is an increasing number of software packages (over 4,200 as of May 2020) available for installation with Spack. Many open source contributions from the international community.</li>
<li>To view available packages: <span class="fixed">spack list</span></li>
<li>Then, to install a desired package: <span class="fixed">spack install <em>packagename</em></span></li>
<li>Additional Spack features:
<ul><li>Allows installations to be customized. Users can specify the version, build compiler, compile-time options, and cross-compile platform, all on the command line.</li>
<li>Allows dependencies of a particular installation to be customized extensively.</li>
<li>Non-destructive installs - Spack installs every unique package/dependency configuration into its own prefix, so new installs will not break existing ones.</li>
<li>Creation of packages is made easy.</li>
</ul></li>
<li>Extensive documentation is available at: <a href="https://spack.readthedocs.io" target="_blank">spack.readthedocs.io</a></li>
</ul><h2><a name="compilers" id="compilers"></a>Compilers</h2>
<h3><a id="compilers-commands" name="compilers-commands"></a>Available Compilers and Invocation Commands - LC x86 systems</h3>
<ul><li>The table below summarizes compiler availability and invocation commands on LC <strong>Linux clusters</strong>.</li>
<li>For <strong>Sierra</strong> and <strong>CORAL EA</strong> compiler information, please see: <a href="/training/tutorials/using-lcs-sierra-system#compilers">hpc.llnl.gov/training/tutorials/using-lcs-sierra-system#compilers</a></li>
<li><strong>Note that parallel compiler commands are actually LC scripts that ultimately invoke the corresponding serial compiler. It is important to use the compiler wrapper scripts because they capture the correct libraries.</strong></li>
<li>For details on the MPI parallel compiler commands, see <a href="https://computing.llnl.gov/tutorials/mpi/">https://computing.llnl.gov/tutorials/mpi/</a></li>
</ul><table class="table table-striped table-bordered"><tr><th colspan="4" scope="col">Linux Cluster Compilers</th>
</tr><tr><td colspan="2" rowspan="1"><strong>Compiler</strong></td>
<td><strong>Serial Command</strong></td>
<td><strong>Parallel Commands</strong></td>
</tr><tr><td colspan="1" rowspan="3">Intel</td>
<td>C</td>
<td><span class="fixed">icc</span></td>
<td><span class="fixed">mpicc</span></td>
</tr><tr><td>C++</td>
<td><span class="fixed">icpc</span></td>
<td><span class="fixed">mpicxx, mpic++</span></td>
</tr><tr><td>Fortran</td>
<td><span class="fixed">ifort</span></td>
<td><span class="fixed">mpif77, mpif90, mpifort</span></td>
</tr><tr><td colspan="1" rowspan="3">GNU</td>
<td>C</td>
<td><span class="fixed">gcc</span></td>
<td><span class="fixed">mpicc</span></td>
</tr><tr><td>C++</td>
<td><span class="fixed">g++</span></td>
<td><span class="fixed">mpicxx, mpic++</span></td>
</tr><tr><td>Fortran</td>
<td><span class="fixed">gfortran</span></td>
<td><span class="fixed">mpif77, mpif90, mpifort</span></td>
</tr><tr><td colspan="1" rowspan="3">PGI</td>
<td>C</td>
<td><span class="fixed">pgcc</span></td>
<td><span class="fixed">mpipgcc, mpicc</span></td>
</tr><tr><td>C++</td>
<td><span class="fixed">pgc++</span></td>
<td><span class="fixed">mpicxx, mpic++</span></td>
</tr><tr><td>Fortran</td>
<td><span class="fixed">pgf77, pgf90, pgfortran</span></td>
<td><span class="fixed">mpif77, mpif90, mpifort</span></td>
</tr><tr><td colspan="1" rowspan="2">LLVM/Clang</td>
<td>C</td>
<td><span class="fixed">clang</span></td>
<td><span class="fixed">mpicc</span></td>
</tr><tr><td>C++</td>
<td><span class="fixed">clang++</span></td>
<td><span class="fixed">mpicxx, mpic++</span></td>
</tr></table><h3><a name="compilers-versions" id="compilers-versions"></a>Compiler Versions and Defaults</h3>
<ul><li>LC maintains multiple versions of each compiler.</li>
</ul><ul><li>The <a href="#modules">Modules</a> <span class="fixed">module avail</span> command is used to list available compilers and versions:</li>
</ul><p><span class="fixed">    module avail intel<br />    module avail gcc<br />    module avail pgi<br />    module avail clang</span></p>
<ul><li>Versions: to determine the actual version you are using, issue the compiler invocation command with its "version" option. For example:</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">Compiler</th>
<th scope="col">Option</th>
<th scope="col">Example</th>
</tr><tr><td>Intel</td>
<td><span class="fixed">version</span></td>
<td><span class="fixed">ifort --version</span></td>
</tr><tr><td>GNU</td>
<td><span class="fixed">version</span></td>
<td><span class="fixed">g++ --version</span></td>
</tr><tr><td>PGI</td>
<td><span class="fixed">-V</span></td>
<td><span class="fixed">pgf90 -V</span></td>
</tr><tr><td>Clang</td>
<td><span class="fixed">--version</span></td>
<td><span class="fixed">clang --version</span></td>
</tr></table><ul><li>To use an alternate version issue the Modules command: <span class="fixed">module load <em>module-name</em></span></li>
</ul><h3><a name="compilers-options" id="compilers-options"></a>Compiler Options</h3>
<ul><li>Each compiler has hundreds of options that determine what the compiler does and how it behaves.</li>
<li>The options used by one compiler mostly differ from other compilers.</li>
<li>Additionally, compilers have different default options.</li>
<li>An in-depth discussion of compiler options is beyond the scope of this tutorial.</li>
<li>See the compiler's documentation, man pages, and/or <span class="fixed">-help</span> or <span class="fixed">--help</span> option for details.</li>
</ul><h3><a name="compilers-docs" id="compilers-docs"></a>Compiler Documentation</h3>
<ul><li>Intel and PGI: compiler docs are included in the <span class="fixed">/opt/</span><em><span class="fixed">compilername</span></em><span> directory</span>. Otherwise, see Intel or PGI web pages.</li>
<li>GNU: see the web pages at <a href="https://gcc.gnu.org/" target="_blank">gcc.gnu.org/</a></li>
<li>LLVM/Clang: see the web pages at <a href="http://clang.llvm.org/docs/" target="_blank">clang.llvm.org/docs/</a></li>
<li>Man pages may/may not be available</li>
</ul><h2><a id="compilers-commands-CORAL" name="compilers-commands-CORAL"></a>Available Compilers and Invocation Commands - LC CORAL Systems</h2>
<h3>Available Compilers</h3>
<p>The following compilers are available on Sierra systems, and are discussed in detail below, along with other relevant compiler related information:</p>
<ul><li>XL: IBM's XL C/C++ and Fortran compilers</li>
<li>Clang: IBM's C/C++ clang compiler</li>
<li>GNU: GNU compiler collection, C, C++, Fortran</li>
<li>PGI: Portland Group compilers</li>
<li>NVCC: NVIDIA's C/C++ compiler</li>
</ul><h3>Compiler Recommendations</h3>
<p>The recommended and supported compilers are those delivered from IBM (XL and Clang ) and NVIDIA (NVCC):</p>
<ul><li>Only XL and Clang compilers from IBM provide OpenMP 4.5 with GPU support.</li>
<li>NVCC offers direct CUDA support</li>
<li>The IBM xlcuf compiler also provides direct CUDA support</li>
</ul><p>Please report all problems to the you may have with these to the LC Hotline so that fixes can be obtained from IBM and NVIDIA.</p>
<p>The other available compilers (GNU and PGI) can be used for experimentation and for comparisons to the IBM compilers:</p>
<ul><li>Versions installed at LC do not provide Open 4.5 with GPU support</li>
<li>If you experience problems with the PGI compilers, LC can forward those issues to PGI.</li>
<li>Using OpenACC on LC's Sierra clusters is not recommended nor supported.</li>
</ul><h3>Wrappers Scripts</h3>
<ul><li>LC has created wrappers for most compiler commands, both serial and MPI versions.</li>
<li>The wrappers perform LC customization and error checking. They also follow a string of links, which include other wrappers.</li>
<li>The wrappers located in /usr/tce/bin (in your PATH) will always point (symbolic link) to the default versions.</li>
</ul><p><span class="note-purple">Note </span>There may also be versions of the serial compiler commands in /usr/bin. Do not use these, as they are missing the LC customizations.</p>
<ul><li>If you load a different module version, your PATH will change, and the location may then be in either /usr/tce/bin or /usr/tcetmp/bin.</li>
<li>To determine the actual location of the wrapper, simply use the command which compilercommand to view its path.</li>
</ul><p>Example: show location of default/current xlc wrapper, load a new version, and show new location:</p>
<pre>% which xlc /usr/tce/packages/xl/xl-2019.02.07/bin/xlc % module load xl/2019.04.19 Due to MODULEPATH changes the following have been reloaded: 1) spectrum-mpi/rolling-release The following have been reloaded with a version change: 1) xl/2019.02.07 =&gt; xl/2019.04.19 % which xlc /usr/tce/packages/xl/xl-2019.04.19/bin/xlc</pre><h2><a name="debuggers" id="debuggers"></a>Debuggers</h2>
<ul><li><span class="note-red">Note</span> Please consult the "Supported Software and Computing Tools" web page located at <a href="/software">hpc.llnl.gov/software</a>.</li>
</ul><h3><a name="total-view" id="total-view"></a>TotalView</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-951" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/totalview-small-gif">totalview.small_.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/totalview.small_.gif"><img alt="TotalView" height="328" width="350" style="width: 350px; height: 328px;" class="media-element file-default" data-delta="81" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/totalview.small_-350x328.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>TotalView is probably the most widely used debugger for parallel programs. It can be used with C/C++ and Fortran programs and supports all common forms of parallelism, including pthreads, openMP, MPI, accelerators and GPUs.</li>
<li>Starting TotalView for serial codes: simply issue the command:</li>
</ul><p><span class="fixed">        totalview <em>myprog</em></span></p>
<ul><li>Starting TotalView for interactive parallel jobs:
<ul><li>Some special command line options are required to run a parallel job through TotalView under SLURM. You need to run <span class="fixed">srun</span> under TotalView, and then specify the <span class="fixed">-a</span> flag followed by 1) srun options, 2) your program, and 3) your program flags (in that order). The general syntax is: <span class="fixed">totalview srun -a -n #processes -p pdebug myprog [prog args]</span></li>
<li>To debug an already running interactive parallel job, simply issue the <span class="fixed">totalview</span> command and then attach to the srun process that started the job.</li>
<li>Debugging batch jobs is covered in LC's <a href="https://computing.llnl.gov/tutorials/totalview/" target="_blank">TotalView tutorial</a> and in the "Debugging in Batch" section below.</li>
</ul></li>
<li>Documentation:
<ul><li><a href="https://computing.llnl.gov/tutorials/totalview/" target="_blank">LC Tutorial</a></li>
<li>Vendor website: <a href="http://www.roguewave.com/" target="_blank">www.roguewave.com/</a></li>
</ul></li>
</ul><h3><a name="ddt" id="ddt"></a>DDT</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-952" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/ddt-small-gif">ddt.small_.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/ddt.small_.gif"><img alt="DDT" height="270" width="350" style="width: 350px; height: 270px;" class="media-element file-default" data-delta="82" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/ddt.small_-350x270.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>DDT stands for "Distributed Debugging Tool", a product of Allinea Software Ltd.</li>
<li>DDT is a comprehensive graphical debugger designed specifically for debugging complex parallel codes. It is supported on a variety of platforms for C/C++ and Fortran. It is able to be used to debug multi-process MPI programs, and multi-threaded programs, including OpenMP.</li>
<li>Currently, LC has a limited number of fixed and floating licenses.</li>
<li>Usage information: see LC's DDT Quick Start information located at: <a href="/software/development-environment-software/allinea-ddt">hpc.llnl.gov/software/development-environment-software/allinea-ddt</a></li>
<li>Documentation: see the vendor website: <a href="http://www.allinea.com" target="_blank">www.allinea.com</a></li>
</ul><h3><a name="stat" id="stat"></a>STAT - Stack Trace Analysis Tool</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-953" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/stat1-gif">STAT1.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/STAT1.gif"><img alt="STAT" height="252" width="350" style="width: 350px; height: 252px;" class="media-element file-default" data-delta="83" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/STAT1-350x252.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>The Stack Trace Analysis Tool gathers and merges stack traces from a parallel application's processes.</li>
<li>STAT is particularly useful for debugging hung programs.</li>
<li>It produces call graphs: 2D spatial and 3D spatial-temporal
<ul><li>The 2D spatial call prefix tree represents a single snapshot of the entire application (see image).</li>
<li>The 3D spatial-temporal call prefix tree represents a series of snapshots from the application taken over time.</li>
</ul></li>
<li>More information: see the STAT web page at: <a href="https://hpc.llnl.gov/software/development-environment-software/stat-stack-trace-analysis-tool">https://hpc.llnl.gov/software/development-environment-software/stat-stack-trace-analysis-tool</a></li>
</ul><h3><a name="debuggers-other" id="debuggers-other"></a>Other Debuggers</h3>
<ul><li>Several other common debuggers are available on LC Linux clusters, though they are not recommended for parallel programs when compared to TotalView and DDT.</li>
<li>PGDBG: the Portland Group Compiler debugger. Documentation: <a href="https://www.pgroup.com/products/pgdbg.htm" target="_blank">www.pgroup.com/products/pgdbg.htm</a></li>
<li>GDB: GNU GDB debugger, a command-line, text-based, single process debugger. Documentation: <a href="http://www.gnu.org/software/gdb" target="_blank">www.gnu.org/software/gdb</a></li>
<li>DDD: GNU DDD debugger is a graphical front-end for command-line debuggers such as GDB, DBX, WDB, Ladebug, JDB, XDB, the Perl debugger, the bash debugger, or the Python debugger. Documentation: <a href="http://www.gnu.org/software/ddd" target="_blank">www.gnu.org/software/ddd</a></li>
</ul><h2><a name="performance-analysis" id="performance-analysis"></a>Performance Analysis Tools</h2>
<ul><li>Consult the  "Development Environment Software" web pages at:  <a href="/software/development-environment-software">hpc.llnl.gov/software/development-environment-software</a> for what may be available here. Some example tools are listed below.</li>
</ul><h3><a name="mem-correctness" id="mem-correctness"></a>Memory Correctness Tools</h3>
<p><strong>Memcheck: </strong>Valgrind's Memcheck tool detects a comprehensive set of memory errors, including reads and writes of unallocated or freed memory and memory leaks.</p>
<p><strong>TotalView: </strong>Allows you to stop execution when heap API problems occur, list memory leaks, paint allocated and deallocated blocks, identify dangling pointers, hold onto deallocated memory, graphically browse the heap, identify the source line and stack backtrace of an allocation or deallocation, summarize heap use by routine, filter and dump heap information, and review memory usage by process or by library.</p>
<p><strong>memP: </strong>The memP tool provides heap profiling through the generation of two reports: a summary of the heap high-water-mark across all processes in a parallel job as well as a detailed task-specific report that provides a snapshot of the heap memory currently in use, including the amount allocated at specific call sites.</p>
<p><strong>Intel Inspector: </strong>Primarily a thread correctness tool, but memory debugging features are included.</p>
<h3><a name="profiling" id="profiling"></a>Profiling, Tracing, and Performance Analysis</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-954" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/perfanalysis-jpg">perfAnalysis.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/perfAnalysis.jpg"><img alt="perf Analysis" height="256" width="400" style="width: 400px; height: 256px;" class="media-element file-default" data-delta="84" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/perfAnalysis-400x256.jpg" /></a>  </div>

  
</div>
</div></div>
<p><strong>Open|SpeedShop: </strong>Open|SpeedShop is a comprehensive performance tool set with a unified look and feel that covers most important performance analysis steps. It offers various different interfaces, including a flexible GUI, a scripting interface, and a Python class. Supported experiments include profiling using PC sampling, inclusive and exclusive execution time analysis, hardware counter support, as well as MPI, I/O, and floating point exception tracing. All analysis is applied on unmodified binaries and can be used on codes with MPI and/or thread parallelism.</p>
<p><strong>TAU: </strong>TAU is a robust profiling and tracing tool from the University of Oregon that includes support for MPI and OpenMP. TAU provides an instrumentation API, but source code can also be automatically instrumented and there is support for dynamic instrumentation as well. Experienced users have applied the tool with good results at LLNL. TAU can be configured with many feature combinations.</p>
<p><strong>HPCToolkit: </strong>HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the largest supercomputers. It uses low overhead statistical sampling of timers and hardware performance counters to collect accurate measurements of a program's work, resource consumption, and inefficiency and attributes them to the full calling context in which they occur. HPCToolkit works with C/C++/Fortran applications that are either statically or dynamically linked. It supports measurement and analysis of serial codes, threaded codes (pthreads, OpenMP), MPI, and hybrid (MPI + threads) parallel codes.</p>
<p><strong>mpiP:</strong> A lightweight MPI profiling library that provides time spent in MPI functions by callsite and stacktrace. This tool is developed and maintained at LLNL, so support and modifications can be quickly addressed. New run-time functionality can be used to generate mpiP data without relinking through the srun-mpip and poe-mpip scripts on Linux and AIX systems.</p>
<p><strong>gprof: </strong> Displays call graph profile data. The gprof command is useful in identifying how a program consumes CPU resources. Gprof does simple function profiling and requires that the code be built and linked with -pg. For parallel programs, in order to get a unique output file for each process, you will need to set the undocumented environment variable GMON_OUT_PREFIX to some non-null string. For example: <span class="fixed">setenv GMON_OUT_PREFIX 'gmon.out.'`/bin/uname -n`</span></p>
<p><strong>pgprof</strong>: PGI profiler - pgprof is a tool which analyzes data generated during execution of specially compiled programs. This information can be used to identify which portions of a program will benefit the most from performance tuning.</p>
<p><strong>PAPI: </strong>Portable hardware performance counter library.</p>
<p><strong>PapiEx: </strong>A PAPI-based performance profiler that measures hardware performance events of an application without having to instrument the application.</p>
<p><strong>VTune Amplifier: </strong>The Intel VTune Amplifier tool is a performance analysis tool for finding hotspots in serial and multithreaded codes. Note the installation on LC machines does not include the advanced hardware analysis capabilities.</p>
<p><strong>Intel Profiler: </strong>The Intel Profiler tool is built into the Intel compiler along with a simple GUI to display the collected results.</p>
<p><strong>Vampir / Vampirtrace: </strong>Full featured trace file visualizer and library for generating trace files for parallel programs.</p>
<h2 class="tutorial-heading">PART II: <a name="running-jobs" id="running-jobs"></a>Running Jobs</h2>
<div>Note: Much of the following sections focus on SLURM (and, to some extent, Moab) commands. While SLURM and Moab commands will work on our CTS systems, our Sierra systems use LSF. Please see the <a href="https://hpc.llnl.gov/banks-jobs/running-jobs/lsf-quick-start-guide">LSF Quickstart</a> and <a href="https://hpc.llnl.gov/banks-jobs/running-jobs/batch-system-commands">this reference</a> for converting between Slurm, Moab, and LSF commands.</div>
<h2><a id="BasicConcepts" name="BasicConcepts"></a>Basic Concepts</h2>
<h3><a id="Jobs" name="Jobs"></a>Jobs</h3>
</div>
<h4><span class="heading3">Simple Definition</span></h4>
<ul><li>To a user, a job can be simply described as a request for compute resources needed to perform computational work.</li>
<li>Jobs typically specify what resources are needed, such as:
<ul><li>Which cluster</li>
<li>Number of nodes</li>
<li>Length of time</li>
<li>Queue to use</li>
<li>Account to charge</li>
<li>...</li>
</ul></li>
<li>Jobs are typically submitted to the Workload Manager by means of a job script. Job scripts are discussed in the <a href="#JobScript">Building a Job Script</a> section.</li>
</ul><h4><span class="heading3">Interactive vs Batch</span></h4>
<h5>Batch</h5>
<ul><li>Refers to jobs that you submit to the workload manager via a job script.</li>
<li>After submitting, you typically wait for your job to be scheduled and then run; typically no command line interacting via stdin, stdout.</li>
<li>The vast majority of jobs on LC clusters are batch jobs.</li>
<li>Most of the remaining material in this tutorial applies to batch jobs.</li>
</ul><h5>Interactive</h5>
<ul><li>Refers to jobs that you launch and interact with real-time from the command line (stdin, stdout).</li>
<li>Do not require a job script</li>
<li>Most often (but not always) used for small, short, debugging / testing runs.</li>
<li>Interactive jobs are discussed further in the: <a href="#Interactive">Interactive Jobs</a> section.</li>
</ul><h4><span class="heading3">Serial vs Parallel</span></h4>
<ul><li>Parallel jobs can range in size from a single node (multi-core) to the full system.</li>
<li>All but a few of LC's production clusters are intended to be used for parallel jobs.</li>
<li>Serial jobs by definition require only one core on a node.</li>
<li>Because running serial jobs on parallel clusters would waste compute resources, LC provides several clusters where serial jobs can be run without wasting compute resources.</li>
</ul><h3><a id="QueueLimits" name="QueueLimits"> </a>Queues and Queue Limits</h3>
<h4><span class="heading3">Queues (also called Pools and/or Partitions)</span></h4>
<ul><li>The majority of nodes on LC's production systems are designated as compute nodes.</li>
<li>Compute nodes are typically divided into queues / pools / partitions based upon how they should be used.</li>
</ul><h5>Batch queue</h5>
<ul><li>Typically comprises most of the compute nodes on a system</li>
<li>Named pbatch</li>
<li>Intended for production work</li>
<li>Configured on all but a few LC systems</li>
</ul><h5>Interactive/debug queue</h5>
<ul><li>Typically comprises a small number of the compute nodes on a system</li>
<li>Named pdebug</li>
<li>Intended for small, short-running interactive and debugging work (not production)</li>
<li>Configured on most LC systems</li>
</ul><h5>OTHER QUEUES</h5>
<ul><li>There may be other queues on some machines, such as the viz, pviz, pgpu, etc.</li>
<li>There are defined limits for each queue, the most important being:
<ul><li>Max/min number of nodes permitted for a job</li>
<li>Max wall-clock time - how long a job is permitted to run</li>
<li>Max number of simultaneous running jobs or max number of nodes permitted across all running jobs.</li>
</ul></li>
<li>No two LC systems have the same queue limits.</li>
<li>Queue limits can and do change!</li>
<li>Notes:
<ul><li>To run jobs that exceed queue limits, users can request Dedicated Application Time (DAT).</li>
<li>Login nodes are a shared, limited resource not intended for production work. They are not associated with any queue.</li>
</ul></li>
</ul><h4><span class="heading3">How Do I Find Out What the Queue Limits Are?</span></h4>
<ul><li>The easiest way to determine the queue configuration and limits for a particular machine is to login to that machine and use the command:
<pre><span class="cmd">news job.lim.quartz</span></pre></li>
<li>This is the same information available in the LC "Machine Status" web pages: (LC internal). Click on the machine name of interest and then look for the "Job Limits" link.<br /><ul type="circle"><li>OCF-CZ: <a href="https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" target="blank">https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi</a></li>
</ul></li>
<li>It is also available on the "MyLC" web pages. Just click on any machine name in the "machine status" or "my accounts" portlets. Then select the "job limits" tab.
<ul><li>OCF-CZ: <a href="https://mylc.llnl.gov" target="_blank">mylc.llnl.gov</a></li>
</ul></li>
<li>For convenience, the queue limits for all of LC's production clusters are summarized below, as of June 2019. All information is subject to change.</li>
</ul><h3><a name="Banks" id="Banks"> </a>Banks</h3>
<h4> <span class="heading3">Bank Hierarchy</span></h4>
<ul><li><strong>In order to run in a queue, whether batch or interactive on a cluster, you need to have two things:</strong>
<ul><li>Login account: your username appears in /etc/passwd</li>
<li>LC bank: your username is associated with a valid LC bank</li>
</ul></li>
</ul><h2><a id="BasicFunctions" name="BasicFunctions"></a>Basic Functions</h2>
<h3><a id="JobScript" name="JobScript"></a>Building a Job Script</h3>
<h4><span class="heading3">The Basics</span></h4>
<ul><li>Users submit jobs to the Workload Manager for scheduling by means of a job script.</li>
<li>A job script is a plain text file that you create with your favorite editor.</li>
<li>Job scripts can include any/all of the following:
<ul><li>Commands, directives and syntax specific to a given batch system</li>
<li>Shell scripting</li>
<li>References to environment variables</li>
<li>Names of executable(s) to run</li>
<li>Comment lines and white space</li>
</ul></li>
<li>Simple Slurm and Moab job control scripts appear below:<br /><table><tr><th>Slurm</th>
<th>Moab</th>
</tr><tr><td>
<pre>#!/bin/tcsh
##### These lines are for Slurm
#SBATCH -N 16
#SBATCH -J parSolve34
#SBATCH -t 2:00:00
#SBATCH -p pbatch
#SBATCH --mail-type=ALL
#SBATCH -A myAccount
#SBATCH -o /p/luster1/joeuser/par_solve/myjob.out

##### These are shell commands
date
cd /p/luster1/joeuser/par_solve
##### Launch parallel job using srun
srun -n128 a.out
echo 'Done'
</pre></td>
<td>
<pre>#!/bin/tcsh
##### These lines are for Moab
#MSUB -l nodes=16
#MSUB -N parSolve34
#MSUB -l walltime=2:00:00
#MSUB -q pbatch
#MSUB -m be
#MSUB -A myAccount
#MSUB -o /p/luster1/joeuser/par_solve/myjob.out

##### These are shell commands
date
cd /p/luster1/joeuser/par_solve
##### Launch parallel job using srun
srun -n128 a.out
echo 'Done'
</pre></td>
</tr></table></li>
</ul><h4><span class="heading3">Options</span></h4>
<ul><li>There are a wide variety of options that can be used in your job script. Some of the more common/useful options are shown below.</li>
<li>
<table class="table"><tr><th>Slurm</th>
<th>Moab</th>
<th>Description/Notes</th>
</tr><tr><td>
<pre>#SBATCH -A account</pre></td>
<td>
<pre>#MSUB -A account</pre></td>
<td>Defines the account (bank) associated with the job.</td>
</tr><tr><td>
<pre>#SBATCH --begin=time</pre></td>
<td>
<pre>#MSUB -a time</pre></td>
<td>Declares the time after which the job is eligible for execution. See man page for syntax.</td>
</tr><tr><td>
<pre>#SBATCH -c #</pre></td>
<td>
<pre>
 </pre></td>
<td>cpus/cores per task</td>
</tr><tr><td>
<pre>#SBATCH -d list</pre></td>
<td>
<pre>#MSUB -l depend=list</pre></td>
<td>Specify job dependencies. </td>
</tr><tr><td>
<pre>#SBATCH -D path</pre></td>
<td>
<pre>#MSUB -d path</pre></td>
<td>Specifies the directory in which the job should begin executing.</td>
</tr><tr><td>
<pre>#SBATCH -e filename</pre></td>
<td>
<pre>#MSUB -e filename</pre></td>
<td>Specifies the file name to be used for stderr.</td>
</tr><tr><td>
<pre>#SBATCH --export=list</pre></td>
<td>
<pre>#MSUB -v list</pre></td>
<td>Specifically adds a list (comma separated) of environment variables that are exported to the job.</td>
</tr><tr><td>
<pre>#SBATCH --license=filesystem</pre><p><br />The default is to require no Lustre file systems</p></td>
<td>
<pre>#MSUB -l gres=filesystem
#MSUB -l gres=ignore</pre><p>The default is to require all mounted Lustre file systems. Use "ignore" to require no file systems (job can run even if file systems are down).</p></td>
<td>Job requires the specified parallel Lustre file system(s). Valid labels are the names of mounted Lustre parallel file systems, such as lustre1, lustre2. The purpose of this option is to prevent jobs from being scheduled if the specified file systems are unavailable.</td>
</tr><tr><td>
<pre>#SBATCH -H</pre></td>
<td>
<pre>#MSUB -h</pre></td>
<td>Put a user hold on the job at submission time.</td>
</tr><tr><td>
<pre>#SBATCH -i filename</pre></td>
<td>
<p> </p>
</td>
<td>Specifies the file name to be used for stdin.</td>
</tr><tr><td>
<pre>#SBATCH -J name</pre></td>
<td>
<pre>#MSUB -N name</pre></td>
<td>Gives a user specified name to the job.</td>
</tr><tr><td>
<pre>default</pre></td>
<td>
<pre>#MSUB -j oe</pre></td>
<td>Combine stdout and stderr into the same output file. This is the default. If you want to give the combined stdout/stderr file a specific name, include the -o flag also.</td>
</tr><tr><td>
<pre>#SBATCH --mail-type=type
(begin, end, fail, requeue, all)</pre></td>
<td>
<pre>#MSUB -m option(s)
(a=abort, b=begin, e=end)</pre></td>
<td>Defines when a mail message about the job will be sent to the user. See the man page for details.</td>
</tr><tr><td>
<pre>#SBATCH -N #</pre></td>
<td>
<pre>#MSUB -l nodes=#</pre></td>
<td>Node count</td>
</tr><tr><td>
<pre>#SBATCH -n #
#SBATCH --ntasks-per-node=#
#SBATCH --tasks-per-node=#</pre></td>
<td>
<pre>#MSUB -l procs=#
#MSUB -l ttc=#</pre></td>
<td>Task count</td>
</tr><tr><td>
<pre>#SBATCH --nice=value</pre></td>
<td>
<pre>#MSUB -p value</pre></td>
<td>Assigns a user priority value to a job.</td>
</tr><tr><td>
<pre>#SBATCH -o filename</pre></td>
<td>
<pre>#MSUB -o filename</pre></td>
<td>Defines the file name to be used for stdout.</td>
</tr><tr><td>
<pre>#SBATCH -p partition</pre></td>
<td>
<pre>#MSUB -q queue</pre></td>
<td>Run the job in the specified partition/queue (pdebug, pbatch, etc.).</td>
</tr><tr><td>
<pre>#SBATCH --qos=exempt
#SBATCH --qos=expedite
#SBATCH --qos=standby</pre></td>
<td>
<pre>#MSUB -l qos=exempt
#MSUB -l qos=expedite
#MSUB -l qos=standby</pre></td>
<td>Defines the quality-of-service to be used for the job.</td>
</tr><tr><td>
<pre>#SBATCH --requeue
#SBATCH --no-requeue</pre></td>
<td>
<pre>#MSUB -r y
#MSUB -l resfailpolicy=requeue
#MSUB -r n
#MSUB -l resfailpolicy=cancel</pre></td>
<td>Specifies whether or not to rerun the job is there is a system failure. The default behavior at LC is to NOT automatically rerun a job in such cases.</td>
</tr><tr><td>
<p> </p>
</td>
<td>
<pre>#MSUB -S path</pre></td>
<td>Specifies the shell which interprets the job script. The default is your login shell.</td>
</tr><tr><td>
<pre>#SBATCH --signal=14@120
#SBATCH --signal=SIGHUP@2:00</pre></td>
<td>
<pre>#MSUB -l signal=14@120
#MSUB -l signal=SIGHUP@2:00</pre></td>
<td>Signaling - specifies the pre-termination signal to be sent to a job at the desired time before expiration of the job's wall clock limit. Default time is 60 seconds.</td>
</tr><tr><td>
<pre>#SBATCH -t time</pre></td>
<td>
<pre>#MSUB -l walltime= time</pre></td>
<td>Specifies the wall clock time limit for the job. See the man page for syntax.</td>
</tr><tr><td>
<pre>#SBATCH --export=ALL</pre></td>
<td>
<pre>#MSUB -V</pre></td>
<td>Declares that all environment variables in the job submission environment are exported to the batch job.</td>
</tr></table></li>
</ul><p> </p>
<h3><a id="Submit" name="Submit"></a>Submitting Jobs</h3>
<h4><span class="heading3">Job Submission Commands</span></h4>
<p> </p>
<ul><li>The <span class="cmd">sbatch</span> and <span class="cmd">msub</span> commands are used to submit your job script to the Workload Manager. Upon successful submission, the job's ID is returned and it is spooled for execution.</li>
<li>These commands accept the same options as the #SBATCH / #MSUB tokens in a batch script.</li>
<li>Examples:<br /><table class="table"><tr><th>Slurm</th>
<th>Moab</th>
</tr><tr><td>
<pre>% sbatch myjobscript
 
Submitted batch job 645133

% sbatch -p pdebug -A physics myjobscript
 
Submitted batch job 645134
</pre></td>
<td>
<pre>% msub myjobscript
 
226783

% msub -q pdebug -A physics myjobscript
 
227243
</pre></td>
</tr></table></li>
</ul><h4><span class="heading3">Usage Notes</span></h4>
<ul><li>Both <span class="cmd">sbatch</span> and <span class="cmd">msub</span> are available on LC SLURM clusters:
<ul><li>Use <span class="cmd">sbatch</span> to submit job scripts with #SBATCH syntax</li>
<li>Use <span class="cmd">msub</span> to submit job scripts with #MSUB syntax</li>
</ul></li>
<li>After you submit your job script, changes to the contents of the script file will have no effect on your job because it has already been spooled to system file space.</li>
<li>Users may submit and queue as many jobs as they like, up to a reasonable configuration defined limit. The actual number of running jobs per user is usually a lower limit, however. These limits can vary between machines.</li>
<li>The default directory is where you submit your job from. If you need to be in another directory, then you will need to explicitly <span class="cmd">cd</span> to, or set the working directory with an #SBATCH / #MSUB option.</li>
</ul><h3><span class="heading3">Monitoring Jobs </span></h3>
<p><span class="heading3">checkjob</span></p>
<ul><li>Displays detailed job state information and diagnostic output for a selected job.</li>
<li>Detailed information is available for queued, blocked, active, and recently completed jobs.</li>
<li>The <span class="cmd">checkjob</span> command is probably the most useful user command for troubleshooting your job, especially if used with the <span class="cmd">-v</span> and <span class="cmd">-v -v</span> flags.</li>
<li>Common/useful options:
<ul><li><span class="cmd">-v</span> shows additional information</li>
<li><span class="cmd">-v -v</span> shows additional information plus job script (if available)</li>
</ul></li>
<li><a href="https://computing.llnl.gov/tutorials/moab/man/checkjob.txt">Checkjob man page HERE</a></li>
<li>Examples below:<br /><table class="table"><tr><td>
<pre>% checkjob 650263
job 650263

AName: vasp_NEB_inter_midbot_t2
State: Running
Creds:  user:vuiuey2  group:vuiuey2  account:ioncond  class:pbatch  qos:normal
WallTime:  01:53:43 of 06:00:00
SubmitTime: Thu May 11 06:50:30
  (Time Queued Total:  1:49:46   Eligible:  1:49:46)

StartTime: Thu May 11 08:40:16
Total Requested Tasks:  1
Total Requested Nodes:  10
Partition: pbatch
Dedicated Resources Per Task: luster1
Node Access: SINGLEJOB
NodeCount: 10

Allocated Nodes: 
quartz[52,362-363,398-399,444-445,2648-2650]

SystemID:  quartz
SystemJID: 650263
IWD:        /p/luster1/vuiuey2/calculations/radiation/V_Br/NEB_inter_midbot_t2
Executable: /p/luster1/vuiuey2/calculations/radiation/V_Br/NEB_inter_midbot_t2/psub.vasp

User Specified Partition List:    quartz
System Available Partition List:  quartz
Partition List: quartz
StartPriority: 1000031

</pre></td>
</tr></table></li>
</ul><h3><a name="BanksUsage" id="BanksUsage"> </a>Banks and Usage Information</h3>
<h4>Overview</h4>
<ul><li>As <a href="#Banks">discussed previously</a>, in order to run on a cluster, you need to have two things:
<ul><li>Login account: your username appears in /etc/passwd</li>
<li>Bank: your username is associated with a valid bank</li>
</ul></li>
<li>Banks are hierarchical and determine the percentage of machine resources (cycles) you are entitled to receive, as discussed in the <a href="#Banks">Banks</a> section of this tutorial.</li>
<li>Every cluster has its own unique bank structure. To view the entire bank hierarchy, use the command:<br /><pre><span class="cmd">mshare -t root</span></pre></li>
<li>You may have an allocation in more than one bank.</li>
<li>If you belong to more than one bank, your banks are not necessarily shared across all of the clusters where you have a login account.</li>
<li>
<p>You have a default bank on each cluster, which may/may not be the same on other clusters.</p>
</li>
</ul><h4><span class="heading3">mshare</span></h4>
<ul><li>Displays bank structure, allocations and usage information</li>
<li>If you want to see your available banks and usage stats for a cluster, this is the best command to use.</li>
<li>This is also the best command for viewing your place within the entire bank hierarchy.</li>
<li><a href="https://computing.llnl.gov/tutorials/moab/man/mshare.txt">mshare man page HERE</a></li>
</ul><h4>sreport</h4>
<ul><li>Reports usage information for a cluster, bank, individual, date range, and more.</li>
<li><a href="https://computing.llnl.gov/tutorials/moab/man/sreport.txt">sreport man page HERE</a></li>
<li>Example: show usage by user (in hours) for the alliance bank on the cluster cab between the dates shown.<br /><pre>% sreport -t hours cluster AccountUtilizationByUser accounts=alliance cluster=cab  start=2/1/12 end=3/1/12

--------------------------------------------------------------------------------
Cluster/Account/User Utilization 2012-02-01T00:00:00 - 2012-02-29T23:59:59 (2505600 secs)
Time reported in CPU Hours
--------------------------------------------------------------------------------
  Cluster         Account     Login     Proper Name       Used 
--------- --------------- --------- --------------- ---------- 
     cab         alliance                              2739237 
     cab          caltech                               500080 
     cab          caltech   br4e33t        Joe User     500076 
     cab          caltech   sthhhd6       Bill User          4 
     cab         michigan                               844339 
     cab         michigan  dhat67s        Mary User       261 
     cab         michigan  hetyyr2         Sam User     38552 
...
...
</pre></li>
</ul><h3><a name="OutputFiles" id="OutputFiles"> </a>Output Files</h3>
<h4>Defaults</h4>
<ul><li>The batch output file is named slurm-jobid.out</li>
<li>stdout and stderr are combined into the same batch output file.</li>
<li>Will be written to the directory where you issued the <span class="cmd">sbatch</span> or <span class="cmd">msub</span> command.</li>
<li>
<p>The name of a job has no effect on the name of the output file.</p>
</li>
<li>
<p>If an output file with the same name already exists, new output will append to it.</p>
</li>
</ul><h3><a id="Guesstimate" name="Guesstimate"></a>Estimating When Your Job Will Start</h3>
<ul><li>One of the most frequently asked questions is "When will my job start?".</li>
<li>Because job scheduling is dynamic, and can change at any moment, picking an exact time is often impossible.</li>
<li>There are a couple ways that you can get an estimate on when your job will start, based on the current situation.</li>
<li>The easiest way is to use the <span class="cmd">checkjob</span> command. Look for the StartTime line - if it exists. Note that not all jobs will show a start time.</li>
<li>For example:</li>
</ul><pre>% checkjob 830063
job 830063

AName: OSU_IMPI51_ICC_64_run_2
State: Idle
Creds:  user:e889till  group:e889till  account:asccasc  class:pbatch  qos:normal
WallTime:  00:00:00 of 01:00:00
SubmitTime: Wed Jun 14 09:12:48
  (Time Queued Total:  5:06:19   Eligible:  5:06:19)

StartTime: Thu Jun 15 00:02:00
Total Requested Tasks:  1
Total Requested Nodes:  64
Partition: pbatch
Dedicated Resources Per Task: ignore
Node Access: SINGLEJOB
NodeCount: 64

SystemID:  quartz
SystemJID: 830063
IWD:        /g/g91/e889till/batcher
Executable: /g/g91/e889till/batcher/quartz.job

User Specified Partition List:    quartz
System Available Partition List:  quartz
Partition List: quartz
StartPriority: 1000147

NOTE: job can not run because there are higher priority jobs.
</pre><ul><li>Sometimes the <span class="cmd">squeue --start</span> command can be used to get an estimate for job start times (and sometimes it can't). For example:
<pre>% squeue --start
  JOBID PARTITION     NAME     USER  ST           START_TIME  NODES NODELIST(REASON)
2173337    pbatch  job.txt  is456i1  PD  2017-06-28T15:31:58      1 (Priority)
2173338    pbatch  job.txt  is456i1  PD  2017-06-28T15:36:02      1 (Priority)
2173339    pbatch  job.txt  is456i1  PD  2017-06-28T16:25:37      1 (Priority)
2173340    pbatch  job.txt  is456i1  PD  2017-06-28T16:26:37      1 (Priority)
2173341    pbatch  job.txt  is456i1  PD  2017-06-28T16:35:37      1 (Priority)
...
2174947    pbatch S180_10p bbbmons3  PD  2017-07-03T19:28:33      1 (Priority)
1247698    pdebug ppmd_IA6   vvvang  PD                  N/A      2 (PartitionNodeLimit)
1863133    pbatch    runme   tttite  PD                  N/A      4 (PartitionNodeLimit)
2156487    pbatch surge110 mrtrang1  PD                  N/A      1 (Dependency)
2163499    pbatch      voh mrtrang1  PD                  N/A      1 (Dependency)
</pre></li>
<li>You can also view the position of your job in the queue relative to other jobs. Either of the commands below will give you a list of idle jobs sorted by priority - highest priority is at the top of the list.<br /><pre><span class="cmd">sprio -l | sort -r -k 4,4
mdiag -p -v | sort -r -k 4,4 </span></pre></li>
<li>For example (some output deleted to fit screen):</li>
<li> <br /><pre>% sprio -l  |  sort -r -k 4,4 
  JOBID  PARTITION  USER   PRIORITY        AGE  FAIRSHARE    JOBSIZE  PARTITION        QOS   NICE
 830811  pbatch   aaarg2    1321517          0     321517          0          0    1000000      0
 830760  pbatch    saaa4    1189400          6     189394          0          0    1000000      0
 830759  pbatch    saaa4    1189400          6     189394          0          0    1000000      0
 830776  pbatch    saaa4    1189397          3     189394          0          0    1000000      0
 830775  pbatch    ddwa9    1189397          3     189394          0          0    1000000      0
 830774  pbatch    rrra8    1189397          3     189394          0          0    1000000      0
...

 830366  pbatch  zhhhh49    1000127        127          0          0          0    1000000      0
 830365  pbatch  taffhh2    1000127        128          0          0          0    1000000      0
 830369  pbatch  zhhhh49    1000125        125          0          0          0    1000000      0
 830493  pbatch  wwwg102    1000102         86         16          0          0    1000000      0
 830576  pbatch  jjjtega    1000090         31         60          0          0    1000000      0
 830810  pbatch e988till    1000000          0          0          0          0    1000000      0
 829743  pbatch  yyymel2    1000000          0          0          0          0    1000000      0</pre></li>
</ul><h2><a id="ParallelJobs" name="ParallelJobs"></a>Parallel Jobs and the srun Command </h2>
<h3><span class="heading3">srun Command:</span></h3>
<ul><li>The Slurm <span class="cmd">srun</span> command is required to launch parallel jobs - both batch and interactive.</li>
<li>It should also be used to launch serial jobs in the pdebug and other interactive queues.</li>
<li>Syntax:
<p><a href="//computing.llnl.gov/tutorials/moab/man/srun.txt" target="_blank">srun</a>   [option list]   [executable]   [args]</p>
<p>Note that srun options must precede your executable.</p>
</li>
<li>
<p>Interactive use example, from the login node command line. Specifies 2 nodes (-N), 72 tasks (-n) and the interactive pdebug partition (-p):</p>
</li>
</ul><pre>% srun -N2 -n72 -ppdebug myexe
</pre><ul><li>Batch use example requesting 16 nodes and 576 tasks (assumes nodes have 36 cores):<br /><table class="table"><tr><th>Slurm</th>
<th>Moab</th>
</tr><tr><td colspan="2">First create a job script that requests nodes and uses <span class="cmd">srun</span> to specify the number of tasks and launch the job:</td>
</tr><tr><td>
<pre>#!/bin/tcsh
#SBATCH -N 16
#SBATCH -t 2:00:00
#SBATCH -p pbatch

# Run info and srun job launch
cd /p/luster1/joeuser/par_solve
srun -n576 a.out
echo 'Done'
</pre></td>
<td>
<pre>#!/bin/tcsh
#MSUB -l nodes=16
#MSUB -l walltime=2:00:00
#MSUB -q pbatch

# Run info and srun job launch
cd /p/luster1/joeuser/par_solve
srun -n576 a.out
echo 'Done'
</pre></td>
</tr><tr><td colspan="2">Then submit the job script from the login node command line:</td>
</tr><tr><td>
<pre>% sbatch myjobscript</pre></td>
<td>
<pre>% msub myjobscript</pre></td>
</tr></table></li>
<li>Primary differences between batch and interactive usage:<br /><table class="table"><tr><th>Difference</th>
<th>Interactive</th>
<th>Batch</th>
</tr><tr><td>Where used:</td>
<td>From login node command line</td>
<td>In batch script</td>
</tr><tr><td>Partition:</td>
<td>Requires specification of an interactive partition, such as pdebug with the <span class="cmd">-p</span> flag</td>
<td>pbatch is default</td>
</tr><tr><td>Scheduling:</td>
<td>If there are available interactive nodes, job will run immediately. Otherwise, it will queue up (fifo) and wait until there are enough free nodes to run it.</td>
<td>The batch scheduler handles when to run your job regardless of the number of nodes available.</td>
</tr></table></li>
<li>More Examples:<br /><pre>srun -n64 -ppdebug my_app</pre><p>64 process job run interactively in pdebug partition</p>
<pre>srun -N64 -n512 my_threaded_app</pre><p>512 process job using 64 nodes. Assumes pbatch partition.</p>
<pre>srun -N4 -n16 -c4 my_threaded_app</pre><p>4 node, 16 process job with 4 cores (threads) per process. Assumes pbatch partition.</p>
<pre>srun -N8 my_app</pre><p>8 node job with a default value of one task per node (8 tasks). Assumes pbatch partition.</p>
<pre>srun -n128 -o my_app.out my_app</pre><p>128 process job that redirects stdout to file my_app.out. Assumes pbatch partition.</p>
<pre>srun -n32 -ppdebug -i my.inp my_app</pre><p>32 process interactive job; each process accepts input from a file called my.inp instead of stdin</p></li>
</ul><h3><span class="heading3">Task Distribution and Binding for Batch Jobs:</span></h3>
<ul><li>The LC default is for the scheduler to distribute tasks as evenly as possible across the allocated nodes.</li>
<li>Examples: if 4 nodes (with 16 cores each) are requested by a batch job using:<br /><pre>#SBATCH -N 4
#MSUB -l nodes=4</pre><p>then the behavior of <span class="cmd">srun</span> -N and -n flags will be as follows:</p>
<p></p><div class="media media-element-container media-default"><div id="file-2139" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/srunnn-gif-1">srunNn.gif</a></h2>
    
  
  <div class="content">
    <img alt="diagram of the behavior of srun -N and -n flags" height="288" width="810" class="media-element file-default" data-delta="170" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/srunNn_1.gif" /></div>

  
</div>
</div>
</li>
<li>Additionally, tasks are bound to specific cores to promote better cache utilization.</li>
<li>Threads associated with a task are likewise bound to the same cores.</li>
</ul><h3><span class="heading3">srun Options</span></h3>
<ul><li><span class="cmd">srun</span> is a powerful command with many (&gt; 100 options) affecting a wide range of job parameters.</li>
<li>For example:
<ul><li>Accounting</li>
<li>Number and placement of processes/threads</li>
<li>Process/thread binding</li>
<li>Job resource requirements; dependencies</li>
<li>Mail notification options</li>
<li>Input, output options</li>
<li>Time limits</li>
<li>Checkpoint, restart options</li>
<li>and much more....</li>
</ul></li>
<li>Some <span class="cmd">srun</span> options may be set via  Slurm environment variables. For example, SLURM_NNODES behaves like the -N option.</li>
<li>See the <a href="https://computing.llnl.gov/tutorials/moab/man/srun.txt" target="_blank">srun man page</a> for details.</li>
</ul><h3><span class="heading3">Parallel Output</span></h3>
<ul><li>Please use a parallel file system for parallel I/O. Lustre parallel file systems are mounted under <span class="file">/p/lustre#</span>.</li>
<li>It's a good idea to launch parallel jobs from a parallel file system even if they aren't doing much I/O. A core dump on a parallel job can hang a non-parallel file system easily.</li>
</ul><h2><span class="heading1"><a id="MultipleJobs" name="MultipleJobs"></a>Running Multiple Jobs From a Single Job Script</span></h2>
<h3><span class="heading3">Motivation </span></h3>
<ul><li>Combining multiple jobs into a single job script means there is only one wait in the queue for the entire job group.</li>
</ul><h3><span class="heading3">Sequential</span></h3>
<ul><li>If one job is dependent upon the completion of a previous job, then this method can be used.</li>
<li>When you submit your job script, be sure to specify enough wall clock time to cover all of the included jobs.</li>
<li>Individual jobs can vary in the number of nodes used, provided none of them exceed the number of nodes allocated to your encompassing job script.</li>
<li>Example below. Assumes 36 cores per node: 16 nodes * 36 cores = 576 tasks max.</li>
<li>
<table class="table table-striped"><tr><th>Slurm</th>
<th>Moab</th>
</tr><tr><td>
<pre>#!/bin/tcsh
#SBATCH -N 16
#SBATCH -t 12:00:00

srun -n576 myjob1
srun -n576 myjob2
srun -N16 -n288 myjob3
srun -N12 -n432 myjob4
</pre></td>
<td>
<pre>#!/bin/tcsh
#MSUB -l nodes=16
#MSUB -l walltime=12:00:00

srun -n576 myjob1
srun -n576 myjob2
srun -N16 -n288 myjob3
srun -N12 -n432 myjob4
</pre></td>
</tr></table></li>
</ul><h3><span class="heading3">Simultaneous</span></h3>
<ul><li>This method can be used if there are no dependencies between jobs.</li>
<li>When you submit your job script, be sure to specify enough nodes to cover all of the included jobs.</li>
<li>You can vary the number of nodes used by individual jobs as long as the aggregate number of nodes doesn't exceed the number of nodes allocated to your encompassing job script.</li>
<li>
<p>Important to remember:</p>
<ul><li>
<p>Put each individual job "in the background" using an ampersand - otherwise they will run sequentially.</p>
</li>
<li>
<p>Include a wait statement to ensure the job script doesn't terminate prematurely.</p>
</li>
<li>With your srun commands, be sure to explicitly specify how many nodes each job requires - or else the scheduler will think each job has access to all nodes, with possible complications.</li>
</ul></li>
</ul><p>Example 1: every job uses the same number of nodes/tasks.<br />Assumes 36 cores per node: 16 nodes * 36 cores = 576 tasks max.</p>
<table class="table"><tr><th>Slurm</th>
<th>Moab</th>
</tr><tr><td>
<pre>#!/bin/tcsh
#SBATCH -N 16
#SBATCH -t 12:00:00

srun -N4 -n144 myjob1 &amp;
srun -N4 -n144 myjob2 &amp;
srun -N4 -n144 myjob3 &amp;
srun -N4 -n144 myjob4 &amp;

wait
</pre></td>
<td>
<pre>#!/bin/tcsh
#MSUB -l nodes=16
#MSUB -l walltime=12:00:00

srun -N4 -n144 myjob1 &amp;
srun -N4 -n144 myjob2 &amp;
srun -N4 -n144 myjob3 &amp;
srun -N4 -n144 myjob4 &amp;

wait
</pre></td>
</tr></table><ul><li>Example 2: jobs differ in the number of nodes/tasks used.<br />Assumes 36 cores per node: 16 nodes * 36 cores = 576 tasks max.</li>
</ul><p> </p>
<table class="table"><tr><th>Slurm</th>
<th>Moab</th>
</tr><tr><td>
<pre>#!/bin/tcsh
#SBATCH -N 16
#SBATCH -t 12:00:00

srun -N4 -n144 myjob1 &amp;
srun -N2 -n72 myjob2 &amp;
srun -N8 -n8 myjob3 &amp;
srun -N2 -n16 myjob4 &amp;

wait
</pre></td>
<td>
<pre>#!/bin/tcsh
#MSUB -l nodes=16
#MSUB -l walltime=12:00:00

srun -N4 -n144 myjob1 &amp;
srun -N2 -n72 myjob2 &amp;
srun -N8 -n8 myjob3 &amp;
srun -N2 -n16 myjob4 &amp;

wait
</pre></td>
</tr></table><h2><a name="Interactive" id="Interactive"> </a>Interactive Jobs</h2>
<h4><a id="interactSLURM" name="interactSLURM"> Interactive Jobs - SLURM</a></h4>
<ul><li>Refers to batch  jobs that you launch and interact with real-time from the command line.</li>
<li>Don't use Login Nodes (in most cases):
<ul><li>Login nodes are shared, oftentimes by many users.</li>
<li>Intended for non-cpu intensive interactive tasks, such as editing, working with files, running GUIs, browsers, debuggers, etc.</li>
<li>Not intended for running cpu-intensive production jobs, which can negatively impact other users.</li>
</ul></li>
<li>Most clusters have a pdebug partition of compute nodes dedicated to small, short, interactive jobs. Another common interactive partition is pvis.</li>
<li>There are several ways to run interactive jobs on LC's Linux clusters, as described below.</li>
<li><strong>Method 1:</strong> Use <span class="cmd">srun</span> on a login node specifying compute nodes in the pdebug or other interactive partition.
<ul><li>To view the partitions on a cluster use the <span class="cmd">sinfo -s</span> command. It shows which partitions are configured, their time limit and how the nodes are currently being used (Allocated / Idle / Other / Total). For example, on Quartz note the pdebug partition:
<pre>% sinfo -s
PARTITION AVAIL  TIMELIMIT   NODES(A/I/O/T)  NODELIST
pdebug       up      30:00       20/26/0/46  quartz[1-46]
pbatch*      up 1-00:00:00  2865/52/13/2930  quartz[47-186,193-378,...,2887-3072]
pall       down   infinite  2885/78/13/2976  quartz[1-186,193-378,...,2887-3072]
</pre></li>
<li>Then use the <span class="cmd">srun</span> command to run the job on compute nodes in pdebug. For example, to run your executable on 2 pdebug nodes using 16 tasks:<br /><pre>% srun -p pdebug -N 2 -n 16 myexe</pre></li>
</ul></li>
<li><strong>Method 2:</strong> Use <span class="cmd">salloc</span> to acquire compute nodes for interactive use.
<ul><li>On a login node, request an allocation of compute nodes, and optionally the partition to acquire them from. The default is usually pbatch, but small short jobs usually get better turnaround time in pdebug. For example, to request 2 pdebug compute nodes:
<pre>% salloc -p pdebug -N 2
salloc: Pending job allocation 3891090
salloc: job 3891090 queued and waiting for resources
salloc: job 3891090 has been allocated resources
salloc: Granted job allocation 3891090</pre></li>
<li>After your allocation is granted, you will be placed on the first node of the allocation. You can verify this as follows:<br /><pre>% squeue -u joeuser
  JOBID PARTITION     NAME      USER ST       TIME  NODES NODELIST(REASON)
3891090    pdebug       sh   joeuser  R       0:07      2 quartz[1-2]
% hostname
quartz1</pre></li>
<li>You can now run whatever you'd like interactively on your job's compute nodes.</li>
</ul></li>
<li><strong>Method 3: </strong>Use <span class="cmd">sxterm / mxterm</span> to acquire compute nodes for interactive use.
<ul><li>Similar to salloc, but different because it will actually "behind the scenes" create a batch job for your request, queue it and then when it starts to run on a compute node, pop open an xterm window on your desktop.</li>
<li>For example requesting 1 node / 1 task / for 30 minutes in the pdebug partition:<br /><pre>% sxterm 1 1 30 -p pdebug
Submitted batch job 3048379</pre></li>
<li>Wait until the xterm window appears on your desktop. You will be on the first node of your job's allocation and can run whatever you'd like interactively.</li>
<li>There's no man page - just invoke "sxterm" with no options and a usage message will display.</li>
<li>The <span class="cmd">mxterm</span> command is similar, with different options. Just invoke "mxterm" for usage information.</li>
</ul></li>
<li><strong>Method 4:</strong> Submit a batch job and then <span class="cmd">rsh/ssh</span> to the compute nodes where it is running.
<ul><li>You can submit a batch job (covered next) that really doesn't do anything except "sleep".</li>
<li>The <span class="cmd">squeue</span> command can be used to show when and where your job is running.</li>
<li>After it starts running, you can <span class="cmd">rsh</span> or <span class="cmd">ssh</span> to any of its compute nodes.</li>
<li>Once on a compute node, you can run whatever you'd like interactively.</li>
</ul></li>
<li>Important usage notes about the pdebug partition:
<ul><li>As the name pdebug implies, interactive jobs should be short, small debugging jobs, not production runs.</li>
<li>Shorter time limit</li>
<li>Fewer number of nodes permitted</li>
<li>There is usually a "good neighbor" policy in effect - don't monopolize the queue or setup streams of jobs</li>
</ul></li>
</ul><h4><a id="interactLSF" name="interactLSF">Interactive Jobs - LSF</a></h4>
<p>There are two main "flavors" of interactive jobs:</p>
<ul><li>Pseudo-terminal shell - uses your existing SSH login window</li>
<li>Xterm - launches a new window using your default login shell</li>
</ul><p>The LSF bsub command, and the LC lalloc command can both be used for interactive jobs.</p>
<p>Examples:</p>
<p>Starting a pseudo-terminal interactive job using bsub:<br />From a login node, the bsub command is used to request 4 nodes in an Interactive pseudo-terminal, X11 Forwarding, Wall clock limit of 10 minutes, in a tcsh shell. After the dispatch the interactive session starts on the first compute node (by default). The bjobs -X command is used to display the compute nodes allocated for this job.</p>
<pre>bsub -nnodes 4 -Ip -XF -W 10 /bin/tcsh

Job &lt;206798&gt; is submitted to default queue &lt;pdebug&gt;.

&lt;&lt;ssh X11 forwarding job&gt;&gt;

&lt;&lt;Waiting for dispatch ...&gt;&gt;

&lt;&lt;Starting on rzansel62&gt;&gt;</pre><p>Starting a pseudo-terminal interactive job using lalloc:<br />This same action can be performed more simply using LC's lalloc command.  Note that by default, lalloc will use the first compute node as a private launch node. For example:</p>
<p> </p>
<pre>lalloc 4 + exec bsub -nnodes 4 -Is -XF -W 60 -core_isolation 2 /usr/tce/packages/lalloc/lalloc-2.0/bin/lexec

Job &lt;281904&gt; is submitted to default queue &lt;pbatch&gt;.

&lt;&lt;ssh X11 forwarding job&gt;&gt;

&lt;&lt;Waiting for dispatch ...&gt;&gt;

&lt;&lt;Starting onlassen4370&gt;&gt; &lt;&lt;Waiting for JSM to become ready ...&gt;&gt;

&lt;&lt;Redirecting to compute nodelassen1214, setting up as private launch node&gt;&gt;

lassen1214%</pre><h2><a id="moreinfo" name="moreinfo"> More Information - LC Documentation and Tutorials</a></h2>
<ul><li>Using LC's Sierra/CORAL  Systems - <a href="https://hpc.llnl.gov/training/tutorials/using-lcs-sierra-system">https://hpc.llnl.gov/training/tutorials/using-lcs-sierra-system</a></li>
<li>LSF Quick Start - <a href="https://hpc.llnl.gov/banks-jobs/running-jobs/lsf-quick-start-guide">https://hpc.llnl.gov/banks-jobs/running-jobs/lsf-quick-start-guide</a></li>
<li>Slurm and Moab Tutorial - <a href="https://computing.llnl.gov/tutorials/moab/">https://computing.llnl.gov/tutorials/moab/</a></li>
<li>Livermore Computing Resources and Environment - <a href="https://computing.llnl.gov/tutorials/lc_resources/">https://computing.llnl.gov/tutorials/lc_resources/</a></li>
<li>Batch System Cross Reference Guide - <a href="https://hpc.llnl.gov/banks-jobs/running-jobs/batch-system-commands">https://hpc.llnl.gov/banks-jobs/running-jobs/batch-system-commands</a></li>
</ul><p>LLNL-WEB-814160</p>
<ul></ul></div></div></div>    </div>
  </div>
</div>


<!-- Needed to activate display suite support on forms -->
  </div>
  
</div> <!-- /.block --></div>
 <!-- /.region -->
                   		</div>
                  </main>
                </div>
      		</div>
    	</div>
	</div>
  	
	

    <footer id="colophon" class="site-footer">
        <div class="container">
            <div class="row">
                <div class="col-sm-12 footer-top">

                    <a class="llnl" href="https://www.llnl.gov/" target="_blank"><img src="/sites/all/themes/tid/images/llnl.png" alt="LLNL"></a>
                    <p>
                        Lawrence Livermore National Laboratory
                        <br>7000 East Avenue • Livermore, CA 94550
                    </p>
                    <p>
                        Operated by Lawrence Livermore National Security, LLC, for the
                        <br>Department of Energy's National Nuclear Security Administration.
                    </p>
                    <div class="footer-top-logos">
                        <a class="nnsa" href="https://www.energy.gov/nnsa/national-nuclear-security-administration" target="_blank"><img src="/sites/all/themes/tid/images/nnsa2.png" alt="NNSA"></a>
                        <a class="doe" href="https://www.energy.gov/" target="_blank"><img src="/sites/all/themes/tid/images/doe_small.png" alt="U.S. DOE"></a>
                        <a class="llns" href="https://www.llnsllc.com/" target="_blank"><img src="/sites/all/themes/tid/images/llns.png" alt="LLNS"></a>
                	</div>



                </div>
                <div class="col-sm-12 footer-bottom">
                	

                    <span>UCRL-MI-131558  &nbsp;|&nbsp;&nbsp;</span><a href="https://www.llnl.gov/disclaimer" target="_blank">Privacy &amp; Legal Notice</a>	 &nbsp;|&nbsp;&nbsp; <a href="mailto:webmaster-comp@llnl.gov">Website Query</a> &nbsp;|&nbsp;&nbsp;<a href="/about-us/contact-us" >Contact Us</a>
                </div>
            </div>
        </div>
    </footer>
</div>
  </body>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/jquery_update/replace/jquery/2.1/jquery.min.js?v=2.1.4"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/jquery-extend-3.4.0.js?v=2.1.4"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/jquery-html-prefilter-3.5.0-backport.js?v=2.1.4"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/jquery.once.js?v=1.2"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/drupal.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/extlink/extlink.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/jquery.flexslider.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/slide.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/lightbox2/js/lightbox.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/matomo/matomo.js?qsohrw"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
var _paq = _paq || [];(function(){var u=(("https:" == document.location.protocol) ? "https://analytics.llnl.gov/" : "http://analytics.llnl.gov/");_paq.push(["setSiteId", "149"]);_paq.push(["setTrackerUrl", u+"piwik.php"]);_paq.push(["setDoNotTrack", 1]);_paq.push(["trackPageView"]);_paq.push(["setIgnoreClasses", ["no-tracking","colorbox"]]);_paq.push(["enableLinkTracking"]);var d=document,g=d.createElement("script"),s=d.getElementsByTagName("script")[0];g.type="text/javascript";g.defer=true;g.async=true;g.src="https://hpc.llnl.gov/sites/default/files/matomo/piwik.js?qsohrw";s.parentNode.insertBefore(g,s);})();
//--><!]]>
</script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/bootstrap.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/mobilemenu.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/custom.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/mods.js?qsohrw"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"tid","theme_token":"X7iGSpdtACU_RX23SjbbR-EALOTfWKA6oS5b-33QmW4","js":{"sites\/all\/modules\/contrib\/jquery_update\/replace\/jquery\/2.1\/jquery.min.js":1,"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/modules\/contrib\/extlink\/extlink.js":1,"sites\/all\/themes\/tid\/js\/jquery.flexslider.js":1,"sites\/all\/themes\/tid\/js\/slide.js":1,"sites\/all\/modules\/contrib\/lightbox2\/js\/lightbox.js":1,"sites\/all\/modules\/contrib\/matomo\/matomo.js":1,"0":1,"sites\/all\/themes\/tid\/js\/bootstrap.js":1,"sites\/all\/themes\/tid\/js\/mobilemenu.js":1,"sites\/all\/themes\/tid\/js\/custom.js":1,"sites\/all\/themes\/tid\/js\/mods.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/system\/system.theme.css":1,"modules\/book\/book.css":1,"sites\/all\/modules\/contrib\/date\/date_api\/date.css":1,"sites\/all\/modules\/contrib\/date\/date_popup\/themes\/datepicker.1.7.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/contrib\/extlink\/extlink.css":1,"sites\/all\/modules\/contrib\/views\/css\/views.css":1,"sites\/all\/modules\/contrib\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/contrib\/lightbox2\/css\/lightbox.css":1,"sites\/all\/modules\/contrib\/print\/print_ui\/css\/print_ui.theme.css":1,"sites\/all\/themes\/tid\/css\/bootstrap.css":1,"sites\/all\/themes\/tid\/css\/flexslider.css":1,"sites\/all\/themes\/tid\/css\/system.menus.css":1,"sites\/all\/themes\/tid\/css\/style.css":1,"sites\/all\/themes\/tid\/font-awesome\/css\/font-awesome.css":1,"sites\/all\/themes\/tid\/css\/treewalk.css":1,"sites\/all\/themes\/tid\/css\/popup.css":1,"sites\/all\/themes\/tid\/css\/mods.css":1}},"lightbox2":{"rtl":0,"file_path":"\/(\\w\\w\/)public:\/","default_image":"\/sites\/all\/modules\/contrib\/lightbox2\/images\/brokenimage.jpg","border_size":10,"font_color":"000","box_color":"fff","top_position":"","overlay_opacity":"0.8","overlay_color":"000","disable_close_click":true,"resize_sequence":0,"resize_speed":400,"fade_in_speed":400,"slide_down_speed":600,"use_alt_layout":false,"disable_resize":false,"disable_zoom":false,"force_show_nav":false,"show_caption":true,"loop_items":false,"node_link_text":"View Image Details","node_link_target":false,"image_count":"Image !current of !total","video_count":"Video !current of !total","page_count":"Page !current of !total","lite_press_x_close":"press \u003Ca href=\u0022#\u0022 onclick=\u0022hideLightbox(); return FALSE;\u0022\u003E\u003Ckbd\u003Ex\u003C\/kbd\u003E\u003C\/a\u003E to close","download_link_text":"","enable_login":false,"enable_contact":false,"keys_close":"c x 27","keys_previous":"p 37","keys_next":"n 39","keys_zoom":"z","keys_play_pause":"32","display_image_size":"original","image_node_sizes":"()","trigger_lightbox_classes":"","trigger_lightbox_group_classes":"","trigger_slideshow_classes":"","trigger_lightframe_classes":"","trigger_lightframe_group_classes":"","custom_class_handler":0,"custom_trigger_classes":"","disable_for_gallery_lists":true,"disable_for_acidfree_gallery_lists":true,"enable_acidfree_videos":true,"slideshow_interval":5000,"slideshow_automatic_start":true,"slideshow_automatic_exit":true,"show_play_pause":true,"pause_on_next_click":false,"pause_on_previous_click":true,"loop_slides":false,"iframe_width":600,"iframe_height":400,"iframe_border":1,"enable_video":false,"useragent":"Mozilla\/5.0 (compatible; AhrefsBot\/7.0; +http:\/\/ahrefs.com\/robot\/)"},"extlink":{"extTarget":0,"extClass":"ext","extLabel":"(link is external)","extImgClass":0,"extIconPlacement":0,"extSubdomains":1,"extExclude":".gov|.com|.org|.io|.be|.us|.edu","extInclude":"-int.llnl.gov|lc.llnl.gov|caas.llnl.gov|exchangetools.llnl.gov","extCssExclude":"","extCssExplicit":"","extAlert":"_blank","extAlertText":"This page is routing you to a page which requires extra authentication. You must have on-site or VPN access.\r\n\r\nPress OK to continue or cancel to return.\r\n\r\nIf this fails or times-out, you are not allowed access to the internal page or the server may be temporarily unavailable.\r\n\r\nIf you have an on-site or VPN account and are still having trouble, please send e-mail to lc-hotline@llnl.gov or call 925-422-4531 for further assistance.","mailtoClass":"mailto","mailtoLabel":"(link sends e-mail)"},"matomo":{"trackMailto":1},"urlIsAjaxTrusted":{"\/training\/tutorials\/livermore-computing-psaap3-quick-start-tutorial":true}});
//--><!]]>
</script>
</html>
