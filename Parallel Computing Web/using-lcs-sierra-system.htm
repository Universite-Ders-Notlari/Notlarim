<!DOCTYPE html>
<html lang="en" dir="ltr"
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:dc="http://purl.org/dc/terms/"
  xmlns:foaf="http://xmlns.com/foaf/0.1/"
  xmlns:og="http://ogp.me/ns#"
  xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
  xmlns:sioc="http://rdfs.org/sioc/ns#"
  xmlns:sioct="http://rdfs.org/sioc/types#"
  xmlns:skos="http://www.w3.org/2004/02/skos/core#"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#">
<head>
<meta charset="utf-8" http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="Generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="/training/tutorials/using-lcs-sierra-system" />
<link rel="shortlink" href="/node/629" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
<link rel="shortcut icon" href="https://hpc.llnl.gov/sites/all/themes/tid/favicon.ico" type="image/vnd.microsoft.icon" />
<title>Using LC&#039;s Sierra Systems | High Performance Computing</title>
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_kShW4RPmRstZ3SpIC-ZvVGNFVAi0WEMuCnI0ZkYIaFw.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_bq48Es_JAifg3RQWKsTF9oq1S79uSN2WHxC3KV06fK0.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_vAm-LJc0tkC-w_c6v7Ekq0bW26Pzl31HvPM6kbvK-pc.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_ca6tstDbY9-H23Ty8uKiDyFQLT1AZftZKldhbTPPnm8.css" media="all" />
<!--[if lt IE 9]><script src="/sites/all/themes/tid/js/html5.js"></script><![endif]-->
</head>
<body class="html not-front not-logged-in no-sidebars page-node page-node- page-node-629 node-type-user-portal-one-column-page">
  <div aria="contentinfo"><noscript><img src="https://analytics.llnl.gov/piwik.php?idsite=149" class="no-border" alt="" /></noscript></div>
    <div id="page">
	<div class="unclassified"></div>
	<div class="headertop">
					<div id="skip-nav" role="navigation" aria-labelledby="skip-nav" class="reveal">
  			<a href="#main-content">Skip to main content</a>
			</div>
					</div>
        <div class="headerwrapbg">
                        <div class="headerwrap-portal">
                <div id="masthead" class="site-header container" role="banner">
                    <div class="row">
                        <div class="llnl-logo col-sm-3">
                            <a href="https://www.llnl.gov" target="_blank" title="Lawrence Livermore National Laboratory">
                                <img src="/sites/all/themes/tid/images/llnl-tab-portal.png" alt="LLNL Home" />
                            </a>
                        </div>
                        <div id="logo" class="site-branding col-sm-4">
                                                            <div id="site-logo">
                                        <!--High Performance Computing<br />Livermore Computing Center-->
                                        																					<a href="/user-portal" class="text-dark" title="Livermore Computing Center High Performance Computing">
                                            <img src="/sites/all/themes/tid/images/hpc.png" alt="Portal Home" />
																					</a>
																				
                                </div>
                                                    </div>
                        <div class="col-sm-5">
                            <div id="top-search">
															<div class="input-group">
																	<form class="navbar-form navbar-search navbar-right" action="/training/tutorials/using-lcs-sierra-system" method="post" id="search-block-form" accept-charset="UTF-8"><div><div class="container-inline">
      <div class="element-invisible">Search form</div>
    <div class="form-item form-type-textfield form-item-search-block-form">
  <label class="element-invisible" for="edit-search-block-form--2">Search </label>
 <input title="Enter the terms you wish to search for." type="text" id="edit-search-block-form--2" name="search_block_form" value="" size="15" maxlength="128" class="form-text" />
</div>
<div class="form-actions form-wrapper" id="edit-actions"><input type="submit" id="edit-submit" name="op" value="" class="form-submit" /></div><input type="hidden" name="form_build_id" value="form-82fguzsJ_MeIRZmgpV9R3-vUQd11iXxpwtJK2UGCEV0" />
<input type="hidden" name="form_id" value="search_block_form" />
</div>
</div></form>                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div id="mainnav">
                    <div class="container">
                        <div class="row">
                            <nav id="Menu" aria-label="Mobile Menu" class="mobilenavi col-md-12"></nav>
                            <nav id="navigation" aria-label="Main Menu">
                                <div id="main-menu" class="main-menu-portal">
                                    <ul class="menu"><li class="first collapsed"><a href="/user-portal">Portal</a></li>
<li class="expanded"><a href="/accounts">Accounts</a><ul class="menu"><li class="first leaf"><a href="/accounts/new-account-setup">New Account Setup</a></li>
<li class="leaf"><a href="/accounts/idm-account-management">IdM Account Management</a></li>
<li class="leaf"><a href="https://hpc.llnl.gov/manuals/access-lc-systems" title="">Access to LC Systems</a></li>
<li class="leaf"><a href="/accounts/computer-coordinator-roles">Computer Coordinator Roles</a></li>
<li class="collapsed"><a href="/accounts/forms">Forms</a></li>
<li class="collapsed"><a href="/accounts/policies">Policies</a></li>
<li class="last leaf"><a href="/accounts/mailing-lists">Mailing Lists</a></li>
</ul></li>
<li class="expanded"><a href="/banks-jobs">Banks &amp; Jobs</a><ul class="menu"><li class="first leaf"><a href="/banks-jobs/allocations">Allocations</a></li>
<li class="expanded"><a href="/banks-jobs/running-jobs">Running Jobs</a><ul class="menu"><li class="first leaf"><a href="/banks-jobs/running-jobs/batch-system-primer">Batch System Primer</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/lsf-user-manual">LSF User Manual</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/lsf-quick-start-guide">LSF Quick Start Guide</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/lsf-commands">LSF Commands</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/slurm-user-manual" title="Guide to using the Slurm Workload/Resource Manager">Slurm User Manual</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/slurm-quick-start-guide">Slurm Quick Start Guide</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/slurm-commands">Slurm Commands</a></li>
<li class="leaf"><a href="/training/tutorials/slurm-and-moab">Slurm and Moab</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/batch-system-commands">Batch System Cross-Reference</a></li>
<li class="last leaf"><a href="/banks-jobs/running-jobs/slurm-srun-versus-ibm-csm-jsrun">Slurm srun versus IBM CSM jsrun</a></li>
</ul></li>
<li class="leaf"><a href="https://hpc.llnl.gov/accounts/forms/asc-dat" title="">ASC DAT Request</a></li>
<li class="last leaf"><a href="https://hpc.llnl.gov/accounts/forms/mic-dat" title="">M&amp;IC DAT Request</a></li>
</ul></li>
<li class="expanded"><a href="/hardware">Hardware</a><ul class="menu"><li class="first collapsed"><a href="/hardware/archival-storage-hardware">Archival Storage Hardware</a></li>
<li class="collapsed"><a href="/hardware/platforms">Compute Platforms</a></li>
<li class="leaf"><a href="/hardware/compute-platforms-gpus">Compute Platforms with GPUs</a></li>
<li class="collapsed"><a href="/hardware/file-systems">File Systems</a></li>
<li class="leaf"><a href="/hardware/testbeds">Testbeds</a></li>
<li class="collapsed"><a href="/hardware/zones">Zones (aka &quot;The Enclave&quot;)</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/lorenz/mylc/mylc.cgi" title="">MyLC (Lorenz)</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi?" title="">CZ Compute Platform Status</a></li>
<li class="leaf"><a href="https://rzlc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" title="">RZ Compute System Status</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/fsstatus/fsstatus.cgi" title="">CZ File System Status</a></li>
<li class="last leaf"><a href="https://rzlc.llnl.gov/fsstatus/fsstatus.cgi" title="">RZ File System Status</a></li>
</ul></li>
<li class="expanded"><a href="/services">Services</a><ul class="menu"><li class="first collapsed"><a href="/services/green-data-oasis">Green Data Oasis (GDO)</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/lorenz/mylc/mylc.cgi" title="">MyLC (Lorenz)</a></li>
<li class="last leaf"><a href="/services/visualization-services">Visualization Services</a></li>
</ul></li>
<li class="expanded"><a href="/software">Software</a><ul class="menu"><li class="first leaf"><a href="/software/archival-storage-software">Archival Storage Software</a></li>
<li class="collapsed"><a href="/software/data-management-tools-projects">Data Management Tools</a></li>
<li class="collapsed"><a href="/software/development-environment-software">Development Environment Software</a></li>
<li class="leaf"><a href="/software/mathematical-software">Mathematical Software</a></li>
<li class="leaf"><a href="/software/modules-and-software-packaging">Modules and Software Packaging</a></li>
<li class="collapsed"><a href="/software/visualization-software">Visualization Software</a></li>
<li class="last leaf"><a href="https://computing.llnl.gov/projects/radiuss" title="">RADIUSS</a></li>
</ul></li>
<li class="last expanded active-trail"><a href="/training" class="active-trail">Training</a><ul class="menu"><li class="first expanded active-trail"><a href="/training/tutorials" class="active-trail">Tutorials</a><ul class="menu"><li class="first leaf"><a href="/training/tutorials/introduction-parallel-computing-tutorial">Introduction to Parallel Computing Tutorial</a></li>
<li class="leaf"><a href="/training/tutorials/llnl-covid-19-hpc-resource-guide">LLNL Covid-19 HPC Resource Guide for New Livermore Computing Users</a></li>
<li class="leaf active-trail"><a href="/training/tutorials/using-lcs-sierra-system" class="active-trail active">Using LC&#039;s Sierra System</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-psaap3-quick-start-tutorial">Livermore Computing PSAAP3 Quick Start Tutorial</a></li>
<li class="leaf"><a href="https://hpc.llnl.gov/sites/default/files/PSAAP-alliance-quickguide.docx" title="">PSAAP Alliance Quick Guide</a></li>
<li class="leaf"><a href="/training/tutorials/linux-tutorial-exercises">Linux Tutorial Exercise One</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-linux-commodity-clusters-overview-part-one">Livermore Computing Linux Clusters Overview Part One</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-linux-commodity-clusters-overview-part-two">Livermore Computing Linux Clusters Overview Part Two</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-resources-and-environment">Livermore Computing Resources and Environment</a></li>
<li class="leaf"><a href="/training/tutorials/slurm-and-moab-exercise">Slurm and Moab Exercise</a></li>
<li class="leaf"><a href="/training/tutorials/slurm-and-moab">Slurm and Moab Tutorial</a></li>
<li class="leaf"><a href="/training/tutorials/totalview-part-2-common-functions">TotalView Part 2:  Common Functions</a></li>
<li class="leaf"><a href="/training/tutorials/totalview-part-3-debugging-parallel-programs">TotalView Part 3: Debugging Parallel Programs</a></li>
<li class="leaf"><a href="/training/tutorials/totalview-tutorial">TotalView Tutorial</a></li>
<li class="leaf"><a href="/training/tutorials/evaluation-form">Tutorial Evaluation Form</a></li>
<li class="leaf"><a href="/training/tutorials/srun-auto-affinity">srun --auto-affinity</a></li>
<li class="last leaf"><a href="/training/tutorials/srun-multi-prog">srun --multi-prog</a></li>
</ul></li>
<li class="collapsed"><a href="/training/documentation">Documentation &amp; User Manuals</a></li>
<li class="leaf"><a href="/training/technical-bulletins-catalog">Technical Bulletins Catalog</a></li>
<li class="collapsed"><a href="/training/workshop-schedule">Training Events</a></li>
<li class="last leaf"><a href="/training/user-meeting-presentations-archive">User Meeting Presentation Archive</a></li>
</ul></li>
</ul>                                                                            <div id="pagetoggle" class="btn-group btn-toggle pull-right" style="margin-right: 15px;">
                                            <a href="/" class="btn btn-default gs">General Site</a>
                                            <a href="/user-portal" class="btn btn-primary up active">User Portal</a>
                                        </div>
                                                                    </div>
                            </nav>
                        </div>
                    </div>
                </div>
            </div>
        </div>
            </div>
		<div id="main-content" class="l2content">
        <div class="container">
    		<div class="row">
        		                <div id="primary" class="content-area col-sm-12">
					                                        <section id="content" role="nav" class="clearfix col-sm-12">

                                                                                    <div id="breadcrumbs">
                                    <h2 class="element-invisible">breadcrumb menu</h2><nav class="breadcrumb" aria-label="breadcrumb-navigation"><a href="/">Home</a> » <a href="/training">Training</a> » <a href="/training/tutorials">Tutorials</a> » Using LC&#039;s Sierra Systems</nav>                                </div>
                                                    
                                            </section>
                  <main>

                                              <div id="content_top">
                                <div class="region region-content-top">
  <div id="block-print-ui-print-links" class="block block-print-ui">

    
    
  
  <div class="content">
    <span class="print_html"><a href="https://hpc.llnl.gov/print/629" title="Display a printer-friendly version of this page." class="print-page" onclick="window.open(this.href); return false" rel="nofollow">Printer-friendly</a></span>  </div>
  
</div> <!-- /.block --></div>
 <!-- /.region -->
                            </div>
                        
                        <div id="content-wrap">
                                                                                                                <div class="region region-content">
  <div id="block-system-main" class="block block-system">

    
    
  
  <div class="content">
    

<div  about="/training/tutorials/using-lcs-sierra-system" typeof="sioc:Item foaf:Document" class="node node-user-portal-one-column-page node-full view-mode-full">
    <div class="row">
    <div class="col-sm-12 ">
      <div class="field field-name-title field-type-ds field-label-hidden"><div class="field-items"><div class="field-item even" property="dc:title"><h1 class="title">Using LC&#039;s Sierra Systems</h1></div></div></div><div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even" property="content:encoded"><h2><a name="contents" id="contents"></a>Table of Contents</h2>
<ol><li><a href="#Abstract">Abstract</a></li>
<li><a href="#Quickstart">Quickstart Guide</a></li>
<li><a href="#Overview">Sierra Overview</a>
<ol><li><a href="#CORAL">CORAL</a></li>
<li><a href="#CORAL-EA">CORAL Early Access Systems</a></li>
<li><a href="#Sierra">Sierra Systems</a></li>
</ol></li>
<li><a href="#Hardware">Sierra Hardware</a>
<ol><li><a href="#Hardware">Sierra Systems General Configuration</a></li>
<li><a href="#POWER8">IBM POWER8 Architecture</a></li>
<li><a href="#POWER9">IBM POWER9 Architecture</a></li>
<li><a href="#Pascal">NVIDIA Tesla P100 (Pascal) Architecture</a></li>
<li><a href="#Volta">NVIDIA Tesla V100 (Volta) Architecture</a></li>
<li><a href="#NVLink">NVLink</a></li>
<li><a href="#Mellanox">Mellanox EDR InfiniBand Network</a></li>
<li><a href="#NVMe">NVMe PCIe SSD (Burst Buffer)</a></li>
</ol></li>
<li><a href="#Accounts">Accounts, Allocations and Banks</a></li>
<li><a href="#Access">Accessing LC's Sierra Machines</a></li>
<li><a href="#Software">Software and Development Environment </a>
<ol><li><a href="#login-nodes">Login Nodes and Launch Nodes</a></li>
<li><a href="#login-shells-files">Login Shells and Files</a></li>
<li><a href="#operating-systems">Operating System</a></li>
<li><a href="#batch-system">Batch System</a></li>
<li><a href="#file-systems">Sierra File Systems</a></li>
<li><a href="#hpss-storage">HPSS Storage</a></li>
<li><a href="#modules">Modules</a></li>
<li><a href="#compilers-supported">Compilers Overview</a></li>
<li><a href="#MathLibs1">Sierra Math Libraries</a></li>
<li><a href="#debuggers">Debuggers, Performance Analysis Tools</a></li>
<li><a href="#visualization-software-compute-resources">Visualization Software and Compute Resources</a></li>
</ol></li>
<li><a href="#Compilers">Compilers on Sierra</a>
<ol><li><a href="#compwrappers">Wrapper Scripts</a></li>
<li><a href="#compversions">Versions</a></li>
<li><a href="#compselect">Selecting Your Compiler Version</a></li>
<li><a href="#XLcompiler">IBM XL Compilers</a></li>
<li><a href="#Clangcompiler">Clang Compiler</a></li>
<li><a href="#GNUcompiler">GNU Compilers</a></li>
<li><a href="#PGIcompiler">PGI Compilers</a></li>
<li><a href="#NVCCcompiler">NVIDIA NVCC Compiler</a></li>
</ol></li>
<li><a href="#MPI">MPI</a></li>
<li><a href="#OpenMP">OpenMP</a></li>
<li><a href="#SysConfig">System Configuration and Status Information</a></li>
<li><a href="#Running">Running Jobs on Sierra Systems</a>
<ol><li><a href="#Running">Overview</a></li>
<li><a hre="">Summary of Job Related Commands</a></li>
<li><a href="#BatchScripts">Batch Scripts and #BSUB / bsub</a></li>
<li><a href="#Interactive">Interactive Jobs: bsub and lalloc commands</a></li>
<li><a href="#lrun">Launching Jobs: the lrun Command</a></li>
<li><a href="#jsrun">Launching Jobs: the jsrun Command and Resource Sets</a></li>
<li><a href="#Dependencies">Job Dependencies</a></li>
<li><a href="#Monitor">Monitoring Jobs: lsfjobs, bjobs, bpeek, bhist commands</a></li>
<li><a href="#Suspend">Suspending / Resuming Jobs: bstop, bresume commands</a></li>
<li><a href="#Modify">Modifying Jobs: bmod command</a></li>
<li><a href="#Signal">Signaling / Killing Jobs: bkill command</a></li>
<li><a href="#CUDA-aware">CUDA-aware MPI</a></li>
<li><a href="#Binding">Process, Thread and GPU Binding: js_task_info</a></li>
<li><a href="#Diagnostics">Node Diagnostics: check_sierra_nodes</a></li>
<li><a href="#BurstBuffer">Burst Buffer Usage</a></li>
</ol></li>
<li><a href="#BanksJobUsage">Banks, Job Usage and Job History Information</a></li>
<li><a href="#LSF">LSF - Additional Information</a>
<ol><li><a href="#R3"> LSF Documentation</a></li>
<li><a href="#LSFconfig">LSF Configuration Commands</a></li>
</ol></li>
<li><a href="#MathLibs2">Math Libraries</a></li>
<li><a href="#Debug">Debugging</a>
<ol><li><a href="#TotalView">TotalView</a></li>
<li><a href="#STAT">STAT</a></li>
<li><a href="#CoreFiles">Core Files</a></li>
</ol></li>
<li><a href="#Performance">Performance Analysis Tools</a></li>
<li><a href="#Evaluation">Tutorial Evaluation Section </a></li>
<li><a href="#References">References &amp; Documentation</a></li>
<li><a href="#Quickstart">Appendix A: Quickstart Guide</a></li>
</ol><h2><a name="Abstract" id="Abstract"></a>Abstract</h2>
<p>This tutorial is intended for users of Livermore Computing's Sierra systems. It begins by providing a brief background on CORAL, leading to the CORAL EA and Sierra systems at LLNL. The CORAL EA and Sierra hybrid hardware architectures are discussed, including details on IBM POWER8 and POWER9 nodes, NVIDIA Pascal and Volta GPUs, Mellanox network hardware, NVLink and NVMe SSD hardware.</p>
<p>Information about user accounts and accessing these systems follows. User environment topics common to all LC systems are reviewed. These are followed by more in-depth usage information on compilers, MPI and OpenMP. The topic of running jobs is covered in detail in several sections, including obtaining system status and configuration information, creating and submitting LSF batch scripts, interactive jobs, monitoring jobs and interacting with jobs using LSF commands.</p>
<p>A summary of available math libraries is presented, as is a summary on parallel I/O. The tutorial concludes with discussions on available debuggers and performance analysis tools.</p>
<p>A Quickstart Guide is included as an appendix to the tutorial, but it is linked at the top of the tutorial table of contents for visibility.</p>
<p><em>Level/Prerequisites:</em> Intended for those who are new to developing parallel programs in the Sierra environment. A basic understanding of parallel programming in C or Fortran is required. Familiarity with MPI and OpenMP is desirable. The material covered by EC3501 - <a href="https://computing.llnl.gov/tutorials/lc_resources" target="_blank">Introduction to Livermore Computing Resources</a> would also be useful.</p>
<p> </p>
<h2><a name="Overview" id="Overview"></a>Sierra Overview</h2>
<h3><a name="CORAL" id="CORAL"></a><strong>CORAL:</strong></h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-983" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/corallogo-jpg-0">coralLogo.jpg</a></h2>
    
  
  <div class="content">
    <img alt="CORAL Logo" height="190" width="392" style="width: 392px; height: 190px;" class="media-element file-default" data-delta="5" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/coralLogo_0.jpg" /></div>

  
</div>
</div></div>
<ul><li><strong><span class="text-danger">C O R A L</span></strong>  = <span class="text-danger"><strong>C</strong></span>ollaboration of <strong><span class="text-danger">O</span></strong>ak <strong><span class="text-danger">R</span></strong>idge, <strong><span class="text-danger">A</span></strong>rgonne, and <strong><span class="text-danger">L</span></strong>ivermore</li>
<li>A first-of-its-kind U.S. Department of Energy (DOE) collaboration between the <a href="https://www.energy.gov/nnsa/national-nuclear-security-administration" target="_blank">NNSA's</a> ASC Program and the <a href="https://www.energy.gov/science/office-science" target="_blank">Office of Science's</a> <a href="https://www.energy.gov/science/ascr/advanced-scientific-computing-research" target="_blank">Advanced Scientific Computing Research program (ASCR)</a>.</li>
<li>CORAL is the next major phase in the DOE's scientific computing roadmap and path to exascale computing.</li>
<li>Will culminate in three ultra-high performance supercomputers at Lawrence Livermore, Oak Ridge, and Argonne national laboratories.</li>
<li>Will be used for the most demanding scientific and national security simulation and modeling applications, and will enable continued U.S. leadership in computing.</li>
<li>The three CORAL systems are:
<ul><li>LLNL: <a href="https://asc.llnl.gov/coral-info" target="_blank">Sierra</a></li>
<li>ORNL: <a href="https://www.olcf.ornl.gov/summit/" target="_blank">Summit</a></li>
<li>ANL: <a href="http://aurora.alcf.anl.gov/" target="_blank">Aurora</a></li>
</ul></li>
<li>LLNL and ORNL systems were delivered in the 2017-18 timeframe. The Argonne system's planned delivery (revised) is in 2021.</li>
<li><a href="https://hpc.llnl.gov/sites/default/files/CORAL.FactSheet.2014.12.17.pdf" target="_blank">DOE / NNSA CORAL Fact Sheet (Dec 17, 2014)</a></li>
<li>Announcements/Press:
<ul><li><a href="https://hpc.llnl.gov/sites/default/files/announcement.DOE_.2014.11.14_0.pdf" target="_blank">Department of Energy (Nov 14, 2014)</a></li>
<li><a href="https://hpc.llnl.gov/sites/default/files/announcement.HPCwire.2014.11.14.pdf" target="_blank">HPCWire(Nov 14, 2014)</a></li>
<li><a href="https://hpc.llnl.gov/sites/default/files/announcement.LLNL_.2014.11.14.pdf" target="_blank">LLNL (Nov 14, 2014)</a></li>
<li><a href="https://www.nextplatform.com/2017/10/05/clever-machinations-livermores-sierra-supercomputer/" target="_blank">Next Platform (Oct 5, 2017)</a></li>
</ul></li>
</ul><h3><a name="CORAL-EA" id="CORAL-EA"></a><strong>CORAL Early Access (EA) Systems</strong></h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-984" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/rayphoto1-jpg-0">rayPhoto1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/rayPhoto1_0.jpg"><img alt="CORAL EA Ray Cluster" height="266" width="400" style="width: 400px; height: 266px;" class="media-element file-default" data-delta="6" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/rayPhoto1_0-400x266.jpg" /></a>  </div>

  
</div>
</div></div>
<ul><li>In preparation for the final delivery Sierra systems, LLNL implemented three "early access" systems, one on each network:
<ul><li><strong>ray</strong> - OCF-CZ</li>
<li><strong>rzmanta</strong> - OCF-RZ</li>
<li><strong>shark</strong> - SCF</li>
</ul></li>
<li>Primary purpose was to provide platforms where Tri-lab users could begin porting and preparing for the hardware and software that would be delivered with the final Sierra systems.</li>
<li>Similar to the final delivery Sierra systems but use the previous generation IBM Power processors and NVIDIA GPUs.</li>
<li>IBM Power Systems S822LC Server:
<ul><li>Hybrid architecture using IBM POWER8+ processors and NVIDIA Pascal GPUs.</li>
</ul></li>
<li>IBM POWER8+ processors:
<ul><li>2 per node (dual-socket)</li>
<li>10 cores/socket; 20 cores per node</li>
<li>8 SMT threads per core; 160 SMT threads per node</li>
<li>Clock: due to adaptive power management options, the clock speed can vary depending upon the system load. At LC speeds can vary from approximately 2 GHz - 4 GHz.</li>
</ul></li>
<li>NVIDIA GPUs:
<ul><li>4 NVIDIA Tesla P100 (Pascal) GPUs per compute node (not on login/service nodes)</li>
<li>3584 CUDA cores per GPU; 14,336 per node</li>
</ul></li>
<li>Memory:
<ul><li>256 GB DDR4 per node</li>
<li>16 GB HBM2 (High Bandwidth Memory 2) per GPU; 732 GB/s peak bandwidth</li>
</ul></li>
<li>NVLINK 1.0:
<ul><li>Interconnect for GPU-GPU and CPU-GPU shared memory</li>
<li>4 links per GPU/CPU with 160 GB/s total bandwidth (bidirectional)</li>
</ul></li>
<li>NVRAM:
<ul><li>1.6 TB NVMe PCIe SSD per compute node (CZ ray system only)</li>
</ul></li>
<li>Network:
<ul><li>Mellanox 100 Gb/s Enhanced Data Rate (EDR) InfiniBand</li>
<li>One dual-port 100 Gb/s EDR Mellanox adapter per node</li>
</ul></li>
<li>Parallel File System: IBM Spectrum Scale (GPFS)
<ul><li>ray: 1.3 PB</li>
<li>rzmanta: 431 TB</li>
<li>shark: 431 TB</li>
</ul></li>
<li>Batch System: IBM Spectrum LSF</li>
<li>System Details:</li>
</ul><table class="table table-striped table-bordered" summary="CORAL Early Access (EA) Systems"><tr><th colspan="12" scope="col">CORAL Early Access (EA) Systems</th>
</tr><tr><td><strong>Cluster</strong></td>
<td><strong>OCF<br />SCF</strong></td>
<td><strong>Architecture</strong></td>
<td><strong>Clock Speed (GHz)</strong></td>
<td><strong>Nodes<br />GPUs</strong></td>
<td><strong>Cores<br />/Node<br />/GPU</strong></td>
<td><strong>Cores Total</strong></td>
<td><strong>Memory/<br />Node (GB)</strong></td>
<td><strong>Memory<br />Total (GB)</strong></td>
<td><strong>TFLOPS<br />Peak</strong></td>
<td><strong>Switch</strong></td>
<td><strong>ASC<br />M&amp;IC</strong></td>
</tr><tr><td><strong>ray</strong></td>
<td>OCF</td>
<td>IBM Power8<br />NVIDIA Tesla P100 (PASCAL)</td>
<td>2.0-4.0<br />1481 MHz</td>
<td>62<br />54*4</td>
<td>20<br />3484</td>
<td>1,240<br />752,544</td>
<td>256<br />16*4</td>
<td>15,872<br />3,456</td>
<td>39.7<br />1,144.8</td>
<td>IB EDR</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>rzmanta</strong></td>
<td>OCF</td>
<td>IBM Power8<br />NVIDIA Tesla P100 (PASCAL)</td>
<td>2.0-4.0<br />1481 MHz</td>
<td>44<br />36*4</td>
<td>20<br />3484</td>
<td>880<br />501,696</td>
<td>256<br />16*4</td>
<td>11,264<br />2,304</td>
<td>28.2<br />763.2</td>
<td>IB EDR</td>
<td>ASC</td>
</tr><tr><td><strong>shark</strong></td>
<td>SCF</td>
<td>IBM Power8<br />NVIDIA Tesla P100 (PASCAL)</td>
<td>2.0-4.0<br />1481 MHz</td>
<td>44<br />36*4</td>
<td>20<br />3484</td>
<td>880<br />501,696</td>
<td>256<br />16*4</td>
<td>11,264<br />2,304</td>
<td>28.2<br />763.2</td>
<td>IB EDR</td>
<td>ASC</td>
</tr></table><ul><li>Additional information:
<ul><li>User Guide: <a href="https://lc.llnl.gov/confluence/display/CORALEA/CORAL+EA+Systems" target="_blank">https://lc.llnl.gov/confluence/display/CORALEA/CORAL+EA+Systems</a> (LC internal wiki)</li>
<li>ray configuration: <a href="https://hpc.llnl.gov/hardware/platforms/Ray" target="_blank">https://hpc.llnl.gov/hardware/platforms/Ray</a></li>
<li>rzmanta configuration: <a href="https://hpc.llnl.gov/hardware/platforms/RZManta" target="_blank">https://hpc.llnl.gov/hardware/platforms/RZManta</a></li>
<li>shark configuration: <a href="https://hpc.llnl.gov/hardware/platforms/Shark" target="_blank">https://hpc.llnl.gov/hardware/platforms/Shark</a></li>
</ul></li>
</ul><h3><a name="Sierra" id="Sierra"></a><strong>Sierra Systems</strong></h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-985" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/sierra-img-3714-png">sierra.IMG_3714.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierra.IMG_3714.png"><img alt="Image of Sierra" height="200" width="400" style="height: 200px; width: 400px;" class="media-element file-default" data-delta="7" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierra.IMG_3714-400x200.png" /></a>  </div>

  
</div>
</div></div>
<ul><li><strong>Sierra</strong> is a classified, 125 petaflop, IBM Power Systems AC922 hybrid architecture system comprised of IBM POWER9 nodes with NVIDIA Volta GPUs. Sierra is a Tri-lab resource sited at <a href="https://www.llnl.gov/" target="_blank">Lawrence Livermore National Laboratory</a>.</li>
<li>Unclassified Sierra systems are similar, but smaller, and include:
<ul><li><strong>lassen</strong> - a 22.5 petaflop system located on LC's CZ zone.</li>
<li><strong>rzansel</strong> - a 1.5 petaflop system is located on LC's RZ zone.</li>
</ul></li>
<li>IBM Power Systems AC922 Server:
<ul><li>Hybrid architecture using IBM POWER9 processors and NVIDIA Volta GPUs.</li>
</ul></li>
<li>IBM POWER9 processors (compute nodes):
<ul><li>2 per node (dual-socket)</li>
<li>22 cores/socket; 44 cores per node</li>
<li>4 SMT threads per core; 176 SMT threads per node</li>
<li>Clock: due to adaptive power management options, the clock speed can vary depending upon the system load. At LC speeds can vary from approximately 2.3 - 3.8 GHz. LC can also set the clock to a specific speed regardless of workload.</li>
</ul></li>
<li>NVIDIA GPUs:
<ul><li>4 NVIDIA Tesla V100 (Volta) GPUs per compute, login, launch node</li>
<li>5120 CUDA cores per GPU; 20,480 per node</li>
</ul></li>
<li>Memory:
<ul><li>256 GB DDR4 per compute node; 170 GB/s peak bandwidth (per socket)</li>
<li>16 GB HBM2 (High Bandwidth Memory 2) per GPU; 900 GB/s peak bandwidth</li>
</ul></li>
<li>NVLINK 2.0:
<ul><li>Interconnect for GPU-GPU and CPU-GPU shared memory</li>
<li>6 links per GPU/CPU with 300 GB/s total bandwidth (bidirectional)</li>
</ul></li>
<li>NVRAM:
<ul><li>1.6 TB NVMe PCIe SSD per compute node</li>
</ul></li>
<li>Network:
<ul><li>Mellanox 100 Gb/s Enhanced Data Rate (EDR) InfiniBand</li>
<li>One dual-port 100 Gb/s EDR Mellanox adapter per node</li>
</ul></li>
<li>Parallel File System: IBM Spectrum Scale (GPFS)</li>
<li>Batch System: IBM Spectrum LSF</li>
<li>Water (warm) cooled compute nodes</li>
<li>System Details:</li>
</ul><table class="table table-striped table-bordered" summary="Sierra Systems (compute nodes)"><tr><th colspan="12" scope="colgroup">
<h3>Sierra Systems (compute nodes)</h3>
</th>
</tr><tr><th scope="col">Cluster</th>
<th scope="col">OCF<br />SCF</th>
<th scope="col">Architecture</th>
<th scope="col">Clock Speed (GHz)</th>
<th scope="col">Nodes<br />GPUs</th>
<th scope="col">Cores<br />/Node<br />/GPU</th>
<th scope="col">Cores Total</th>
<th scope="col">Memory/<br />Node (GB)</th>
<th scope="col">Memory<br />Total (GB)</th>
<th scope="col">TFLOPS<br />Peak</th>
<th scope="col">Switch</th>
<th scope="col">ASC<br />M&amp;IC</th>
</tr><tr><td><strong>sierra</strong></td>
<td>SCF</td>
<td>IBM Power9<br />NVIDIA TeslaV100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>4320<br />4320*4</td>
<td>44<br />5120</td>
<td>190,080<br />88,473,600</td>
<td>256<br />16*4</td>
<td>1,105,920<br />276,480</td>
<td>125,000</td>
<td>IB EDR</td>
<td>ASC</td>
</tr><tr><td><strong>lassen</strong></td>
<td>OCF</td>
<td>IBM Power9<br />NVIDIA TeslaV100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>774<br />774*4</td>
<td>44<br />5120</td>
<td>34,056<br />15,851,520</td>
<td>256<br />16*4</td>
<td>198,144<br />49,536</td>
<td>22,508</td>
<td>IB EDR</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>rzansel</strong></td>
<td>OCF</td>
<td>IBM Power9<br />NVIDIA TeslaV100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>54<br />54*4</td>
<td>44<br />5120</td>
<td>2376<br />1,105,920</td>
<td>256<br />16*4</td>
<td>13,824<br />3,456</td>
<td>1,570</td>
<td>IB EDR</td>
<td>ASC</td>
</tr></table><h3 class="clear-floats"><strong>Photos</strong></h3>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1072" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/sierradelivery1-jpg-0">sierraDelivery1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierraDelivery1_0.jpg"><img alt="Sierra being delivered" height="200" width="300" style="height: 200px; width: 300px;" class="media-element file-default" data-delta="64" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierraDelivery1_0-300x200.jpg" /></a>  </div>

  
</div>
</div></div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1073" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/sierradelivery2-jpg-0">sierraDelivery2.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierraDelivery2_0.jpg"><img alt="Boxes during Sierra delivery" height="200" width="300" style="height: 200px; width: 300px;" class="media-element file-default" data-delta="65" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierraDelivery2_0-300x200.jpg" /></a>  </div>

  
</div>
</div></div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1074" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/sierrasiting1-jpg-0">sierraSiting1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierraSiting1_0.jpg"><img alt="Sierra siting1" height="209" width="300" style="height: 209px; width: 300px;" class="media-element file-default" data-delta="66" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierraSiting1_0-300x209.jpg" /></a>  </div>

  
</div>
</div></div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1075" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/sierrasiting4-jpg-0">sierraSiting4.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierraSiting4_0.jpg"><img alt="Sierra siting4" height="219" width="300" style="height: 219px; width: 300px;" class="media-element file-default" data-delta="67" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierraSiting4_0-300x219.jpg" /></a>  </div>

  
</div>
</div></div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1076" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/sierrasiting2-jpg-0">sierraSiting2.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierraSiting2_0.jpg"><img alt="Sierra siting2" height="185" width="300" style="height: 185px; width: 300px;" class="media-element file-default" data-delta="68" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierraSiting2_0-300x185.jpg" /></a>  </div>

  
</div>
</div></div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1077" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/sierrasiting3-jpg-0">sierraSiting3.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierraSiting3_0.jpg"><img alt="View of sierra " height="200" width="300" style="height: 200px; width: 300px;" class="media-element file-default" data-delta="69" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierraSiting3_0-300x200.jpg" /></a>  </div>

  
</div>
</div></div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1079" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/sierra-img-3709-png-2">sierra.IMG_3709.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierra.IMG_3709_2.png"><img alt="View of Sierra rack" height="150" width="300" style="height: 150px; width: 300px;" class="media-element file-default" data-delta="70" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierra.IMG_3709_2-300x150.png" /></a>  </div>

  
</div>
</div></div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1080" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/sierra-img-3711-png-0">sierra.IMG_3711.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierra.IMG_3711_0.png"><img alt="Wide view of Sierra rack" height="150" width="300" style="height: 150px; width: 300px;" class="media-element file-default" data-delta="71" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierra.IMG_3711_0-300x150.png" /></a>  </div>

  
</div>
</div></div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1081" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/sierra-img-3714-png-1">sierra.IMG_3714.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierra.IMG_3714_1.png"><img alt="Multiple Sierra racks" height="150" width="300" style="height: 150px; width: 300px;" class="media-element file-default" data-delta="72" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierra.IMG_3714_1-300x150.png" /></a>  </div>

  
</div>
</div></div>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p>                   </p>
<p> </p>
<h2><a name="Hardware" id="Hardware"></a>Hardware</h2>
<h3><a name="sierra-general" id="sierra-general"></a>Sierra Systems General Configuration</h3>
<h4><strong>Sierra Systems General Configuration Diagram</strong></h4>
<p></p><div class="media media-element-container media-default"><div id="file-1605" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/sierraconfiguration-png-1">sierraConfiguration.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierraConfiguration_1.png"><img alt="diagram of Sierra Systems general configuration; Powerpoint available by request." height="551" width="800" style="width: 800px; height: 551px;" class="media-element file-default" data-delta="102" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierraConfiguration_1-800x551.png" /></a>  </div>

  
</div>
</div>
<p> </p>
<h4><strong>System Components</strong></h4>
<ul><li>The basic components of a Sierra system are the same as other LC systems. They include:
<ul><li>Frames / Racks  </li>
<li>Nodes</li>
<li>File Systems</li>
<li>Networks</li>
<li>HPSS Archival Storage</li>
</ul></li>
</ul><h4><strong>Frames / Racks</strong></h4>
<ul><li>Frames are the physical cabinets that hold most of a cluster's components:
<ul><li>Nodes of various types</li>
<li>Switch components</li>
<li>Other network and cluster management components</li>
<li>Parallel file system disk resources (usually in separate racks)</li>
</ul></li>
<li>Power and console management - frames include hardware and software that allow system administrators to perform most tasks remotely.</li>
</ul><h4><strong>Nodes</strong></h4>
<ul><li>Sierra systems consist of several different node types:
<ul><li>Compute nodes</li>
<li>Login / Launch nodes</li>
<li>I/O nodes</li>
<li>Service / management nodes</li>
</ul></li>
<li>Compute Nodes:
<ul><li>Comprise the heart of a system. This is where parallel user jobs run.</li>
<li>Dual-socket IBM POWER9 (AC922) nodes</li>
<li>4 NVIDIA Tesla V100 (Volta) GPUs per node</li>
</ul></li>
<li>Login / Launch Nodes:
<ul><li>When you connect to Sierra, you are placed on a login node. This is where users perform interactive, non-production work: edit files, launch GUIs, submit jobs and interact with the batch system.</li>
<li>Launch nodes are similar to login nodes, but are dedicated to managing user jobs, which in turn launch parallel jobs on compute nodes using jsrun (discussed later).</li>
<li>Login / launch nodes are shared by multiple users and should not be used themselves to run parallel jobs.</li>
<li>IBM Power9 with 4 NVIDIA Volta GPUs (same as compute nodes)</li>
</ul></li>
<li>I/O Nodes:
<ul><li>Dedicated file servers for IBM Spectrum Scale parallel file systems</li>
<li>Not directly accessible to users</li>
<li>IBM Power9, dual-socket; no GPUs</li>
</ul></li>
<li>Service / Management Nodes:
<ul><li>Reserved for system related functions and services</li>
<li>Not directly accessible to users</li>
<li>IBM Power9, dual-socket; no GPUs</li>
</ul></li>
</ul><h4><strong>Networks</strong></h4>
<ul><li>Sierra systems have a Mellanox 100 Gb/s Enhanced Data Rate (EDR) InfiniBand network:
<ul><li>Internal, inter-node network for MPI communications and I/O traffic between compute nodes and I/O nodes.</li>
<li>See the <a href="#Mellanox">Mellanox EDR InfiniBand Network</a> section for details.</li>
</ul></li>
<li>InfiniBand networks connect other clusters and parallel file servers.</li>
<li>A GigE network connects InfiniBand networks, HPSS and external networks and systems.</li>
</ul><h4><strong>File Systems</strong></h4>
<ul><li>Parallel file systems: Sierra systems use IBM Spectrum Scale. Other clusters use Lustre.</li>
<li>Other file systems (not shown) such as NFS (home directories, temp) and infrastructure services</li>
</ul><h4><strong>Archival HPSS Storage</strong></h4>
<ul><li>Details and usage information available at: <a href="https://computing.llnl.gov/tutorials/lc_resources/#Archival" target="_blank">https://computing.llnl.gov/tutorials/lc_resources/#Archival</a>.</li>
</ul><h3><a name="POWER8" id="POWER8"></a>IBM POWER8 Architecture</h3>
<p><strong>Used by LLNL's Early Access systems ray, rzmanta, shark</strong></p>
<h4><strong>IBM POWER8 SL822LC Node Key Features</strong></h4>
<ul><li>2 IBM "POWER8+" processors (dual-socket)</li>
<li>Up to 4 NVIDIA Tesla P100 (Pascal) GPUs</li>
<li>NVLink GPU-CPU and GPU-GPU interconnect technology</li>
<li>Memory:
<ul><li>Up to 1024 GB DDR4 memory per node</li>
<li>LC's Early Access systems compute nodes have 256 GB memory</li>
<li>Each processor connects to 4 memory riser cards with 4 DIMMs;</li>
<li>Processor-to-memory peak bandwidth of 115 GB/s bandwidth per processor, 230 GB/s memory bandwidth per node</li>
</ul></li>
<li>L4 cache: up to 64 MB per processor, in 16 MB banks of memory buffers</li>
<li>Storage: 2 disk bays for 2 hard disk drives (HDD) or 2 solid state drives (SSD). Optional NVMe SSD support in PCIe slots.</li>
<li>Coherent Accelerator Processor Interface (CAPI), which allows accelerators plugged into a PCIe slot to access the processor bus by using a low latency, high-speed protocol interface.</li>
<li>5 integrated PCIe Gen 3 slots:
<ul><li>1 PCIe x8 G3 LP slot, CAPI enabled</li>
<li>1 PCIe x16 G3, CAPI enabled</li>
<li>1 PCIe x8 G3</li>
<li>2 PCIe x16 G3, CAPI enabled that support GPU or PCIe adapters</li>
</ul></li>
<li>Adaptive power management</li>
<li>I/O ports: 2x USB 3.0; 2x 1 GB Ethernet; VGA</li>
<li>2 hotswap, redundant power supplies (no power redundancy with GPU(s) installed)</li>
<li>19-inch rackmount hardware (2U)</li>
<li>LLNL's Early Access POWER8 nodes:
<ul><li>Compute nodes are model 8335-GTB and login nodes are model 8335-GCA. The primary difference is that compute nodes include 4 NVIDIA Pascal GPUs and Power8 processors with NVLink technology.</li>
<li>Power8 processors use 10 cores</li>
<li>Memory: 256 GB per node</li>
<li>The CZ Early Access cluster "Ray" also has 1.6 TB NVMe PCIe SSD (attached solid state storage).</li>
</ul></li>
<li>Images
<ul><li>A POWER8 compute node and its primary components are shown below. Relevant individual components are discussed in more detail in sections below.</li>
<li>Click for a larger image. (<em>Source: "IBM Power Systems S822LC for High Performance Computing Technical Overview and Introduction". IBM Redpaper publication REDP-5405-00 by Alexandre Bicas Caldeira, Volker Haug, Scott Vetter. September, 2016</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1082" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/power8nodecomponents1-jpg-0">power8nodeComponents1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power8nodeComponents1_0.jpg"><img alt=" POWER8 SL822LC node with 4 NVIDIA Pascal GPUs" height="288" width="500" style="height: 288px; width: 500px;" class="media-element file-default" data-delta="73" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power8nodeComponents1_0-500x288.jpg" /></a>  </div>

  
</div>
</div><br />POWER8 SL822LC node with 4 NVIDIA Pascal GPUs</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1083" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/power8systemdiagram-png-0">power8systemDiagram.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power8systemDiagram_0.png"><img alt=" POWER8 SL822LC node logical system diagram" height="400" width="263" style="height: 400px; width: 263px;" class="media-element file-default" data-delta="74" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power8systemDiagram_0-263x400.png" /></a>  </div>

  
</div>
</div><br />POWER8 SL822LC node logical system diagram</div>
<h4><strong>POWER8 Processor Key Characteristics</strong></h4>
<ul><li>IBM 22 nm Silicon-On-Insulator (SOI) technology; 4.2 billion transistors</li>
<li>Up to 12 cores (LLNL's Early Access processors have 10 cores)</li>
<li>L1 data cache: 64 KB per core, 8-way, private</li>
<li>L1 instruction cache: 32 KB per core, 8-way, private</li>
<li>L2 cache: 512 KB per core, 8-way, private</li>
<li>L3 cache: 96 MB (12 core version), 8-way, shared as 8 MB banks per core</li>
<li>Hardware transactional memory</li>
<li>Clock: due to adaptive power management options, the clock speed can vary depending upon the system load. At LLNL speeds can vary from approximately 2 GHz - 4 GHz.</li>
<li>Images:
<ul><li>Images of the POWER8 processor chip (12 core version) are shown below. Click for a larger version. (<em>Source: "An Introduction to POWER8 Processor". IBM presentation by Joel M. Tendler. Georgia IBM POWER User Group, January 16, 2014</em>)</li>
</ul></li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-1003" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/power8chips-jpg">power8chips.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power8chips.jpg"><img alt="Power8 processor chip" height="235" width="600" style="height: 235px; width: 600px;" class="media-element file-default" data-delta="22" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power8chips-600x235.jpg" /></a>  </div>

  
</div>
</div>
<h4><strong>POWER8 Core Key Features</strong></h4>
<ul><li>The POWER8 processor core is a 64-bit implementation of the IBM Power Instruction Set Architecture (ISA) Version 2.07</li>
<li>Little Endian</li>
<li>8-way Simultaneous Multithreading (SMT)</li>
<li>Floating point units: Two integrated multi-pipeline vector-scalar. Run both scalar and SIMD-type instructions, including the Vector Multimedia Extension (VMX) instruction set and the improved Vector Scalar Extension (VSX) instruction set. Each is capable of up to eight single precision floating point operations per cycle (four double precision floating point operations per cycle)</li>
<li>Two symmetric fixed-point execution units</li>
<li>Two symmetric load and store units and two load units, all four of which can also run simple fixed-point instructions</li>
<li>Enhanced prefetch, branch prediction, out-of-order execution</li>
<li>Images:
<ul><li>Images of the POWER8 cores are shown below. Click for a larger version. (<em>Source: "An Introduction to POWER8 Processor". IBM presentation by Joel M. Tendler. Georgia IBM POWER User Group, January 16, 2014</em></li>
</ul></li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-1004" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/power8cores-jpg">power8cores.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power8cores.jpg"><img alt="Power8 core" height="330" width="600" style="height: 330px; width: 600px;" class="media-element file-default" data-delta="23" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power8cores-600x330.jpg" /></a>  </div>

  
</div>
</div>
<h4><strong>References and More Information</strong></h4>
<ul><li>IBM Redbook: <a href="https://hpc.llnl.gov/sites/default/files/RedBook.Power8_.sg248280.pdf" target="_blank">"Implementing an IBM High-Performance Computing Solution on IBM Power System S822LC".</a> Publication SG24-8280-00. July 2016.</li>
<li>IBM Redpaper: <a href="https://hpc.llnl.gov/sites/default/files/Power8TechOverview.redpaper5405.pdf" target="_blank">"IBM Power Systems S822LC for High Performance Computing Technical Overview and Introduction".</a> Publication REDP-5404-00. September 2016.</li>
</ul><h3><a name="POWER9" id="POWER9"></a>IBM POWER9 Architecture</h3>
<p><strong>Used by LLNL's Sierra systems sierra, lassen, rzansel</strong></p>
<h4><strong>IBM POWER9 AC922 Node Key Features</strong></h4>
<ul><li>2 IBM POWER9 processors (dual-socket)</li>
<li>Up to 6 NVIDIA Tesla V100 (Volta) GPUs</li>
<li>NVLink2 GPU-CPU and GPU-GPU interconnect technology</li>
<li>Memory: Up to 2 TB, from 16 DDR4 Sockets.
<ul><li>Up to 2 TB DDR4 memory per node</li>
<li>LC's Sierra systems compute nodes have 256 GB memory</li>
<li>Each processor connects to 8 DDR4 DIMMs</li>
<li>Processor-to-memory bandwidth (max hardware peak) of 170 GB/s per processor, 340 GB/s per node.</li>
</ul></li>
<li>Storage: 2 disk bays for 2 hard disk drives (HDD) or 2 solid state drives (SSD). Optional NVMe SSD support in PCIe slots.</li>
<li>Coherent Accelerator Processor Interface (CAPI) 2.0, which allows accelerators plugged into a PCIe slot to access the processor bus by using a low latency, high-speed protocol interface.</li>
<li>4 integrated PCIe Gen 4 slots providing ~2x the data bandwidth of PCIe Gen 3:
<ul><li>2 PCIe x16 G4, CAPI enabled</li>
<li>1 PCIe x8 G4, CAPI enabled</li>
<li>1 PCIe x4 G4</li>
</ul></li>
<li>Adaptive power management</li>
<li>I/O ports: 2x USB 3.0; 2x 1 GB Ethernet; VGA</li>
<li>2 hotswap, redundant power supplies</li>
<li>19-inch rackmount hardware (2U)</li>
<li>Images (click for larger image)
<ul><li>Sierra POWER9 AC922 compute node and its primary components. Relevant individual components are discussed in more detail in sections below.</li>
<li>Sierra POWER9 AC922 node diagram. <em>(Adapted from: "IBM Power System AC922 Introduction and Technical Overview". IBM Redpaper publication REDP-5472-00 by Alexandre Bicas Caldeira. March, 2018</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1606" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/power9-ac922node3-png">power9-AC922node3.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power9-AC922node3.png"><img alt="picture of node" height="257" width="400" style="font-family: Oxygen, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; background-color: transparent; height: 257px; width: 400px;" class="media-element file-default" data-delta="103" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power9-AC922node3-400x257.png" /></a>  </div>

  
</div>
</div><br />Sierra POWER9 AC922 node with 4 NVIDIA Volta GPUs</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1936" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/power9-ac922systemdiagram2-png-1">power9-AC922systemDiagram2.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power9-AC922systemDiagram2_1.png"><img alt="Sierra POWER9 AC922 node diagram" height="260" width="300" style="font-family: Oxygen, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; background-color: transparent; font-weight: 400; font-size: 14px; color: rgb(48, 48, 48); height: 260px; width: 300px; margin-left: 25px; margin-right: 25px;" class="media-element file-default" data-delta="104" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power9-AC922systemDiagram2_1-300x260.png" /></a>  </div>

  
</div>
</div><br />Sierra POWER9 AC922 node diagram</div>
<h4><strong>POWER9 Processor Key Characteristics</strong></h4>
<ul><li>IBM 14 nm Silicon-On-Insulator (SOI) technology; 8 billion transistors</li>
<li>IBM offers POWER9 in two different designs: Scale-Out and Scale-Up</li>
<li>Scale-Out:
<ul><li>Designed for traditional datacenter clusters utilizing single-socket and dual-socket servers.</li>
<li>Optimized for Linux servers</li>
<li>24-core and 12-core models</li>
</ul></li>
<li>Scale-Up:
<ul><li>Designed for NUMA servers with four or more sockets, supporting large amounts of memory capacity and throughput.</li>
<li>Optimized for PowerVM servers</li>
<li>24-core and 12-core models</li>
</ul></li>
<li>Core variants: Some POWER9 models vary the number of active cores and have 16, 18, 20 or 22 cores. LLNL's AC922 compute nodes use 22 cores.</li>
<li>Hardware threads:
<ul><li>12-core processors are SMT8 (8 hardware threads/core)</li>
<li>24-core processors are SMT4 (4 hardware threads/core).</li>
</ul></li>
<li>L1 data cache: 32 KB per core, 8-way, private</li>
<li>L1 instruction cache: 32 KB per core, 8-way, private</li>
<li>L2 cache: 512 KB per core (SMT8), 512 KB per core pair (SMT4), 8-way, private</li>
<li>L3 cache: 120 MB, 20-way, shared as twelve 10 MB banks</li>
<li>Clock: due to adaptive power management options, the clock speed can vary depending upon the system load. At LC speeds can vary from approximately 2.3 - 3.8 GHz. LC can also set the clock to a specific speed regardless of workload.</li>
<li>High-throughput on-chip fabric: Over 7 TB/s aggregate bandwidth via on-chip switch connecting cores to memory, PCIe, GPUs, etc.</li>
<li>Images:
<ul><li>Schematics of the POWER9 processor chip variants are shown below. Click for a larger version. (<em>Source: "POWER9 Processor for the Cognitive Era". IBM presentation by Brian Thompto. Hot Chips 28 Symposium, October 2016</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1086" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/power9scaleout-png-0">power9ScaleOut.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power9ScaleOut_0.png"><img alt="Power9 Scale Out" height="282" width="400" style="height: 282px; width: 400px;" class="media-element file-default" data-delta="77" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power9ScaleOut_0-400x282.png" /></a>  </div>

  
</div>
</div><br />Scale-Out Models</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1087" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/power9scaleup-png-0">power9ScaleUp.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power9ScaleUp_0.png"><img alt="Power9 Scale Up Model" height="270" width="400" style="width: 400px; height: 270px;" class="media-element file-default" data-delta="78" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power9ScaleUp_0-400x270.png" /></a>  </div>

  
</div>
</div><br />Scale-Up Models</div>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<ul><li>Images of the POWER9 processor chip die are shown below. Click for a larger version. (<em>Source: "POWER9 Processor for the Cognitive Era". IBM presentation by Brian Thompto. Hot Chips 28 Symposium, October 2016</em>)</li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1088" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/power9die1-jpg-0">power9die1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power9die1_0.jpg"><img alt="Power9 processor chip die1" height="370" width="400" style="height: 370px; width: 400px;" class="media-element file-default" data-delta="79" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power9die1_0-400x370.jpg" /></a>  </div>

  
</div>
</div></div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1089" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/power9die2-jpg-0">power9die2.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power9die2_0.jpg"><img alt="Power9 processor chip die2" height="371" width="400" style="height: 371px; width: 400px;" class="media-element file-default" data-delta="80" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power9die2_0-400x371.jpg" /></a>  </div>

  
</div>
</div></div>
<h4><strong>POWER9 Core Key Features</strong></h4>
<ul><li>The POWER9 processor core is a 64-bit implementation of the IBM Power Instruction Set Architecture (ISA) Version 3.0</li>
<li>Little Endian</li>
<li>8-way (SMT8) or 4-way (SMT4) hardware threads</li>
<li>Basic building block of both SMT4 and SMT8 cores is a slice:
<ul><li>A slice is a rudimentary 64-bit single threaded processing element with a load store unit (LSU), integer unit (ALU) and vector scalar unit (VSU, doing SIMD and floating point).</li>
<li>Two slices are combined to make a 128-bit "super-slice"</li>
<li>Both SMT4 and SMT8 cores contain the same number of slices (threads) = 96.</li>
</ul></li>
<li>Shorter fetch-to-compute pipeline than POWER8; reduced by 5 cycles.</li>
<li>Instructions per cycle: 128 for SMT8, 64 for SMT4</li>
<li>Images:
<ul><li>Schematic of a POWER9 SMT4 core is shown below. Click for a larger version. (<em>Source: "POWER9 Processor for the Cognitive Era". IBM presentation by Brian Thompto. Hot Chips 28 Symposium, October 2016</em>)</li>
</ul></li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-1011" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/power9smt4core-png">power9smt4core.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power9smt4core.png"><img alt="POWER9 SMT4 core" height="258" width="500" style="height: 258px; width: 500px;" class="media-element file-default" data-delta="30" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power9smt4core-500x258.png" /></a>  </div>

  
</div>
</div>
<h4><strong>References and More Information:</strong></h4>
<ul><li>"POWER9 Processor for the Cognitive Era". IBM presentation by Brian Thompto. Hot Chips 28 Symposium, October 2016</li>
<li><a href="https://en.wikichip.org/wiki/ibm/microarchitectures/power9" target="_blank">"POWER9 - Microarchitectures - IBM".</a> wikichip.org website.</li>
<li><a href="https://www.tomshardware.com/news/summit-supercomputer-nvidia-ibm-power9-volta,35962.html" target="_blank">"Regaining America's Supercomputing Supremacy With The Summit Supercomputer". </a>Paul Alcorn on the tomshardware.com website, November 20, 2017.</li>
<li><a href="https://www.nextplatform.com/2017/12/05/power9-to-the-people/" target="_blank">"POWER9 To The People".</a> Timothy Prickett Morgan on the nextplatform.com website, December 5, 2017.</li>
</ul><h3><a name="Pascal" id="Pascal"></a>NVIDIA Tesla P100 (Pascal) Architecture</h3>
<p><strong>Used by LLNL's Early Access systems ray, rzmanta, shark</strong></p>
<h4><strong>Tesla P100 Key Features</strong></h4>
<ul><li>"Extreme performance" for HPC and Deep Learning:
<ul><li>5.3 TFLOPS of double-precision floating point (FP64) performance</li>
<li>10.6 TFLOPS of single-precision (FP32) performance</li>
<li>21.2 TFLOPS of half-precision (FP16) performance</li>
</ul></li>
<li>NVLink: NVIDIA's high speed, high bandwidth interconnect
<ul><li>Connects multiple GPUs to each other, and GPUs to the CPUs</li>
<li>4 NVLinks per GPU</li>
<li>Up to 160 GB/s bidirectional bandwidth between GPUs (5x the bandwidth of PCIe Gen 3 x16)</li>
</ul></li>
<li>HBM2: High Bandwidth Memory 2
<ul><li>Memory is located on same physical package as the GPU, providing 3x the bandwidth of previous GPUs such as the Maxwell GM200</li>
<li>Highly tuned 16 GB HBM2 memory subsystem delivers 732 GB/sec peak memory bandwidth on Pascal.</li>
</ul></li>
<li>Unified Memory:
<ul><li>Significant advancement and a major new hardware and software-based feature of the Pascal GP100 GPU architecture.</li>
<li>First NVIDIA GPU to support hardware page faulting, and when combined with new 49-bit (512 TB) virtual addressing, allows transparent migration of data between the full virtual address spaces of both the GPU and CPU.</li>
<li>Provides a single, seamless unified virtual address space for CPU and GPU memory.</li>
<li>Greatly simplifies GPU programming - programmers no longer need to manage data sharing between two different virtual memory systems.</li>
</ul></li>
<li>Compute Preemption:
<ul><li>New hardware and software feature that allows compute tasks to be preempted at instruction-level granularity.</li>
<li>Prevents long-running applications from either monopolizing the system or timing out. For example, both interactive graphics tasks and interactive debuggers can run simultaneously with long-running compute tasks.</li>
</ul></li>
<li>Images:
<ul><li>NVIDIA Tesla P100 with Pascal GP100 GPU. Click for larger image. (<em>Source: NVIDIA Tesla P100 Whitepaper. NVIDIA publication WP-08019-001_v01.1. 2016</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1090" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/teslap100front1-jpg-0">teslaP100front1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/teslaP100front1_0.jpg"><img alt="NVIDIA Tesla P100 with Pascal GP100 GPU front" height="223" width="400" style="height: 223px; width: 400px;" class="media-element file-default" data-delta="81" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/teslaP100front1_0-400x223.jpg" /></a>  </div>

  
</div>
</div><br />Front</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1091" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/teslap100back1-jpg-0">teslaP100back1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/teslaP100back1_0.jpg"><img alt="NVIDIA Tesla P100 with Pascal GP100 GPU back" height="224" width="400" style="height: 224px; width: 400px;" class="media-element file-default" data-delta="82" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/teslaP100back1_0-400x224.jpg" /></a>  </div>

  
</div>
</div><br />Back</div>
<div class="clear-floats"> </div>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<ul><li>IBM Power System S822LC with two IBM POWER8 CPUs and four NVIDIA Tesla P100 GPUs connected via NVLink. Click for larger image. (<em>Source: <a href="https://blogs.nvidia.com/blog/2016/09/08/ibm-servers-nvlink/">https://blogs.nvidia.com/blog/2016/09/08/ibm-servers-nvlink/</a></em>)</li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-1014" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/power8withpascals1-jpg">power8withPascals1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/power8withPascals1.jpg"><img alt="IBM POWER8 with PASCALs" height="245" width="400" style="width: 400px; height: 245px;" class="media-element file-default" data-delta="33" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/power8withPascals1-400x245.jpg" /></a>  </div>

  
</div>
</div>
<h4><strong>Pascal GP100 GPU Components</strong></h4>
<ul><li>A full GP100 includes 6 Graphics Processing Clusters (GPC)</li>
<li>Each GPC has 10 Pascal Streaming Multiprocessors (SM) for a total of 60 SMs</li>
<li>Each SM has:
<ul><li>64 single-precision CUDA cores for a total of 3840 single-precision cores</li>
<li>4 Texture Units for a total of 240 texture units</li>
<li>32 double-precision units for a total of 1920 double-precision units</li>
<li>16 load/store units, 16 special function units, register files, instruction buffers and cache, warp schedulers and dispatch units</li>
</ul></li>
<li>L2 cache size of 4096 KB</li>
<li><span class="note-green">Note</span> the Tesla P100 does not use a full Pascal GP100. It uses 56 SMs instead of 60, for a total core count of 3584</li>
<li>Images:
<ul><li>Diagrams of a full Pascal GP100 GPU and a single SM. Click for larger image. (<em>Source: NVIDIA Tesla P100 Whitepaper. NVIDIA publication WP-08019-001_v01.1. 2016</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1092" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/teslap100blockdiagram1-png-0">teslaP100blockdiagram1.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/teslaP100blockdiagram1_0.png"><img alt="Pascal GP100 Full GPU with 60 SM Units" height="235" width="400" style="height: 235px; width: 400px;" class="media-element file-default" data-delta="83" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/teslaP100blockdiagram1_0-400x235.png" /></a>  </div>

  
</div>
</div><br />Pascal GP100 Full GPU with 60 SM Units</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1093" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/pascalgp100-sm-png-1">pascalGP100-SM.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/pascalGP100-SM_1.png"><img alt="Pascal GP100 SM Unit" height="289" width="400" style="height: 289px; width: 400px;" class="media-element file-default" data-delta="84" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/pascalGP100-SM_1-400x289.png" /></a>  </div>

  
</div>
</div><br />Pascal GP100 SM Unit</div>
<h4><strong>References and More Information</strong></h4>
<ul><li>NVIDIA Whitepaper: <a href="https://hpc.llnl.gov/sites/default/files/pascal-architecture-whitepaper_0.pdf" target="_blank">"NVIDIA Tesla P100".</a> Publication WP-08019-001_v01.1. 2016.</li>
<li>NVIDIA developers blog: <a href="https://devblogs.nvidia.com/inside-pascal/" target="_blank">"Inside Pascal: NVIDIA's Newest Computing Platform"</a> by Mark Harris, NVIDIA. June 19, 2016.</li>
</ul><h3><a name="Volta" id="Volta"></a>NVIDIA Tesla V100 (Volta) Architecture</h3>
<p><strong>Used by LLNL's Sierra systems sierra, lassen, rzansel</strong></p>
<h4><strong>Tesla P100 Key Features</strong></h4>
<ul><li>New Streaming Multiprocessor (SM) Architecture Optimized for Deep Learning:
<ul><li>50% more energy efficient than the previous generation Pascal design, enabling major boosts in FP32 and FP64 performance in the same power envelope.</li>
<li>Tensor Cores designed specifically for deep learning deliver up to 12x higher peak TFLOPS for training and 6x higher peak TFLOPS for inference.</li>
<li>With independent parallel integer and floating-point data paths, the Volta SM is also much more efficient on workloads with a mix of computation and addressing calculations.</li>
<li>Independent thread scheduling capability enables finer-grain synchronization and cooperation between parallel threads.</li>
<li>Combined L1 data cache and shared memory unit significantly improves performance while also simplifying programming.</li>
</ul></li>
<li>Performance:
<ul><li>7.8 TFLOPS of double-precision floating point (FP64) performance</li>
<li>15.7 TFLOPS of single-precision (FP32) performance</li>
<li>125 Tensor TFLOPS</li>
</ul></li>
<li>Second-Generation NVIDIA NVLink:
<ul><li>Delivers higher bandwidth, more links, and improved scalability for multi-GPU and multi-GPU/CPU system configurations.</li>
<li>Supports up to six NVLink links and total bandwidth of 300 GB/sec, compared to four NVLink links and 160 GB/s total bandwidth on Pascal.</li>
<li>Now supports CPU mastering and cache coherence capabilities with IBM Power 9 CPU-based servers.</li>
<li>The new NVIDIA DGX-1 with V100 AI supercomputer uses NVLink to deliver greater scalability for ultra-fast deep learning training.</li>
</ul></li>
<li>HBM2 Memory: Faster, Higher Efficiency
<ul><li>Highly tuned 16 GB HBM2 memory subsystem delivers 900 GB/sec peak memory bandwidth.</li>
<li>The combination of both a new generation HBM2 memory from Samsung, and a new generation memory controller in Volta, provides 1.5x delivered memory bandwidth versus Pascal GP100, with up to 95% memory bandwidth utilization running many workloads.</li>
</ul></li>
<li>Volta Multi-Process Service (MPS):
<ul><li>Enables multiple compute applications to share GPUs.</li>
<li>Volta MPS also triples the maximum number of MPS clients from 16 on Pascal to 48 on Volta.</li>
</ul></li>
<li>Enhanced Unified Memory and Address Translation Services:
<ul><li>Provides a single, seamless unified virtual address space for CPU and GPU memory.</li>
<li>Greatly simplifies GPU programming - programmers no longer need to manage data sharing between two different virtual memory systems.</li>
<li>Includes new access counters to allow more accurate migration of memory pages to the processor that accesses them most frequently, improving efficiency for memory ranges shared between processors.</li>
<li>On IBM Power platforms, new Address Translation Services (ATS) support allows the GPU to access the CPU's page tables directly.</li>
</ul></li>
<li>Maximum Performance and Maximum Efficiency Modes:
<ul><li>In Maximum Performance mode, the Tesla V100 accelerator will operate up to its TDP (Thermal Design Power) level of 300 W to accelerate applications that require the fastest computational speed and highest data throughput.</li>
<li>Maximum Efficiency Mode allows data center managers to tune power usage of their Tesla V100 accelerators to operate with optimal performance per watt. A not-to-exceed power cap can be set across all GPUs in a rack, reducing power consumption dramatically, while still obtaining excellent rack performance.</li>
</ul></li>
<li>Cooperative Groups and New Cooperative Launch APIs:
<ul><li>Cooperative Groups is a new programming model introduced in CUDA 9 for organizing groups of communicating threads.</li>
<li>Allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions.</li>
<li>Basic Cooperative Groups functionality is supported on all NVIDIA GPUs since Kepler. Pascal and Volta include support for new cooperative launch APIs that support synchronization amongst CUDA thread blocks. Volta adds support for new synchronization patterns.</li>
</ul></li>
<li>Volta Optimized Software:
<ul><li>New versions of deep learning frameworks such as Caffe2, MXNet, CNTK, TensorFlow, and others harness the performance of Volta to deliver dramatically faster training times and higher multi-node training performance.</li>
<li>Volta-optimized versions of GPU accelerated libraries such as cuDNN, cuBLAS, and TensorRT leverage the new features of the Volta GV100 architecture to deliver higher performance for both deep learning inference and High Performance Computing (HPC) applications.</li>
<li>The NVIDIA CUDA Toolkit version 9.0 includes new APIs and support for Volta features to provide even easier programmability.</li>
</ul></li>
<li>Images:
<ul><li>NVIDIA Tesla V100 with Volta GV100 GPU. Click for larger image. (<em>Source: NVIDIA Tesla V100 Whitepaper. NVIDIA publication WP-08608-001_v1.1. August 2017</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1094" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/teslav100front1-jpg-0">teslaV100front1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/teslaV100front1_0.jpg"><img alt="NVIDIA Tesla V100 Front" height="211" width="400" style="height: 211px; width: 400px;" class="media-element file-default" data-delta="85" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/teslaV100front1_0-400x211.jpg" /></a>  </div>

  
</div>
</div><br />Front</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1095" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/teslav100back1-jpg-0">teslaV100back1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/teslaV100back1_0.jpg"><img alt="NVIDIA Tesla P100 back image" height="210" width="400" style="height: 210px; width: 400px;" class="media-element file-default" data-delta="86" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/teslaV100back1_0-400x210.jpg" /></a>  </div>

  
</div>
</div><br />Back</div>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<ul><li>IBM Power System AC922 with two IBM POWER9 CPUs and four NVIDIA Tesla V100 GPUs connected via NVLink. </li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-1020" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/power9withvoltas1-jpg">power9withVoltas1.jpg</a></h2>
    
  
  <div class="content">
    <img alt="IBM Power9 with Voltas" height="277" width="413" class="media-element file-default" data-delta="39" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/power9withVoltas1.jpg" /></div>

  
</div>
</div>
<h4><strong>Volta GV100 GPU Components</strong></h4>
<ul><li>A full GV100 includes 6 Graphics Processing Clusters (GPC)</li>
<li>Each GPC has 14 Volta Streaming Multiprocessors (SM) for a total of 84 SMs</li>
<li>Each SM has:
<ul><li>64 single-precision floating-point cores; GPU total of 5376</li>
<li>64 single-precision integer cores; GPU total of 5376</li>
<li>32 double-precision floating-point cores; GPU total of 2688</li>
<li>8 Tensor Cores; GPU total of 672</li>
<li>4 Texture Units; GPU total of 168</li>
<li>32 load/store units, 4 special function units, register files, instruction buffers and cache, warp schedulers and dispatch units</li>
</ul></li>
<li>L2 cache size of 6144 KB</li>
<li><span class="note-green">Note</span> the Tesla V100 does not use a full Volta GV100. It uses 80 SMs instead of 84, for a total "CUDA" core count of 5120 versus 5376.</li>
<li>Images:
<ul><li>Diagrams of a full Volta GV100 GPU and a single SM. Click for larger image. (<em>Source: NVIDIA Tesla V100 Whitepaper. NVIDIA publication WP-08608-001_v1.1. August 2017</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1096" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/teslav100blockdiagram1-png-0">teslaV100blockdiagram1.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/teslaV100blockdiagram1_0.png"><img alt="Volta GV100 Full GPU with 84 SM Units" height="235" width="400" style="height: 235px; width: 400px;" class="media-element file-default" data-delta="87" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/teslaV100blockdiagram1_0-400x235.png" /></a>  </div>

  
</div>
</div><br />Volta GV100 Full GPU with 84 SM Units</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1097" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/voltagv100-sm-png-0">voltaGV100-SM.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/voltaGV100-SM_0.png"><img alt="Volta GV100 SM Unit" height="533" width="400" style="height: 533px; width: 400px;" class="media-element file-default" data-delta="88" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/voltaGV100-SM_0-400x533.png" /></a>  </div>

  
</div>
</div><br />Volta GV100 SM Unit</div>
<h4><strong>References and More Information</strong></h4>
<ul><li>NVIDIA Whitepaper: <a href="https://hpc.llnl.gov/sites/default/files/volta-architecture-whitepaper.pdf" target="_blank">"NVIDIA Tesla V100 GPU Architecture".</a> Publication WP-08608-001_v1.1. August 2017.</li>
<li>NVIDIA developers blog: <a href="https://devblogs.nvidia.com/inside-volta/" target="_blank">"Inside Volta: The World's Most Advanced Data Center GPU"</a> by Luke Durant, Olivier Giroux, Mark Harris and Nick Stam, NVIDIA. May 10, 2017.</li>
</ul><h3><a name="NVLink" id="NVLink"></a>NVLink</h3>
<h4><strong>Overview</strong></h4>
<ul><li>NVLink is NVIDIA's high-speed interconnect technology for GPU accelerated computing. Used to connect GPUs to GPUs and/or GPUs to CPUs.</li>
<li>Significantly increases performance for both GPU-to-GPU and GPU-to-CPU communications.</li>
<li>NVLink - first generation
<ul><li>Debuted with Pascal GPUs</li>
<li>Used on LC's Early Access systems (ray, rzmanta, shark)</li>
<li>Supports up to 4 NVLink links per GPU.</li>
<li>Each link provides a 40 GB/s bidirectional connection to another GPU or a CPU, yielding an aggregate bandwidth of 160 GB/s.</li>
</ul></li>
<li>NVLink 2.0 - second generation
<ul><li>Debuted with Volta GPUs</li>
<li>Used on LC's Sierra systems (sierra, lassen, rzansel)</li>
<li>Supports up to 6 NVLink links per GPU.</li>
<li>Each link provides a 50 GB/s bidirectional connection to another GPU or a CPU, yielding an aggregate bandwidth of 300 GB/s.</li>
</ul></li>
<li>Multiple links can be "ganged" to increase bandwidth between two endpoints</li>
<li>Numerous NVLink topologies are possible, and different configurations can be optimized for different applications.</li>
<li><strong>LC's NVLink configurations:</strong>
<ul><li>Early Access systems (ray, rzmanta, shark): Each CPU is connected to 2 GPUs by 2 NVLinks each. Those GPUs are connected to each other by 2 NVLinks each</li>
<li>Sierra systems (sierra, lassen, rzansel): Each CPU is connected to 2 GPUs by 3 NVLinks each. Those GPUs are connected to each other by 3 NVLinks each</li>
<li>GPUs on different CPUs do not connect to each other with NVLinks</li>
</ul></li>
<li>Images:
<ul><li>Two representative NVLink 2.0 topologies are shown below. (<em>Source: NVIDIA Tesla V100 Whitepaper. NVIDIA publication WP-08608-001_v1.1. August 2017</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1098" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/nvlinkconfig1-png-0">NVLinkConfig1.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/NVLinkConfig1_0.png"><img alt="V100 with NVLink Connected GPU-to-GPU and GPU-to-CPU (LC&amp;#039;s Sierra systems)" height="325" width="326" style="height: 325px; width: 326px;" class="media-element file-default" data-delta="89" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/NVLinkConfig1_0-326x325.png" /></a>  </div>

  
</div>
</div><br />V100 with NVLink Connected GPU-to-GPU and GPU-to-CPU<br />(LC's Sierra systems)</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1099" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/nvlinkconfig2-png-0">NVLinkConfig2.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/NVLinkConfig2_0.png"><img alt="Hybrid Cube Mesh NVLink GPU-to-GPU Topology with V100" height="325" width="400" style="height: 325px; width: 400px;" class="media-element file-default" data-delta="90" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/NVLinkConfig2_0-400x325.png" /></a>  </div>

  
</div>
</div><br />Hybrid Cube Mesh NVLink GPU-to-GPU Topology with V100</div>
<h4><strong>References and More Information</strong></h4>
<ul><li>NVIDIA Whitepaper: <a href="https://hpc.llnl.gov/sites/default/files/volta-architecture-whitepaper.pdf" target="_blank">"NVIDIA Tesla V100 GPU Architecture".</a> Publication WP-08608-001_v1.1. August 2017.</li>
<li>NVIDIA Whitepaper: <a href="https://hpc.llnl.gov/sites/default/files/pascal-architecture-whitepaper_0.pdf" target="_blank">"NVIDIA Tesla P100".</a> Publication WP-08019-001_v01.1. 2016.</li>
</ul><h3><a name="Mellanox" id="Mellanox"></a>Mellanox EDR InfiniBand Network</h3>
<h4><strong>Hardware</strong></h4>
<ul><li>Mellanox EDR InfiniBand is used for both Early Access and Sierra systems:
<ul><li>EDR = Enhanced Data Rate</li>
<li>100 Gb/s bandwidth rating</li>
</ul></li>
<li>Adapters:
<ul><li>Nodes have one dual-port Mellanox ConnectX EDR Infiniband adapter (at LC)</li>
<li>Both PCIe Gen 3.0 and Gen 4.0 capable</li>
<li>Adapter ports connect to level 1 switches</li>
</ul></li>
<li>Top-of-Rack (TOR) level 1 (edge) switches:
<ul><li>Mellanox Switch-IB with 36 ports</li>
<li>Down ports connect to node adapters</li>
<li>Up ports connect to level 2 switches</li>
</ul></li>
<li>Director level 2 (core) switches:
<ul><li>Mellanox CS7500 with 648 ports</li>
<li>Holds 18 Mellanox Switch-IB 36-port leafs</li>
<li>Ports connect down to level 1 switches</li>
</ul></li>
<li>Images:
<ul><li>Mellanox EDR InfiniBand network hardware components are shown below. Click for larger image. (<em>Source: mellanox.com</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1100" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mellanoxconnectx-5adapter-jpg-0">mellanoxConnectX-5adapter.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/mellanoxConnectX-5adapter_0.jpg"><img alt="Mellanox ConnectX dual-port IB adapter" height="189" width="240" style="font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; font-size: 14px; font-family: Oxygen, sans-serif; width: 240px; height: 189px;" class="media-element file-default" data-delta="91" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/mellanoxConnectX-5adapter_0-240x189.jpg" /></a>  </div>

  
</div>
</div><br /><span>Mellanox ConnectX dual-port I</span><span>B adapter</span></div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1101" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mellanoxswitch-ib-2-jpg-0">mellanoxSwitch-IB-2.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/mellanoxSwitch-IB-2_0.jpg"><img alt="Mellanox Switch-IB Top-of-Rack (edge) switches" height="81" width="240" style="font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; font-size: 14px; font-family: Oxygen, sans-serif; background-color: transparent; height: 81px; width: 240px;" class="media-element file-default" data-delta="92" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/mellanoxSwitch-IB-2_0-240x81.jpg" /></a>  </div>

  
</div>
</div><br />Mellanox Switch-IB Top-of-Rack (edge) switches</div>
<div class="clear-float"> </div>
<div class="float-left"><br /><div class="media media-element-container media-default"><div id="file-1103" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mellanoxcs7500-labeled-jpg-0">mellanoxCS7500-labeled.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/mellanoxCS7500-labeled_0.jpg"><img alt="Mellanox CS7500 labeled" height="290" width="240" style="font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; font-size: 14px; font-family: Oxygen, sans-serif; height: 290px; width: 240px;" class="media-element file-default" data-delta="94" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/mellanoxCS7500-labeled_0-240x290.jpg" /></a>  </div>

  
</div>
</div><br />Mellanox CS7500 labeled<br /> </div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1102" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mellanoxcs7500-jpg-0">mellanoxCS7500.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/mellanoxCS7500_0.jpg"><img alt="Mellanox CS7500 Director (core) switch" height="451" width="175" style="font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; font-size: 14px; font-family: Oxygen, sans-serif; height: 451px; width: 175px;" class="media-element file-default" data-delta="93" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/mellanoxCS7500_0-175x451.jpg" /></a>  </div>

  
</div>
</div><br />Mellanox CS7500 Director (core) switch</div>
<h4><strong>Topology and LC Sierra Configuration</strong></h4>
<ul><li>Tapered Fat Tree, Single Plane Topology
<ul><li>Fat Tree: switches form a hierarchy with higher level switches having more (hence, fat) connections down than lower level switches.</li>
<li>Tapered: the number of connections down for lower level switches are increased by a ratio of two-to-one.</li>
<li>Single Plane: nodes connect to a single fat tree network.</li>
</ul></li>
<li>Sierra configuration details:
<ul><li>Each rack has 18 nodes and 2 TOR switches</li>
<li>Each node's dual-port adapter connects to both of its rack's TOR switches with one port each. That equals 18 uplinks to each TOR within a rack.</li>
<li>Each TOR switch has 12 uplinks to Director switches, at least one per Director switch</li>
<li>There are 9 Director switches</li>
<li>Because each TOR switch has 12 uplinks and there are only 9 Director switches, there are 3 extra uplinks per TOR switch. These are used to connect twice to 3 of the 9 Director switches.</li>
<li><span class="note-green">Note</span> Sierra has a "modified" 2:1 Tapered Fat Tree. It's actually 1.5 to 1 (18 links down, 12 links up for each TOR switch).</li>
</ul></li>
<li>At LC, adapters connect to level 1 switches via copper cable. Level 1 switches connect to level 2 switches via optic fiber.</li>
<li>Images:
<ul><li>Topology diagrams shown below. Click for larger image.</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1104" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/fattreediagram-png-1">fatTreeDiagram.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/fatTreeDiagram_1.png"><img alt="Fat Tree Network" height="169" width="400" style="height: 169px; width: 400px;" class="media-element file-default" data-delta="95" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/fatTreeDiagram_1-400x169.png" /></a>  </div>

  
</div>
</div><br />Fat Tree Network</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1105" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/sierranetwork-jpg-0">sierraNetwork.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierraNetwork_0.jpg"><img alt="Sierra Network" height="364" width="400" style="height: 364px; width: 400px;" class="media-element file-default" data-delta="96" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierraNetwork_0-400x364.jpg" /></a>  </div>

  
</div>
</div><br />Sierra Network</div>
<h4><strong>References and More Information</strong></h4>
<ul><li><a href="https://hpc.llnl.gov/sites/default/files/Mellanox.CS7500switch.pdf" target="_blank">Mellanox CS7500 InfiniBand Switch Brochure</a>. Mellanox Technologies 2017.</li>
</ul><h3><a name="NVMe" id="NVMe"></a>NVMe PCIe SSD (Burst Buffer)</h3>
<h4><strong>Overview</strong></h4>
<ul><li>NVMe PCIe SSD:
<ul><li>SSD = Solid State Drive; non-volatile storage device with no moving parts</li>
<li>PCIe = Peripheral Component Interconnect Express; standard high-speed serial bus connection.</li>
<li>NVMe = Non-Volatile Memory Express; device interface specification for accessing non-volatile storage media attached via PCIe bus</li>
</ul></li>
<li>Fast and intermediate storage layer positioned between the front-end computing processes and the back-end storage systems.</li>
<li>Primary purpose of this fast storage is to act as a "Burst Buffer" for improving I/O performance. Computation can continue while the fast SSD "holds" data (such as checkpoint files) being written to slower disk.</li>
<li>Mounted as a file system local to a compute node (not global storage).<br /><div class="media media-element-container media-default"><div id="file-1942" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/burstbufferarch-png">burstbufferArch.png</a></h2>
    
  
  <div class="content">
    <img alt="Sierra Burst Buffer Architecture Diagram" height="269" width="780" style="width: 780px; height: 269px;" class="media-element file-default" data-delta="105" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/burstbufferArch.png" /></div>

  
</div>
</div></li>
<li>Sierra systems (sierra, lassen, rzansel):
<ul><li>Compute nodes have 1.6 TB SSD.</li>
<li>The login and launch nodes also have this SSD, but from a user perspective, it's not really usable.</li>
<li>Managed via the LSF scheduler.</li>
</ul></li>
<li>CORAL Early Access systems:
<ul><li>Ray compute nodes have 1.6 TB SSD. The shark and rzmanta systems do not have SSD.</li>
<li>Mounted under <span class="fixed">/l/nvme</span> (lower case "L" / nvme)</li>
<li>Users can write/read directly to this location</li>
<li>Unlike Sierra systems, it is not managed via LSF</li>
</ul></li>
<li>As with all SSDs, life span is shortened with writes</li>
<li>Performance:  the Samsung literature (see References below) cites different performance numbers for the SSD used in Sierra systems. Both are shown below:<br /><table class="table table-bordered"><tr><th scope="col"><strong>Samsung PM1725a brochure</strong></th>
<th scope="col"><strong>Samsung PM1725a data sheet</strong></th>
</tr><tr><td>6400 MB/s Sequential Read BW</td>
<td>5840 MB/s Sequential Read BW</td>
</tr><tr><td>3000 MB/s Sequential Write BW</td>
<td>2100 MB/s Sequential Write BW</td>
</tr><tr><td>1080K IOPS Random Read</td>
<td>1000K IOPS Random Read</td>
</tr><tr><td>170K IOPS Random Write</td>
<td>140K IOPS Random Write</td>
</tr></table></li>
<li>Usage information:
<ul><li>See the <a href="#BurstBuffer">Burst Buffer Usage</a> section of this tutorial</li>
<li>Sierra confluence wiki:  <a href="https://lc.llnl.gov/confluence/display/SIERRA/Burst+Buffers" target="_blank">https://lc.llnl.gov/confluence/display/SIERRA/Burst+Buffers</a>.</li>
</ul></li>
<li>Images:
<ul><li>1.6 TB NVMe PCIe SSD. Click for larger image. (<em>Sources: samsung.com and hgst.com</em>)</li>
</ul></li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1106" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/samsungpm1725nvme-jpg-0">samsungPM1725nvme.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/samsungPM1725nvme_0.jpg"><img alt="Samsung PM1725" height="200" width="300" style="height: 200px; width: 300px;" class="media-element file-default" data-delta="97" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/samsungPM1725nvme_0-300x200.jpg" /></a>  </div>

  
</div>
</div><br />Samsung PM1725</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1107" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/ultrastarsn100nvme2-jpg-0">ultrastarSN100nvme2.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/ultrastarSN100nvme2_0.jpg"><img alt="HGST Ultrastar SN100 (front)" height="191" width="300" style="height: 191px; width: 300px;" class="media-element file-default" data-delta="98" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/ultrastarSN100nvme2_0-300x191.jpg" /></a>  </div>

  
</div>
</div><br />HGST Ultrastar SN100 (front)</div>
<div class="float-left"><div class="media media-element-container media-default"><div id="file-1108" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/ultrastarsn100nvme1-jpg-0">ultrastarSN100nvme1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/ultrastarSN100nvme1_0.jpg"><img alt="HGST Ultrastar SN100 (back)" height="201" width="300" style="height: 201px; width: 300px;" class="media-element file-default" data-delta="99" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/ultrastarSN100nvme1_0-300x201.jpg" /></a>  </div>

  
</div>
</div><br />HGST Ultrastar SN100 (back)</div>
<h4><strong>References and More Information</strong></h4>
<ul><li><a href="https://hpc.llnl.gov/sites/default/files/SamsungPM1725aNVMeSSDbrochure.pdf" target="_blank">Samsung PM1725 Brochure. </a>SSD used on Sierra systems.</li>
<li>Samsung 1.6TB HHHL PM1725a data sheet:  <a href="http://www.samsung.com/semiconductor/ssd/enterprise-ssd/MZPLL1T6HEHP/" target="_blank">http://www.samsung.com/semiconductor/ssd/enterprise-ssd/MZPLL1T6HEHP/</a></li>
<li><a href="https://hpc.llnl.gov/sites/default/files/Ultrastar-SN100-Series-NVMe-PCIe-SSD-DataSheet.pdf" target="_blank">HGST Ultrastar SN100 Data Sheet.</a> SSD used on the Ray system.</li>
</ul><h2><a name="Accounts" id="Accounts"></a>Accounts, Allocations and Banks</h2>
<h3><strong> Accounts</strong></h3>
<ul><li>Only a brief summary of LC account request procedures is included below. For details, see: <a href="https://hpc.llnl.gov/accounts" target="_blank">https://hpc.llnl.gov/accounts</a></li>
<li>Sierra:
<ul><li>Sierra is considered a Tri-lab Advanced Technology System (ATS).</li>
<li>Accounts on the classified sierra system are restricted to approved Tri-lab (LLNL, LANL, SNL) users.</li>
<li>Guided by the ASC Advanced Technology Computing Campaign (ATCC) proposal process and usage model.</li>
</ul></li>
<li>Accounts for the other Sierra systems (lassen, rzansel) and Early Access systems (ray, shark, rzmanta) follow the usual account request processes, summarized below.</li>
<li>LLNL and Collaborators:
<ul><li>Go to <a href="https://lc-idm.llnl.gov/" target="_blank">https://lc-idm.llnl.gov</a></li>
<li>OCF resource: lassen, rzansel, ray, rzmanta</li>
<li>SCF resource: shark</li>
</ul></li>
<li>LANL and Sandia:
<ul><li>Go to <a href="https://rails-rn-prod.sandia.gov/sarape" target="_blank">https://sarape.sandia.gov</a></li>
<li>LLNL resources: lassen, rzansel, ray, rzmanta and shark (depending on clearance/citizenship)</li>
<li>Sponsor: Greg Tomaschke, <a href="mailto:tomaschke1@llnl.gov">tomaschke1@llnl.gov</a>, 925-423-0561</li>
</ul></li>
<li>PSAAP centers:
<ul><li>Go to <a href="https://rails-rn-prod.sandia.gov/sarape" target="_blank">https://sarape.sandia.gov</a></li>
<li>LLNL resources: lassen, ray</li>
<li>Sponsor: Blaise Barney, <a href="mailto:blaiseb@llnl.gov">blaiseb@llnl.gov</a>, 925-422-2578</li>
</ul></li>
<li>For any questions or problems regarding accounts, please contact the LC Hotline account specialists:
<ul><li>Email: <a href="mailto://lc-support.llnl.gov">lc-support.llnl.gov</a></li>
<li>Phone: 925-422-4533</li>
</ul></li>
</ul><h3><strong>Allocations and Banks</strong></h3>
<ul><li>Sierra allocations and banks follow the ASC Advanced Technology Computing Campaign (ATCC) proposal process and usage model
<ul><li>Approved ATCC proposals are provided with an atcc bank / allocation</li>
<li>Additionally, ASC executive discretionary banks (lanlexec, llnlexec and snlexec) are provided for important Tri-lab work not falling explicitly under an ATCC proposal.</li>
</ul></li>
<li>Lassen is similar to other LC systems - users need to be in a valid "bank" in order to run jobs.</li>
<li>Rzansel and the CORAL EA systems currently use a "guests" group/bank for most users.</li>
</ul><h3><strong>Bank Related Commands</strong></h3>
<ul><li>IBM's Spectrum LSF software is used to schedule/manage jobs run on all Sierra systems.  LSF is very different than Slurm used on other LC systems. </li>
<li>Familiar Slurm commands for getting bank and usage information are not available. </li>
<li>The most useful command to obtain bank allocation and usage information is the LC developed <span class="fixed">lshare</span> command.</li>
<li>The <span class="fixed">lshare</span> command and several other related commands are discussed in the <a href="#BanksJobUsage">Banks, Job Usage and Job History Information</a> section of this tutorial.</li>
</ul><h2><a name="Access" id="Access"></a>Accessing LC's Sierra Machines<strong>​</strong></h2>
<h3><strong>Overview</strong></h3>
<ul><li>The instructions below summarize the basics for connecting to LC's Sierra systems. Additional access related information can be found at:
<ul><li>LLNL: <a href="https://hpc.llnl.gov/manuals/access-lc-systems" target="_blank">https://hpc.llnl.gov/manuals/access-lc-systems</a>.</li>
<li>LANL: <a href="https://hpc.lanl.gov/networks/red-network/red-network-tri-lab-user-access.html" target="_blank">https://hpc.lanl.gov/networks/red-network/red-network-tri-lab-user-access.html</a> (requires LANL authentication)</li>
<li>Sandia: <a href="https://hpc.sandia.gov/access/index.html" target="_blank">https://hpc.sandia.gov/access/index.html</a></li>
</ul></li>
<li>SSH (version 2) is used to connect to all LC machines:
<ul><li>From a terminal window command line, simply <span class="fixed">ssh <em>machinename</em></span>, where machinename is the name of the cluster.</li>
<li>SSH keys can be used between LC machines only. Instructions can be found at: <a href="https://hpc.llnl.gov/manuals/access-lc-systems/setting-ssh-keys" target="_blank">https://hpc.llnl.gov/manuals/access-lc-systems/setting-ssh-keys</a>.</li>
<li>Additional SSH details can be found at <a href="https://computing.llnl.gov/tutorials/lc_resources/#ssh" target="_blank">https://computing.llnl.gov/tutorials/lc_resources/#ssh</a></li>
</ul></li>
</ul><div class="float-right"><div class="media media-element-container media-default"><div id="file-1039" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/screen-shot-2018-05-09-1-19-12-pm-png">Screen Shot 2018-05-09 at 1.19.12 PM.png</a></h2>
    
  
  <div class="content">
    <img alt="CS/SCF and RZ RSA tokens" height="76" width="320" class="media-element file-default" data-delta="56" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/Screen%20Shot%202018-05-09%20at%201.19.12%20PM.png" /></div>

  
</div>
</div></div>
<ul><li class="clear-floats">RSA tokens are used for authentication:
<ul><li class="clear-floats">Static 4-8 character PIN + 6 digits from token</li>
<li class="clear-floats">There is one token for the CZ and SCF, and one token for the RZ.</li>
<li class="clear-floats">Sandia / LANL Tri-lab logins can be done without tokens</li>
</ul></li>
<li>Machine names and login nodes:
<ul><li>Each system has a single cluster login name, such as <span class="fixed">sierra</span>, <span class="fixed">lassen</span>, <span class="fixed">ray</span>, etc.</li>
<li>A full <span class="fixed">llnl.gov</span> domain name is required if coming from outside LLNL.</li>
<li>Successfully logging into the cluster will place you on one of the available login nodes.</li>
<li>User logins are distributed across login nodes for load balancing.</li>
<li>To view available login nodes use the <span class="fixed">nodeattr -c login</span> command.</li>
<li>You can ssh from one login node to another, which may be useful if there are problems with the login node you are on.</li>
</ul></li>
<li>X11 Forwarding
<ul><li>In order to display GUIs back to your local workstation, your SSH session will need to have X11 Forwarding enabled.</li>
<li>This is easily done by including the <span class="fixed">-X </span>(uppercase X) or <span class="fixed">-Y</span> option with your ssh command. For example: <span class="fixed">ssh -X sierra.llnl.gov</span></li>
<li>Your local workstation will also need to have X server software running. This comes with Linux by default. For Macs, something like XQuartz (<a href="http://www.xquartz.org/" target="_blank">http://www.xquartz.org/</a>) can be used. For Windows, there are several options - LLNL provides X-Win32 with a site license.</li>
</ul></li>
<li>SSH Clients
<ul><li>Used instead of a terminal window SSH command - mostly applies to Windows machines.</li>
<li>You will need to follow the instructions for your specific client.</li>
<li>Instructions for using X-Win32, provided by LLNL, can be found at: <a href="https://hpc.llnl.gov/manuals/access-lc-systems/x-win32-configuration" target="_blank">https://hpc.llnl.gov/manuals/access-lc-systems/x-win32-configuration</a>.</li>
</ul></li>
</ul><h3><strong>How To Connect</strong></h3>
<ul><li>Use the table below to connect to LC's Sierra systems.</li>
</ul><table class="table table-striped table-bordered" summary="How to connect to LC's Sierra systems"><tr><th scope="row">Going to <strong>↓</strong> Coming from <strong>→</strong></th>
<th scope="col">LLNL</th>
<th scope="col">LANL/Sandia</th>
<th scope="col">Other/Internet</th>
</tr><tr><th scope="row">
<p>SCF</p>
<p>sierra<br />shark</p>
</th>
<td>
<ul><li>Need to be logged into an SCF network machine</li>
<li><span class="fixed">ssh <em>loginmachine</em></span> command, or connect to <em>machinename</em> via your local SSH application</li>
<li>Userid: LC username</li>
<li>Password: PIN + OTP token code</li>
</ul></td>
<td>
<ul><li>Login and kerberos authenticate with forwardable credentials (<span class="fixed">kinit -f</span>) on a local, classified network machine.</li>
<li>For LANL only: then connect to the LANL gateway:<br /><span class="fixed">ssh red-wtrw</span></li>
<li><span class="fixed">ssh -l <em>lc_userid loginmachnine</em>.llnl.gov</span></li>
<li>no password required</li>
</ul></td>
<td>
<ul><li>Login and authenticate on local Securenet attached machine</li>
<li><span class="fixed">ssh -l <em>lc_userid</em> <em>loginmachine</em>.llnl.gov</span></li>
<li>Password: PIN + OTP token code</li>
</ul></td>
</tr><tr><th scope="row">
<p>OCF-CZ</p>
<p>lassen<br />ray</p>
</th>
<td>
<ul><li>Need to be logged into an OCF network machine</li>
<li><span class="fixed">ssh </span><span class="fixed"><em>loginmachine</em></span> or connect via your local SSH application</li>
<li>Userid: LC username</li>
<li>Password: PIN + OTP token code</li>
</ul></td>
<td>
<ul><li>Begin on a LANL/Sandia iHPC login node. For example, at LANL start from <strong>ihpc-gate1.lanl.gov</strong>; at Sandia start from <strong>ihpc.sandia.gov</strong></li>
<li><span class="fixed">ssh -l <em><em>lc_userid</em> </em><em>loginmachine</em>.llnl.gov</span></li>
<li>no password required</li>
</ul></td>
<td>
<ul><li>Login to a local unclassified network machine</li>
<li><span class="fixed">ssh</span> using your LC username or connect via your local SSH application. For example:<br /><span class="fixed">ssh -l <em>lc_userid </em><em>loginmachine</em>.llnl.gov</span></li>
<li>Userid: LC username</li>
<li>Password: PIN + OTP token code</li>
</ul></td>
</tr><tr><th scope="row">
<p>OCF-RZ</p>
<p>rzansel<br />rzmanta</p>
</th>
<td>
<ul><li>Need to be logged into a machine that is <strong>not</strong> part of the OCF Collaboration Zone (CZ)</li>
<li><span class="fixed">ssh </span><span class="fixed"><em>loginmachine</em></span> or connect via your local SSH application</li>
<li>Userid: LC username</li>
<li>Password: PIN + RZ RSA Token</li>
</ul></td>
<td>
<ul><li>Begin on a LANL/Sandia iHPC login node. For example, at LANL start from <strong>ihpc-gate1.lanl.gov</strong>; at Sandia start from <strong>ihpc.sandia.gov</strong></li>
<li><span class="fixed"><span class="fixed">ssh -l <span class="fixed"><em>lc_userid</em></span> <span class="fixed"><em>loginmachine</em></span>.llnl.gov</span></span></li>
<li>Password: LLNL PIN + RZ RSA Token</li>
</ul><p><span class="note-green">*** Note: Effective Aug 2019 ***</span><br />LANL/Sandia users can ssh to RZ systems directly from their iHPC node. No need to connect to rzgw.llnl.gov first. .</p>
</td>
<td>
<ul><li>Start LLNL VPN client on local machine and authenticate to VPN with your LLNL OUN and PIN + OTP token code</li>
<li><span class="fixed"><strong>ssh -l <em><strong><em>lc_userid</em></strong> </em><strong><em>loginmachine</em></strong>.llnl.gov</strong></span> or connect via your local SSH application.</li>
<li>Userid: LC username</li>
<li>Password: PIN + RZ RSA Token</li>
</ul></td>
</tr></table><h2><a name="Software" id="Software"></a>Software and Development Environment</h2>
<h3><a name="similarities-differences" id="similarities-differences"></a><strong>Similarities and Differences</strong></h3>
<ul><li>The Sierra software and development environment is similar in a number of ways to LC's other production clusters. Common topics are briefly discussed below, and covered in more detail in the <a href="/training/tutorials/livermore-computing-resources-and-environment" target="_blank">Introduction to LC Resource</a>s tutorial.</li>
<li>Sierra systems are also <em><strong>very different</strong></em> from other LC systems in important ways. These differences are summarized below and covered in detail later in other sections.</li>
</ul><h4><a name="login-nodes" id="login-nodes"></a><strong>Login Nodes:</strong></h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1040" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/clusterlogin-gif">clusterLogin.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/clusterLogin_1.gif"><img alt="Sierra cluster login node" height="264" width="300" style="width: 300px; height: 264px;" class="media-element file-default" data-delta="57" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/clusterLogin_1-300x264.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>Each LC cluster has a single, unique hostname used for login connections. This is called the "cluster login".</li>
<li>The cluster login is actually an alias for the real login nodes. It "rotates" logins between the actual login nodes for load balancing purposes.</li>
<li>For example: <strong>sierra.llnl.gov</strong> is the cluster login which distributes user logins over any number of physical login nodes.</li>
<li>The number of physical login nodes on any given LC cluster varies.</li>
<li>Login nodes are where you perform interactive, non-cpu intensive work: launch tools, edit files, submit batch jobs, run interactive jobs, etc.
<ul><li>Shared by multiple users</li>
<li>Should not be used to run production or parallel jobs, or perform long running parallel compiles/builds. These activities can impact other users.</li>
</ul></li>
<li>Users don't need to know (in most cases) the actual login node they are rotated onto - unless there are problems. Using the <span class="fixed">hostname</span> command will indicate the actual login node name for support purposes.</li>
<li>If the login node you are on is having problems, you can <span class="fixed">ssh</span> directly to another one. To find the list of available login nodes, use the command: <span class="fixed"><span>nodeattr -c</span> login</span></li>
<li>Cross-compilation is not necessary on Sierra clusters because login nodes have the same architecture as compute nodes.</li>
</ul><h3><a name="launch-nodes" id="launch-nodes"></a><strong>Launch Nodes</strong></h3>
<ul><li>In addition to login nodes, Sierra systems have a set of nodes that are dedicated to launching and managing user jobs. These are called <strong><em>launch nodes</em></strong>.</li>
<li>Typically, users submit jobs from a login node:
<ul><li>Batch jobs: a job script is submitted with the <span class="fixed">bsub</span> command</li>
<li>Interactive jobs: a shell or xterm session is requested using the <span class="fixed">bsub</span> or<span class="fixed"> lalloc</span> commands</li>
</ul></li>
<li>The job is then migrated to a launch node where LSF takes over.  An allocation of compute node(s) is acquired.</li>
<li>Finally, the job is started on the compute node allocation
<ul><li>If it's a parallel job using the <span class="fixed">jsrun</span> command the parallel tasks will run on these nodes</li>
<li>Serial jobs and the actual job command script will run on the first compute node as a "private launch node" (by default at LC)</li>
</ul></li>
<li>Further details on launch nodes are discussed as relevant in the <a href="#Running">Running Jobs Section</a>.</li>
</ul><h3><a name="login-shells-files" id="login-shells-files"></a><strong>Login Shells and Files</strong></h3>
<ul><li>Your login shell is established when your LC account is initially setup. The usual login shells are supported:<br /><span class="fixed">/bin/bash<br />/bin/csh<br />/bin/ksh<br />/bin/sh<br />/bin/tcsh<br />/bin/zsh</span></li>
</ul><ul><li>All LC users automatically receive a set of login files. These include:</li>
</ul><p><span class="fixed">   .cshrc        .kshenv       .login        .profile<br />                 .kshrc        .logout<br />   .cshrc.linux  .kshrc.linux  .login.linux  .profile.linux</span></p>
<ul><li>Which files are of interest depend upon your shell</li>
<li>Note for bash and zsh users: LC does not provide <span class="fixed">.bashrc</span>, <span class="fixed">.bash_profile</span>, <span class="fixed">.zprofile</span> or <span class="fixed">.zshrc</span> files at this time.</li>
<li>These files and usage details are further discussed at: <a href="https://computing.llnl.gov/tutorials/lc_resources/#HomeDirectories" target="_blank">https://computing.llnl.gov/tutorials/lc_resources/#HomeDirectories</a>.</li>
</ul><h3><a name="operating-systems" id="operating-systems"></a><strong>Operating System</strong></h3>
<ul><li>Sierra systems run Red Hat Enterprise Linux (RHEL). The current version can be determined by using the command:  <span class="fixed">cat /etc/redhat-release</span></li>
<li>Although they do not run the standard TOSS stack like other LC Linux clusters, LC has implemented some TOSS configurations, such as using <span class="fixed">/usr/tce</span> instead of <span class="fixed">/usr/local</span>.</li>
</ul><h3><a name="batch-system" id="batch-system"></a><strong>Batch System</strong></h3>
<ul><li>Unlike most other LC clusters, Sierra systems do NOT use Slurm as their workload manager / batch system.</li>
<li>IBM's Platform LSF Batch System software is used to schedule/manage jobs run on all Sierra systems.</li>
<li>LSF is very different from Slurm:
<ul><li>Will require a bit of a learning curve for new users.</li>
<li>Existing job scripts will require modification.</li>
<li>Other scripts using Slurm commands will also require modification</li>
</ul></li>
<li>LSF is discussed in detail in the <a href="#Running">Running Jobs Section</a> of this tutorial.</li>
</ul><h3><a name="file-systems" id="file-systems"></a><strong>File Systems</strong></h3>
<ul><li>Sierra systems mount the usual LC file systems.</li>
<li>The only significant differences are:
<ul><li>Parallel file systems: IBM's Spectrum Scale product is used instead of Lustre.</li>
<li>NVMe SSD (burst buffer) storage is available</li>
</ul></li>
<li>Available file systems are summarized in the table below and discussed in more detail in the <a href="https://computing.llnl.gov/tutorials/lc_resources/#FileSystems" target="_blank">File Systems Section</a> of the Livermore Computing Resources and Environment tutorial.</li>
</ul><table class="table table-striped table-bordered" summary="Summary of Available File Systems"><tr><th scope="col">File System</th>
<th scope="col">Mount Points</th>
<th scope="col">Backed Up?</th>
<th scope="col">Purged?</th>
<th scope="col">Comments</th>
</tr><tr><td>Home directories</td>
<td><span class="fixed">/g/g0 - /g/g99</span></td>
<td>Yes</td>
<td>No</td>
<td>24 GB quota; safest file system; includes .snapshot directory for online backups</td>
</tr><tr><td>Workspace</td>
<td><span class="fixed">/usr/workspace/ws</span></td>
<td>No</td>
<td>No</td>
<td>1 TB quota for each user and each group; includes .snapshot directory for online backups</td>
</tr><tr><td>Local tmp</td>
<td><span class="fixed">/tmp<br />/usr/tmp<br />/var/tmp</span></td>
<td>No</td>
<td>Yes</td>
<td>Node local temporary file space; small; actually resides in node memory, not physical disk</td>
</tr><tr><td>Collaboration</td>
<td><span class="fixed">/usr/gapps<br />/usr/gdata<br />/collab/usr/gapps<br />/collab/usr/gdata</span></td>
<td>Yes</td>
<td>No</td>
<td>User managed application directories; intended for collaborative development and usage</td>
</tr><tr><td>Parallel</td>
<td>
<p><span class="fixed">/p/gpfs1<br />/p/gscratch*</span></p>
</td>
<td>No</td>
<td>Yes</td>
<td>Intended for parallel I/O; large, shared by all users on a cluster. IBM's Spectrum Scale (not Lustre).<br />Mounted as /p/gpfs1 on sierra, lassen and rzansel.  Mounted as /p/gscratch* on ray, rzmanta and shark.</td>
</tr><tr><td>Burst buffer</td>
<td><span class="fixed">$BBPATH</span></td>
<td>No</td>
<td>Yes</td>
<td>Each node has a 1.6 TB NVMe PCIe SSD. Available only when requested through bsub. See <a href="#NVMe">NVMe PCIe SSD (Burst Buffer)</a> for details.<br />For CORAL EA systems, only ray compute nodes have the 1.6 TB NVMe, and it is statically mounted under /l/nvme.</td>
</tr><tr><td>HPSS archival storage</td>
<td><span class="fixed">server based</span></td>
<td>No</td>
<td>No</td>
<td>Virtually unlimited archival storage; accessed by "ftp storage" from LC machines.</td>
</tr><tr><td>FIS</td>
<td><span class="fixed">server based</span></td>
<td>No</td>
<td>Yes</td>
<td>File Interchange System; for transferring files between unclassified/classified networks</td>
</tr></table><h3><a name="hpss-storage" id="hpss-storage"></a><strong>HPSS Storage</strong></h3>
<ul><li>As with all other production LC systems, Sierra systems have access to LC's High Performance Storage System (HPSS) archival storage.</li>
<li>The HPSS system is named <strong>storage.llnl.gov</strong> on both the OCF and SCF.</li>
<li>LC does not backup temporary file systems, including the scratch parallel file systems. Users should backup their important files to storage.</li>
<li>Several different file transfer tools are available.</li>
<li>See <a href="https://computing.llnl.gov/tutorials/lc_resources/#Archival" target="_blank">https://computing.llnl.gov/tutorials/lc_resources/#Archival</a> for details on using HPSS storage.</li>
</ul><h3><a name="modules" id="modules"></a><strong>Modules</strong></h3>
<ul><li>As with LC's TOSS 3 systems, Lmod modules are used for most software packages, such as compilers, MPI and tools.</li>
<li>Dotkits are no longer used.</li>
<li>Users only need to know a few commands to effectively use modules - see the table below.</li>
<li><span class="note-green">Note</span> The "ml" shorthand can be used instead of "module" - for example: "ml avail"</li>
<li>See Using <a href="https://hpc.llnl.gov/software/modules-and-software-packaging" target="_blank">https://hpc.llnl.gov/software/modules-and-software-packaging</a> for more information.</li>
</ul><table class="table table-striped table-bordered" summary="Commands to effectively use modules"><tr><th scope="col">Command</th>
<th scope="col">Shorthand</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">module avail</span></td>
<td><span class="fixed">ml avail</span></td>
<td>List available modules</td>
</tr><tr><td><span class="fixed">module load <em>package</em></span></td>
<td><span class="fixed">ml load <em>package</em></span></td>
<td>Load a selected module</td>
</tr><tr><td><span class="fixed">module list</span></td>
<td><span class="fixed">ml</span></td>
<td>Show modules currently loaded</td>
</tr><tr><td><span class="fixed">module unload <em>package</em></span></td>
<td><span class="fixed">ml unload <em>package</em></span></td>
<td>Unload a previously loaded module</td>
</tr><tr><td><span class="fixed">module purge</span></td>
<td><span class="fixed">ml purge</span></td>
<td>Unload all loaded modules</td>
</tr><tr><td><span class="fixed">module reset</span></td>
<td><span class="fixed">ml reset</span></td>
<td>Reset loaded modules to system defaults</td>
</tr><tr><td><span class="fixed">module update</span></td>
<td><span class="fixed">ml update</span></td>
<td>Reload all currently loaded modules</td>
</tr><tr><td><span class="fixed">module display <em>package</em></span></td>
<td>n/a</td>
<td>Display the contents of a selected module</td>
</tr><tr><td><span class="fixed">module spider</span></td>
<td><span class="fixed">ml spider</span></td>
<td>List all modules (not just available ones)</td>
</tr><tr><td><span class="fixed">module keyword <em>key</em></span></td>
<td><span class="fixed">ml keyword <em>key</em></span></td>
<td>Search for available modules by keyword</td>
</tr><tr><td><span class="fixed">module<br />module help</span></td>
<td><span class="fixed">ml keyword key</span></td>
<td>Display module help</td>
</tr></table><h3><a name="compilers-supported" id="compilers-supported"></a><strong>Compilers</strong></h3>
<ul><li>The following compilers are available and supported on LC's Sierra systems:</li>
</ul><table class="table table-striped table-bordered" summary="Compilers available and supported on LC's Sierra systems"><tr><th scope="col">Compiler</th>
<th scope="col">Description</th>
</tr><tr><td><strong>XL</strong></td>
<td>IBM's XL C/C++ and Fortran compilers</td>
</tr><tr><td><strong>Clang</strong></td>
<td>IBM's C/C++ clang compiler</td>
</tr><tr><td><strong>GNU</strong></td>
<td>GNU compiler collection, C, C++, Fortran</td>
</tr><tr><td><strong>PGI</strong></td>
<td>Portland Group compilers</td>
</tr><tr><td><strong>NVCC</strong></td>
<td>NVIDIA's C/C++ compiler</td>
</tr><tr><td><strong>Wrapper scripts</strong></td>
<td>LC provides wrappers for most compiler commands (serial GNU are the only exceptions). Additionally, LC provides wrappers for the MPI compiler commands.</td>
</tr></table><ul><li>Compilers are discussed in detail in the <a href="https://computing.llnl.gov/tutorials/sierra/index.html#Compilers">Compilers</a> section.</li>
</ul><h3><a name="MathLibs1" id="MathLibs1"></a><strong>Math Libraries</strong></h3>
<ul><li>The following math libraries are available and supported on LC's Sierra systems:</li>
</ul><table class="table table-striped table-bordered" summary="Math libraries available and supported on LC's Sierra systems"><tr><th scope="col">Library</th>
<th scope="col">Description</th>
</tr><tr><td>ESSL</td>
<td>IBM's Engineering Scientific Subroutine Library</td>
</tr><tr><td>MASS, MASSV</td>
<td>IBM's Mathematical Acceleration Subsystem libraries</td>
</tr><tr><td>BLAS, LAPACK, ScaLAPACK</td>
<td>Netlib Linear Algebra Packages</td>
</tr><tr><td>FFTW</td>
<td>Fast Fourier Transform library</td>
</tr><tr><td>PETSc</td>
<td>Portable, Extensible Toolkit for Scientific Computation library</td>
</tr><tr><td>GSL</td>
<td>GNU Scientific Library</td>
</tr><tr><td>CUDA Tools</td>
<td>Math libraries included in the NVIDIA CUDA toolkit</td>
</tr></table><ul><li>See the <a href="#MathLibs2">Math Libraries</a> section for specific details for these libraries.</li>
<li>Also see LC's <a href="/software/mathematical-software" target="_blank">Mathematical Software Overview</a> manual and the <a href="https://www-lc.llnl.gov/linmath/" target="_blank">LINMath Website</a> for more information about math libraries in general, and where users can download math library source code to build their own libraries.</li>
</ul><h3><a name="debuggers" id="debuggers"></a><strong>Debuggers and Performance Analysis Tools</strong></h3>
<ul><li>LC's Development and Environment group maintains a number of debuggers and performance analysis tools that are able to be used on LC's systems.</li>
<li>The <a href="#debuggers">Debuggers</a> and <a href="#Performance">Performance Analysis Tools</a> sections of this tutorial describe what's available on LC's Sierra platforms and provide pointers for their use.</li>
<li>Also see the "Development Environment Software" web page located at <a href="https://hpc.llnl.gov/software/development-environment-software" target="_blank">https://hpc.llnl.gov/software/development-environment-software</a> for more information.</li>
</ul><h3><a name="visualization-software-compute-resources" id="visualization-software-compute-resources"></a><strong>Visualization Software and Compute Resources</strong></h3>
<ul><li>Visualization software and services are provided by LC's Information Management and Graphics Group (IMGG).</li>
<li>Data and Visualization webpage: <a href="https://hpc.llnl.gov/data-vis" target="_blank">https://hpc.llnl.gov/data-vis</a></li>
<li>Visualization Software: <a href="https://hpc.llnl.gov/data-vis/vis-software" target="_blank">https://hpc.llnl.gov/data-vis/vis-software</a></li>
</ul><h2><a name="Compilers" id="Compilers"></a>Compilers</h2>
<h3><strong>Available Compilers</strong></h3>
<ul><li>The following compilers are available on Sierra systems, and are discussed in detail below, along with other relevant compiler related information:
<ul><li><a href="#XLcompiler">XL</a>: IBM's XL C/C++ and Fortran compilers</li>
<li><a href="#Clangcompiler">Clang</a>: IBM's C/C++ clang compiler</li>
<li><a href="#GNUcompiler">GNU</a>: GNU compiler collection, C, C++, Fortran</li>
<li><a href="#PGIcompiler">PGI</a>: Portland Group compilers</li>
<li><a href="#NVCCcompiler">NVCC</a>: NVIDIA's C/C++ compiler</li>
</ul></li>
</ul><h3><strong>Compiler Recommendations</strong></h3>
<ul><li>The recommended and supported compilers are those delivered from IBM (XL and Clang ) and NVIDIA (NVCC):
<ul><li>Only XL and Clang compilers from IBM provide OpenMP 4.5 with GPU support.</li>
<li>NVCC offers direct CUDA support</li>
<li>The IBM <span class="fixed">xlcuf</span> compiler also provides direct CUDA support</li>
<li>Please report all problems to the you may have with these to the LC Hotline so that fixes can be obtained from IBM and NVIDIA.</li>
</ul></li>
<li>The other available compilers (GNU and PGI) can be used for experimentation and for comparisons to the IBM compilers:
<ul><li>Versions installed at LC do not provide Open 4.5 with GPU support</li>
<li>If you experience problems with the PGI compilers, LC can forward those issues to PGI.</li>
</ul></li>
<li>Using OpenACC on LC's Sierra clusters is not recommended nor supported.</li>
</ul><h3><a name="compwrappers" id="compwrappers"></a><strong>Wrappers Scripts</strong></h3>
<ul><li>LC has created wrappers for most compiler commands, both serial and MPI versions.</li>
<li>The wrappers perform LC customization and error checking. They also follow a string of links, which include other wrappers.</li>
<li>The wrappers located in <span class="fixed">/usr/tce/bin</span> (in your PATH) will always point (symbolic link) to the default versions.</li>
<li><span class="note-green">Note</span> There may also be versions of the serial compiler commands in <span class="fixed">/usr/bin</span>. Do not use these, as they are missing the LC customizations.</li>
<li>If you load a different module version, your PATH will change, and the location may then be in either <span class="fixed">/usr/tce/bin</span> or <span class="fixed">/usr/tcetmp/bin</span>.</li>
<li>To determine the actual location of the wrapper, simply use the command <span class="fixed">which <em>compilercommand</em></span> to view its path.</li>
<li>Example: show location of default/current xlc wrapper, load a new version, and show new location:</li>
</ul><pre><span class="text-info"><span>% </span><span><span class="text-danger"><span>which xlc</span></span></span>
<span>/usr/tce/packages/xl/xl-2019.02.07/bin/xlc</span></span>

<span class="text-info"><span>% <span class="text-danger">module load xl/2019.04.19</span>
Due to MODULEPATH changes the following have been reloaded:
1) spectrum-mpi/rolling-release</span></span>

<span class="text-info"><span>The following have been reloaded with a version change:
1) xl/2019.02.07 =&gt; xl/2019.04.19</span></span>

<span class="text-info"><span>% <span class="text-danger">which xlc</span>
/usr/tce/packages/xl/xl-2019.04.19/bin/xlc</span></span></pre><h3><a name="compversions" id="compversions"></a><strong>Versions</strong></h3>
<ul><li>There are several ways to determine compiler versions, discussed below.</li>
<li>The default version of compiler wrappers is pointed to from <span class="fixed">/usr/tce/bin</span>.</li>
<li>To see available compiler module versions use the command <span class="fixed">module avail</span>:
<ul><li>An <strong>(L)</strong> indicates which version is currently loaded.</li>
<li>A <strong>(D)</strong> indicates the default version.</li>
</ul></li>
<li>For example:</li>
</ul><pre>% <span class="text-danger">module avail</span>
------------------------------- /usr/tce/modulefiles/Compiler/xl/2019.04.19 --------------------------------
   spectrum-mpi/rolling-release (L,D)    spectrum-mpi/2018.08.13    spectrum-mpi/2019.01.22
   spectrum-mpi/2018.04.27               spectrum-mpi/2018.08.30    spectrum-mpi/2019.01.30
   spectrum-mpi/2018.06.01               spectrum-mpi/2018.10.10    spectrum-mpi/2019.01.31
   spectrum-mpi/2018.06.07               spectrum-mpi/2018.11.14    spectrum-mpi/2019.04.19
   spectrum-mpi/2018.07.12               spectrum-mpi/2018.12.14
   spectrum-mpi/2018.08.02               spectrum-mpi/2019.01.18

--------------------------------------- /usr/tcetmp/modulefiles/Core ---------------------------------------
   StdEnv                    (L)      glxgears/1.2                         pgi/18.3
   archer/1.0.0                       gmake/4.2.1                          pgi/18.4
   bsub-wrapper/1.0                   gmt/5.1.2                            pgi/18.5
   bsub-wrapper/2.0          (D)      gnuplot/5.0.0                        pgi/18.7
   cbflib/0.9.2                       grace/5.1.25                         pgi/18.10            (D)
   clang/coral-2017.11.09             gsl/2.3                              pgi/19.1
   clang/coral-2017.12.06             gsl/2.4                              pgi/19.3
   clang/coral-2018.04.17             gsl/2.5                       (D)    pgi/19.4
   clang/coral-2018.05.18             hwloc/1.11.10-cuda                   pgi/19.5
   clang/coral-2018.05.22             ibmppt/alpha-2.4.0                   python/2.7.13
   clang/coral-2018.05.23             ibmppt/beta-2.4.0                    python/2.7.14
   clang/coral-2018.08.08             ibmppt/beta2-2.4.0                   python/2.7.16        (D)
   clang/upstream-2018.12.03          ibmppt/workshop.181017               python/3.6.4
   clang/upstream-2019.03.19          ibmppt/2.3                           python/3.7.2
   clang/upstream-2019.03.26 (D)      ibmppt/2.4.0                         rasmol/2.7.5.2
   clang/6.0.0                        ibmppt/2.4.0.1                       scorep/3.0.0
   cmake/3.7.2                        ibmppt/2.4.0.2                       scorep/2019.03.16
   cmake/3.8.2                        ibmppt/2.4.0.3                       scorep/2019.03.21    (D)
   cmake/3.9.2               (D)      ibmppt/2.4.1                  (D)    setup-ssh-keys/1.0
   cmake/3.12.1                       jsrun/unwrapped                      sqlcipher/3.7.9
   cmake/3.14.5                       jsrun/2019.01.19                     tau/2.26.2
   coredump/cuda_fullcore             jsrun/2019.05.02              (D)    tau/2.26.3           (D)
   coredump/cuda_lwcore               lalloc/1.0                           totalview/2016.07.22
   coredump/fullcore                  lalloc/2.0                    (D)    totalview/2017X.3.1
   coredump/lwcore           (D)      lapack/3.8.0-gcc-4.9.3               totalview/2017.0.12
   coredump/lwcore2                   lapack/3.8.0-xl-2018.06.27           totalview/2017.1.21
   cqrlib/1.0.5                       lapack/3.8.0-xl-2018.11.26    (D)    totalview/2017.2.11  (D)
   cuda/9.0.176                       lapack/3.8.0-P9-xl-2018.11.26        valgrind/3.13.0
   cuda/9.0.184                       lc-diagnostics/0.1.0                 valgrind/3.14.0      (D)
   cuda/9.1.76                        lmod/7.4.17                   (D)    vampir/9.5
   cuda/9.1.85                        lrun/2018.07.22                      vampir/9.6           (D)
   cuda/9.2.64                        lrun/2018.10.18                      vmd/1.9.3
   cuda/9.2.88                        lrun/2019.05.07               (D)    xforms/1.0.91
   cuda/9.2.148              (L,D)    makedepend/1.0.5                     xl/beta-2018.06.27
   cuda/10.1.105                      memcheckview/3.13.0                  xl/beta-2018.07.17
   cuda/10.1.168                      memcheckview/3.14.0           (D)    xl/beta-2018.08.08
   cvector/1.0.3                      mesa3d/17.0.5                        xl/beta-2018.08.24
   debugCQEmpi                        mesa3d/19.0.1                 (D)    xl/beta-2018.09.13
   essl/sys-default                   mpifileutils/0.8                     xl/beta-2018.09.26
   essl/6.1.0                         mpifileutils/0.9              (D)    xl/beta-2018.10.10
   essl/6.1.0-1                       mpip/3.4.1                           xl/beta-2018.10.29
   essl/6.2                  (D)      neartree/5.1.1                       xl/beta-2018.11.02
   fftw/3.3.8                         patchelf/0.8                         xl/beta-2019.06.13
   flex/2.6.4                         petsc/3.7.6                          xl/beta-2019.06.19
   gcc/4.9.3                 (D)      petsc/3.8.3                          xl/test-2019.03.22
   gcc/7.2.1-redhat                   petsc/3.9.0                   (D)    xl/2018.04.29
   gcc/7.3.1                          pgi/17.4                             xl/2018.05.18
   gdal/1.9.0                         pgi/17.7                             xl/2018.11.26
   git/2.9.3                          pgi/17.9                             xl/2019.02.07        (D)
   git/2.20.0                (D)      pgi/17.10                            xl/2019.04.19        (L)
   git-lfs/2.5.2                      pgi/18.1

---------------------------------- /usr/share/lmod/lmod/modulefiles/Core -----------------------------------
   lmod/6.5.1    settarg/6.5.1

--------------------- /collab/usr/global/tools/modulefiles/blueos_3_ppc64le_ib_p9/Core ---------------------
   hpctoolkit/2019.03.10

  Where:
   L:  Module is loaded
   D:  Default Module

Use "module spider" to find all possible modules.
Use "module keyword key1 key2 ..." to search for all possible modules matching any of
the "keys".</pre><ul><li>You can also use any of the following commands to get version information:</li>
</ul><pre> module display <em>compiler</em>
 module help <em>compiler</em>
 module key <em>compiler</em>
 module spider <em>compiler</em></pre><ul><li>Examples below, using the IBM XL compiler (some output omitted):</li>
</ul><pre>% <span class="text-danger">module display xl</span>

-----------------------------------------------------------------------------------------
   /usr/tcetmp/modulefiles/Core/xl/2019.04.19.lua:
-----------------------------------------------------------------------------------------
help([[LLVM/XL compiler beta 2019.04.19

IBM XL C/C++ for Linux, V16.1.1 (5725-C73, 5765-J13)
Version: 16.01.0001.0003

IBM XL Fortran for Linux, V16.1.1 (5725-C75, 5765-J15)
Version: 16.01.0001.0003
]])
whatis("Name: XL compilers")
whatis("Version: 2019.04.19")
whatis("Category: Compilers")
whatis("URL: http://www.ibm.com/software/products/en/xlcpp-linux")
family("compiler")
prepend_path("MODULEPATH","/usr/tce/modulefiles/Compiler/xl/2019.04.19")
prepend_path("PATH","/usr/tce/packages/xl/xl-2019.04.19/bin")
prepend_path("MANPATH","/usr/tce/packages/xl/xl-2019.04.19/xlC/16.1.1/man/en_US")
prepend_path("MANPATH","/usr/tce/packages/xl/xl-2019.04.19/xlf/16.1.1/man/en_US")
prepend_path("NLSPATH","/usr/tce/packages/xl/xl-2019.04.19/xlf/16.1.1/msg/%L/%N")
prepend_path("NLSPATH","/usr/tce/packages/xl/xl-2019.04.19/xlC/16.1.1/msg/%L/%N")
prepend_path("NLSPATH","/usr/tce/packages/xl/xl-2019.04.19/msg/%L/%N")

% <span class="text-danger">module help xl</span>

------------------------- Module Specific Help for "xl/2019.04.19" --------------------------
LLVM/XL compiler beta 2019.04.19

IBM XL C/C++ for Linux, V16.1.1 (5725-C73, 5765-J13)
Version: 16.01.0001.0003

IBM XL Fortran for Linux, V16.1.1 (5725-C75, 5765-J15)
Version: 16.01.0001.0003

% <span class="text-danger">module key xl</span>

-----------------------------------------------------------------------------------------
The following modules match your search criteria: "xl"
-----------------------------------------------------------------------------------------

  hdf5-parallel: hdf5-parallel/1.10.4

  hdf5-serial: hdf5-serial/1.10.4

  lapack: lapack/3.8.0-xl-2018.06.27, lapack/3.8.0-xl-2018.11.26, ...

  netcdf-c: netcdf-c/4.6.3

  spectrum-mpi: spectrum-mpi/rolling-release, spectrum-mpi/2017.04.03, ...

  xl: xl/beta-2018.06.27, xl/beta-2018.07.17, xl/beta-2018.08.08, xl/beta-2018.08.24, ...

-----------------------------------------------------------------------------------------
To learn more about a package enter:
   $ module spider Foo
where "Foo" is the name of a module
To find detailed information about a particular package you
must enter the version if there is more than one version:
   $ module spider Foo/11.1

% <span class="text-danger">module spider xl</span>

-----------------------------------------------------------------------------------------
  xl:
-----------------------------------------------------------------------------------------
     Versions:
        xl/beta-2018.06.27
        xl/beta-2018.07.17
        xl/beta-2018.08.08
        xl/beta-2018.08.24
        xl/beta-2018.09.13
        xl/beta-2018.09.26
        xl/beta-2018.10.10
        xl/beta-2018.10.29
        xl/beta-2018.11.02
        xl/beta-2019.06.13
        xl/beta-2019.06.19
        xl/test-2019.03.22
        xl/2018.04.29
        xl/2018.05.18
        xl/2018.11.26
        xl/2019.02.07
        xl/2019.04.19

-----------------------------------------------------------------------------------------

% <span class="text-danger">module spider xl/beta-2019.06.19</span>

-----------------------------------------------------------------------------------------
  xl: xl/beta-2019.06.19
-----------------------------------------------------------------------------------------

    This module can be loaded directly: module load xl/beta-2019.06.19

    Help:
      LLVM/XL compiler beta beta-2019.06.19
    
      IBM XL C/C++ for Linux, V16.1.1 (5725-C73, 5765-J13)
      Version: 16.01.0001.0004
    
      IBM XL Fortran for Linux, V16.1.1 (5725-C75, 5765-J15)
      Version: 16.01.0001.0004</pre><ul><li>Finally, simply passing the <span class="fixed">--version</span> option to the compiler invocation command will usually provide the version of the compiler. For example:</li>
</ul><pre>% <span class="text-danger">xlc --version</span>
IBM XL C/C++ for Linux, V16.1.1 (5725-C73, 5765-J13)
Version: 16.01.0001.0003

% <span class="text-danger">gcc --version</span>
gcc (GCC) 4.9.3
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

% <span class="text-danger">clang --version</span>
clang version 9.0.0 (/home/gbercea/patch-compiler ad50cf1cbfefbd68e23c3b615a8160ee65722406) (ibmgithub:/CORAL-LLVM-Compilers/llvm.git 07bbe5e2922ece3928bbf9f093d8a7ffdb950ae3)
Target: powerpc64le-unknown-linux-gnu
Thread model: posix
InstalledDir: /usr/tce/packages/clang/clang-upstream-2019.03.26/ibm/bin</pre><h3><a name="compselect" id="compselect"></a><strong>Selecting Your Compiler and MPI Version</strong></h3>
<ul><li>Compiler and MPI software is installed as packages under <span class="fixed">/usr/tce/packages</span> and/or <span class="fixed">/usr/tcetmp/packages</span>.</li>
<li>LC provides default packages for compilers and MPI. To see the current defaults, use the <span class="fixed">module avail</span> command, as shown above in the <a href="#compversions">Versions</a> discussion. Note that a <strong>(D)</strong> next to a package shows that it is the default.</li>
<li>The default versions will change as newer versions are released.
<ul><li>It's recommended that you use the most recent default compilers to stay abreast of new fixes and features.</li>
<li>You may need to recompile your entire application when the default compilers change.</li>
</ul></li>
<li>LMOD modules are used to select alternate compiler and MPI packages.</li>
<li>To select an alternate version of a compiler and/or MPI, use the following procedure:</li>
</ul><ol><li>Use <span class="fixed">module list</span> to see what's currently loaded</li>
<li>Use <span class="fixed">module key</span> <span class="fixed">compiler</span> to see what compilers and MPI packages are available.</li>
<li>Use <span class="fixed">module load</span> <em><span class="fixed">package</span></em> to load the selected package.</li>
<li>Use <span class="fixed">module list</span> again to confirm your selection was loaded.</li>
</ol><ul><li>Examples below (some output omitted):</li>
</ul><pre>% <span class="text-danger">module list</span>

Currently Loaded Modules:
  1) xl/2019.02.07   2) spectrum-mpi/rolling-release   3) cuda/9.2.148   4) StdEnv

% <span class="text-danger">module key compiler</span>

-------------------------------------------------------------------------------------------
The following modules match your search criteria: "compiler"
-------------------------------------------------------------------------------------------

  clang: clang/coral-2017.11.09, clang/coral-2017.12.06, clang/coral-2018.04.17, ...

  cuda: cuda/9.0.176, cuda/9.0.184, cuda/9.1.76, cuda/9.1.85, cuda/9.2.64, cuda/9.2.88, ...

  gcc: gcc/4.9.3, gcc/7.2.1-redhat, gcc/7.3.1

  lalloc: lalloc/1.0, lalloc/2.0

  pgi: pgi/17.4, pgi/17.7, pgi/17.9, pgi/17.10, pgi/18.1, pgi/18.3, pgi/18.4, pgi/18.5, ...

  spectrum-mpi: spectrum-mpi/rolling-release, spectrum-mpi/2017.04.03, ...

  xl: xl/beta-2018.06.27, xl/beta-2018.07.17, xl/beta-2018.08.08, xl/beta-2018.08.24, ...

-------------------------------------------------------------------------------------------
To learn more about a package enter:
   $ module spider Foo
where "Foo" is the name of a module
To find detailed information about a particular package you
must enter the version if there is more than one version:
   $ module spider Foo/11.1

% <span class="text-danger">module load xl/2019.04.19</span>

Due to MODULEPATH changes the following have been reloaded:
  1) spectrum-mpi/rolling-release

The following have been reloaded with a version change:
  1) xl/2019.02.07 =&gt; xl/2019.04.19

% <span class="text-danger">module list</span>

Currently Loaded Modules:
  1) cuda/9.2.148   2) StdEnv   3) xl/2019.04.19   4) spectrum-mpi/rolling-release

% <span class="text-danger">module load pgi</span>

Lmod is automatically replacing "xl/2019.04.19" with "pgi/18.10"

Due to MODULEPATH changes the following have been reloaded:
  1) spectrum-mpi/rolling-release

% <span class="text-danger">module list</span>

Currently Loaded Modules:
  1) cuda/9.2.148   2) StdEnv   3) pgi/18.10   4) spectrum-mpi/rolling-release</pre><ul><li>Notes:
<ul><li>When a new compiler package is loaded, the MPI package will be reloaded to use a version built with the selected compiler.</li>
<li>Only one compiler package is loaded at a time, with a version of the IBM XL compiler being the default. If a new compiler package is loaded, it will replace what is currently loaded. The default compiler commands for all compilers will remain in your PATH however.</li>
</ul></li>
</ul><h3><a name="XLcompiler" id="XLcompiler"></a><strong>IBM XL Compilers</strong></h3>
<ul><li>As discussed previously:
<ul><li><a href="#compwrappers">Wrapper scripts</a>: Used by LC for most compiler commands.</li>
<li><a href="#compversions">Versions</a>: There is a default version for each compiler, and usually several alternate versions also.</li>
<li><a href="#compselect">Selecting your compiler and MPI</a></li>
</ul></li>
<li>XL compiler commands are shown in the table below.</li>
</ul><table class="table table-striped table-bordered" summary="IBM XL Compiler Commands"><tr><th colspan="6" scope="col">IBM XL Compiler Commands</th>
</tr><tr><td><strong>Language</strong></td>
<td>Serial</td>
<td>Serial +<br />OpenMP 4.5</td>
<td>MPI</td>
<td>MPI +<br />OpenMP 4.5</td>
<td>Comments</td>
</tr><tr><td><strong>C</strong></td>
<td><span class="fixed">xlc</span></td>
<td><span class="fixed">xlc-gpu</span></td>
<td><span class="fixed">mpixlc<br />mpicc</span></td>
<td><span class="fixed">mpixlc-gpu<br />mpicc-gpu</span></td>
<td colspan="1" rowspan="3">The -gpu commands add the flags:<br /><span class="fixed">-qsmp=omp<br />-qoffload</span></td>
</tr><tr><td><strong>C++</strong></td>
<td><span class="fixed">xlC<br />xlc++</span></td>
<td><span class="fixed">xlC-gpu<br />xlc++-gpu</span></td>
<td><span class="fixed">mpixlC<br />mpiCC<br />mpic++<br />mpicxx</span></td>
<td><span class="fixed">mpixlC-gpu<br />mpiCC-gpu<br />mpic++-gpu<br />mpicxx-gpu</span></td>
</tr><tr><td><strong>Fortran</strong></td>
<td><span class="fixed">xlf<br />xlf90<br />xlf95<br />xlf2003<br />xlf2008</span></td>
<td><span class="fixed">xlf-gpu<br />xlf90-gpu<br />xlf95-gpu<br />xlf2003-gpu<br />xlf2008-gpu</span></td>
<td><span class="fixed">mpixlf<br />mpifort<br />mpif77<br />mpif90</span></td>
<td><span class="fixed">mpixlf-gpu<br />mpifort-gpu<br />mpif77-gpu<br />mpif90-gpu</span></td>
</tr></table><ul><li>Thread safety: LC always aliases the XL compiler commands to their <span class="fixed">_r</span> (thread safe) versions. This is to prevent some known problems, particularly with Fortran. <span class="note-green">Note</span> the <span class="fixed">/usr/bin/xl*</span> commands are not aliased as such, and they are not LC wrapper scripts - use is discouraged.</li>
<li>OpenMP with NVIDIA GPU offloading is supported. For convenience, LC provides the <span class="fixed">-gpu</span> commands, which set the option<span class="fixed"> -qsmp=omp</span> for OpenMP and <span class="fixed">-qoffload</span> for GPU offloading. Users can do this themselves without using the -gpu commands.</li>
<li>Optimizations:
<ul><li>The <span class="fixed">-O0</span> <span class="fixed">-O2</span> <span class="fixed">-O3</span> <span class="fixed">-Ofast</span> options cause the compiler to run optimizing transformations to the user code, for both CPU and GPU code.</li>
<li>Options to target the Power8 architecture: <span class="fixed">-qarch=pwr8 -qtune=pwr8</span></li>
<li>Options to target the Power9 (Sierra) architecture: <span class="fixed">-qarch=pwr9 -qtune=pwr9</span></li>
</ul></li>
<li>Debugging - recommended options:
<ul><li><span class="fixed">-g -O0 -qsmp=omp:noopt -qoffload -qfullpath</span></li>
<li><span class="fixed">noopt </span>- This sub-option will minimize the OpenMP optimization. Without this, XL compilers will still optimize the code for your OpenMP code despite -O0. It will also disable RT inlining thus enabling GPU debug information</li>
<li><span class="fixed">-qfullpath</span> - adds the absolute paths of your source files into DWARF helping TotalView locate the source even if your executable moves to a different directory.</li>
</ul></li>
<li>Documentation:
<ul><li>XLC/C++: Select the relevant version of Little Endian documents at <a href="https://www-01.ibm.com/support/docview.wss?uid=swg27036675" target="_blank">https://www-01.ibm.com/support/docview.wss?uid=swg27036675</a></li>
<li>XLF: Select the relevant version of Little Endian documents at <a href="https://www-01.ibm.com/support/docview.wss?uid=swg27036672" target="_blank">https://www-01.ibm.com/support/docview.wss?uid=swg27036672</a></li>
<li>IBM Redbook - Section 6.1.1 of "Implementing an IBM High-Performance Computing Solution on IBM Power System S822LC": <a href="https://www.redbooks.ibm.com/redbooks/pdfs/sg248280.pdf" target="_blank">https://www.redbooks.ibm.com/redbooks/pdfs/sg248280.pdf</a></li>
<li>IBM White Paper "Code Optimization with the IBM XL compilers on Power Architectures": <a href="https://www-01.ibm.com/support/docview.wss?uid=swg27005174&amp;aid=1" target="_blank">https://www-01.ibm.com/support/docview.wss?uid=swg27005174&amp;aid=1</a></li>
</ul></li>
</ul><h3><a name="Clangcompiler" id="Clangcompiler"></a><strong>IBM Clang Compiler</strong></h3>
<ul><li>The Sierra systems use the Clang compiler from IBM.</li>
<li>As discussed previously:
<ul><li><a href="#compwrappers">Wrapper scripts:</a> Used by LC for most compiler commands.</li>
<li><a href="#compversions">Versions:</a> There is a default version for each compiler, and usually several alternate versions also.</li>
<li><a href="#compselect">Selecting your compiler and MPI</a></li>
</ul></li>
<li>Clang compiler commands are shown in the table below.</li>
</ul><table class="table table-headed table-bordered" summary="Clang Compiler Commands"><tr><th colspan="6" scope="col">Clang Compiler Commands</th>
</tr><tr><td><strong>Language</strong></td>
<td><strong>Serial</strong></td>
<td><strong>Serial +<br />OpenMP 4.5</strong></td>
<td><strong>MPI</strong></td>
<td><strong>MPI +<br />OpenMP 4.5</strong></td>
<td><strong>Comments</strong></td>
</tr><tr><td>C</td>
<td><span class="fixed">clang</span></td>
<td><span class="fixed">clang-gpu</span></td>
<td><span class="fixed">mpiclang</span></td>
<td><span class="fixed">mpiclang-gpu</span></td>
<td colspan="1" rowspan="2">The -gpu commands add the flags:<br /><span class="fixed">-fopenmp<br />-fopenmp-targets=nvptx64-nvidia-cuda</span><br /> </td>
</tr><tr><td>C++</td>
<td><span class="fixed">clang++</span></td>
<td><span class="fixed">clang++-gpu</span></td>
<td><span class="fixed">mpiclang++</span></td>
<td><span class="fixed">mpiclang++-gpu</span></td>
</tr></table><ul><li>OpenMP with NVIDIA GPU offloading is supported. For convenience, LC provides the <span class="fixed">-gpu</span> commands, which set the option <span class="fixed">-fopenmp</span> for OpenMP and <span class="fixed">-fopenmp-targets=nvptx64-nvidia-cuda</span> for GPU offloading. Users can do this themselves without using the -gpu commands. However, use of LC's -gpu commands is recommended at this time since the native Clang flags are verbose and subject to change.</li>
<li>Documentation:
<ul><li>Use the<strong> <span class="fixed">clang -help</span></strong> command for a summary of available options.</li>
<li>Clang LLVM website at: <a href="http://clang.llvm.org/" target="_blank">http://clang.llvm.org/</a></li>
</ul></li>
</ul><h3><a name="GNUcompiler" id="GNUcompiler"></a><strong>GNU Compilers</strong></h3>
<ul><li>As discussed previously:
<ul><li><a href="#compwrappers">Wrapper scripts:</a> Used by LC for most compiler commands.</li>
<li><a href="#compversions">Versions:</a> There is a default version for each compiler, and usually several alternate versions also.</li>
<li><a href="#compselect">Selecting your compiler and MPI</a></li>
</ul></li>
<li>GNU compiler commands are shown in the table below.</li>
</ul><table class="table table-striped table-bordered" summary="GNU Compiler Commands"><tr><th colspan="6" scope="col">GNU Compiler Commands</th>
</tr><tr><td><strong>Language</strong></td>
<td><strong>Serial</strong></td>
<td><strong>Serial +<br />OpenMP 4.5</strong></td>
<td><strong>MPI</strong></td>
<td><strong>MPI +<br />OpenMP 4.5</strong></td>
<td><strong>Comments</strong></td>
</tr><tr><td>C</td>
<td><span class="fixed">gcc<br />cc</span></td>
<td><span class="fixed">n/a</span></td>
<td><span class="fixed">mpigcc</span></td>
<td><span class="fixed">n/a</span></td>
<td colspan="1" rowspan="3">For OpenMP use the flag: <span class="fixed">-fopenmp</span><br /> </td>
</tr><tr><td>C++</td>
<td><span class="fixed">g++<br />c++</span></td>
<td><span class="fixed">n/a</span></td>
<td><span class="fixed">mpig++</span></td>
<td><span class="fixed">n/a</span></td>
</tr><tr><td>Fortran</td>
<td><span class="fixed">gfortran</span></td>
<td><span class="fixed">n/a</span></td>
<td><span class="fixed">mpigfortran</span></td>
<td><span class="fixed">n/a</span></td>
</tr></table><ul><li>OpenMP with NVIDIA GPU offloading is NOT currently provided. OpenMP 4.5 is supported starting with version 6.1, however it is not for NVIDIA GPU offload. Target regions are implemented on the multicore host instead.</li>
<li>Optimization flags:
<ul><li>POWER8: <span class="fixed">-mcpu=power8 -mtune=power8</span></li>
<li>Also see Section 6.1.2 of the IBM Redbook: <a href="https://www.redbooks.ibm.com/redbooks/pdfs/sg248280.pdf" target="_blank">Implementing an IBM High-Performance Computing Solution on IBM Power System S822LC</a></li>
<li>POWER9: <span class="fixed">-mcpu=powerpc64le -mtune=powerpc64le</span></li>
</ul></li>
<li>Documentation:
<ul><li>GNU online documentation at: <a href="https://gcc.gnu.org/onlinedocs/" target="_blank">https://gcc.gnu.org/onlinedocs/</a></li>
</ul></li>
</ul><h3><a name="PGIcompiler" id="PGIcompiler"></a><strong>PGI Compilers</strong></h3>
<ul><li>As discussed previously:
<ul><li><a href="#compwrappers">Wrapper scripts:</a> Used by LC for most compiler commands.</li>
<li><a href="#compversions">Versions:</a> There is a default version for each compiler, and usually several alternate versions also.</li>
<li><a href="#compselect">Selecting your compiler and MPI</a></li>
</ul></li>
<li>PGI compiler commands are shown in the table below.</li>
</ul><table class="table table-striped table-bordered" summary="PGI Compiler Commands"><tr><th colspan="6" scope="col">PGI Compiler Commands</th>
</tr><tr><td><strong>Language</strong></td>
<td><strong>Serial</strong></td>
<td><strong>Serial +<br />OpenMP 4.5</strong></td>
<td><strong>MPI</strong></td>
<td><strong>MPI +<br />OpenMP 4.5</strong></td>
<td><strong>Comments</strong></td>
</tr><tr><td>C</td>
<td><span class="fixed">pgcc<br />cc</span></td>
<td><span class="fixed">n/a</span></td>
<td><span class="fixed">mpipgcc</span></td>
<td><span class="fixed">n/a</span></td>
<td colspan="1" rowspan="3">pgf90 and pgfortran are the same compiler, supporting the Fortran 2003 language specification<br />For OpenMP use the flag: <span class="fixed">-mp</span></td>
</tr><tr><td>C++</td>
<td><span class="fixed">pgc++</span></td>
<td><span class="fixed">n/a</span></td>
<td><span class="fixed">mpipgc++</span></td>
<td><span class="fixed">n/a</span></td>
</tr><tr><td>Fortran</td>
<td><span class="fixed">pgf90<br />pgfortran</span></td>
<td><span class="fixed">n/a</span></td>
<td><span class="fixed">mpipgf90<br />mpipgfortran</span></td>
<td><span class="fixed">n/a</span></td>
</tr></table><ul><li>OpenMP with NVIDIA GPU offloading is NOT currently provided. Most of OpenMP 4.5 is supported, however it is not for NVIDIA GPU offload. Target regions are implemented on the multicore host instead. See the product documentation (link below) "Installation Guide and Release Notes" for details.</li>
<li>GPU support is via CUDA and OpenACC.</li>
<li>Documentation:
<ul><li>PGI Compilers - select OpenPOWER docs: <a href="https://www.pgroup.com/resources/docs/" target="_blank">https://www.pgroup.com/resources/docs/</a></li>
<li>Presentation from the ORNL Workshop Jan. 2017: <a href="https://lc.llnl.gov/confluence/download/attachments/271712306/1.7.SummitDev_PGI_OpenPOWER_Portathon.pdf?version=1&amp;modificationDate=1485544749000&amp;api=v2" target="_blank">Porting to OpenPower &amp; Tesla with PGI</a></li>
</ul></li>
</ul><h3><a name="NVCCcompiler" id="NVCCcompiler"></a><strong>NVIDIA NVCC Compiler</strong></h3>
<ul><li>The NVIDIA<span class="fixed"> nvcc</span> compiler driver is used to compile C/C++ CUDA code:
<ul><li><span class="fixed">nvcc</span> compiles the CUDA code.</li>
<li>Non-CUDA compilation steps are forwarded to a C/C++ host (backend) compiler supported by <span class="fixed">nvcc</span>.</li>
<li><span class="fixed">nvcc</span> also translates its options to appropriate host compiler command line options.</li>
<li>NVCC currently supports XL, GCC, and PGI C++ backends, with GCC being the default.</li>
</ul></li>
<li>Location:
<ul><li>The NVCC C/C++ compiler is located under <span class="fixed">usr/tce/packages/cuda/</span>.</li>
<li>Other NVIDIA software and utilities (like nvprof, nvvp) are located here also.</li>
<li>The default CUDA build should be in your default PATH.</li>
</ul></li>
<li>As discussed previously:
<ul><li><a href="#compversions">Versions:</a> There is a default version for each compiler, and usually several alternate versions also.</li>
<li><a href="#compselect">Selecting your compiler and MPI</a></li>
</ul></li>
<li>Architecture flag:
<ul><li>Tesla P100 (Pascal) for Early Access systems: <span class="fixed">-arch=sm_60</span></li>
<li>Tesla V100 (Volta) for Sierra systems: <span class="fixed">-arch=sm_70</span></li>
</ul></li>
<li>Selecting a host compiler:
<ul><li>The GNU C/C++ compiler is used as the backend compiler by default.</li>
<li>To select a different backend compiler, use the <span class="fixed">-ccbin=<em>compiler</em></span> flag. For example:</li>
</ul></li>
</ul><p><span class="fixed">      nvcc -arch=sm_70 -ccbin=xlC myprog.cu<br />      nvcc -arch=sm_70 -ccbin=clang myprog.cu</span></p>
<ul><li>The alternate backend compiler needs to be in your path. Otherwise you need to specify the full pathname.</li>
</ul><ul><li>Source file suffixes:</li>
<li>Source files with CUDA code should have a <span class="fixed">.cu</span> suffix.</li>
<li>If source files have a different suffix, use the <span class="fixed">-x cu</span> flag. For example:</li>
</ul><p><span class="fixed">   nvcc -arch=sm_70 -ccbin=xlc -x cu myprog.c</span></p>
<ul><li>Documentation:
<ul><li>NVDIA NVCC: <a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/" target="_blank">https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/</a></li>
</ul></li>
</ul><h2><a name="MPI" id="MPI"></a>MPI</h2>
<h3><strong>IBM Spectrum MPI</strong></h3>
<ul><li>IBM Spectrum MPI is the only supported MPI library on LC's Sierra and CORAL EA systems.
<ul><li>Based on Open MPI 3.0.0</li>
<li>Basic architecture and functionality are similar.</li>
<li>Open MPI information: <a href="https://www.open-mpi.org/" target="_blank">https://www.open-mpi.org/</a>.</li>
</ul></li>
<li>IBM Spectrum MPI supports many, but not all of the features offered by Open MPI. It also adds some unique features of its own.</li>
<li>Implements MPI API 3.1.0</li>
<li>Supported features and usage notes:
<ul><li>64-bit Little Endian for IBM Power Systems, with and without GPUs.</li>
<li>Thread safety: MPI_THREAD_MULTIPLE (multiple threads executing within the MPI library). However, multithreaded I/O is not supported.</li>
<li>GPU support using <a href="#CUDA-aware">CUDA-aware MPI</a> and NVIDIA CPUDirect RDMA. </li>
<li>Parallel I/O: supports only ROMIO version 3.1.4. Multithreaded I/O is not supported. See the <a href="https://hpc.llnl.gov/sites/default/files/MPI-SpectrumUserGuide.pdf" target="_blank">Spectrum MPI User's Guide for details</a>.</li>
<li>MPI Collective Operations: defaults to using IBM's libcollectives library. Provides optimized collective algorithms and GPU memory buffer support. Using the Open MPI collectives is also supported. See the Spectrum MPI User's Guide for details.</li>
<li>Mellanox Fabric Collective Accelerator (FCA) support for accelerating collective operations.</li>
<li>Portable Hardware Locality (hwloc) support for displaying hardware topology information.</li>
<li>IBM Platform LSF workload manager is supported</li>
<li>Debugger support for Allinea DDT and Rogue Wave TotalView.</li>
<li>Process Management Interface Exascale (PMIx) support - see <a href="https://github.com/pmix" target="_blank">https://github.com/pmix</a> for details.</li>
</ul></li>
<li>Spectrum MPI provides the <span class="fixed">ompi_info</span> command for reporting detailed information on the MPI installation. Simply type <span class="fixed">ompi_info</span>.</li>
<li>Limitations: <a href="http://hpc.llnl.gov/sites/default/files/SpectrumMPI.Limitations.pdf">excerpted in this pdf </a>.</li>
<li>For additional information about IBM Spectrum MPI, see the links under "Documentation" below.</li>
</ul><h3><strong>Other MPI Libraries</strong></h3>
<ul><li>LC has installed MPICH-GDR MPI on Lassen for evaluation and testing. At the current time, it is not supported as a "full production" MPI library</li>
<li>Interested users are welcome to try it out.  Details can be found on the LC Confluence wiki at: <a href="https://lc.llnl.gov/confluence/display/SIERRA/Additional+MPI+Implementations" target="_blank">https://lc.llnl.gov/confluence/display/SIERRA/Additional+MPI+Implementations</a></li>
</ul><h3><strong>Versions</strong></h3>
<ul><li>Use the <span class="fixed">module avail mpi</span> command to display available MPI packages. For example:</li>
</ul><pre>% <span class="text-danger">module avail mpi</span>

---------------------- /usr/tce/modulefiles/Compiler/xl/2019.02.07 ----------------------
   spectrum-mpi/rolling-release (L,D)    spectrum-mpi/2018.11.14
   spectrum-mpi/2018.04.27               spectrum-mpi/2018.12.14
   spectrum-mpi/2018.06.01               spectrum-mpi/2019.01.18
   spectrum-mpi/2018.06.07               spectrum-mpi/2019.01.22
   spectrum-mpi/2018.07.12               spectrum-mpi/2019.01.30
   spectrum-mpi/2018.08.02               spectrum-mpi/2019.01.31
   spectrum-mpi/2018.08.13               spectrum-mpi/2019.04.19
   spectrum-mpi/2018.08.30               spectrum-mpi/2019.06.24
   spectrum-mpi/2018.10.10

----------------------------- /usr/tcetmp/modulefiles/Core ------------------------------
   debugCQEmpi         mpifileutils/0.9 (D)    vampir/9.5
   mpifileutils/0.8    mpip/3.4.1              vampir/9.6 (D)

  Where:
   L:  Module is loaded
   D:  Default Module

Use "module spider" to find all possible modules.
Use "module keyword key1 key2 ..." to search for all possible modules matching any of
the "keys".</pre><p> </p>
<ul><li>As noted above, the default version is indicated with a <strong>(D)</strong>, and the currently loaded version with a <strong>(L)</strong>.</li>
<li>For more detailed information about versions, see the discussion under <a href="#compversions">Compilers ==&gt; Versions</a>.</li>
<li>Selecting an alternate MPI version: simply use the command <span class="fixed">module load <em>package</em></span>.</li>
<li>For more additional discussion on selecting alternate versions, see <a href="#compselect">Compilers ==&gt; Selecting Your Compiler and MPI Version</a>.</li>
</ul><h3><strong>MPI and Compiler Dependency</strong></h3>
<ul><li>Each available version of MPI is built with each version of the available compilers.</li>
<li>The MPI package you have loaded will depend upon the compiler package you have loaded, and vice-versa:
<ul><li>Changing the compiler will automatically load the appropriate MPI-compiler build.</li>
<li>Changing the MPI package will automatically load an appropriate MPI-compiler build.</li>
</ul></li>
<li>For example:
<ul><li>Show the currently loaded modules</li>
<li>Show details on the loaded MPI module</li>
<li>Load a different compiler and show how it changes the MPI build that's loaded</li>
</ul></li>
</ul><pre>% <span class="text-danger">module list</span>
Currently Loaded Modules:
  1) xl/2019.02.07   2) spectrum-mpi/rolling-release   3) cuda/9.2.148   4) StdEnv

% <span class="text-danger">module whatis spectrum-mpi/rolling-release</span>
spectrum-mpi/rolling-release                              : mpi/spectrum-mpi
spectrum-mpi/rolling-release                              : spectrum-mpi-rolling-release for xl-2019.02.07 compilers

% <span class="text-danger">module load pgi</span>
Lmod is automatically replacing "xl/2019.02.07" with "pgi/18.10"

% <span class="text-danger">module whatis spectrum-mpi/rolling-release</span>
spectrum-mpi/rolling-release                              : mpi/spectrum-mpi
spectrum-mpi/rolling-release                              : spectrum-mpi-rolling-release for pgi-18.10 compilers</pre><h3><strong>MPI Compiler Commands</strong></h3>
<ul><li>LC uses wrapper scripts for all of its MPI compiler commands. See discussion on <a href="#compwrappers">Wrapper Scripts</a>.</li>
<li>The table below lists the MPI commands for each compiler family.</li>
</ul><table class="table table-bordered table-headed" summary="MPI Compiler Commands"><tr><th scope="row">Compiler</th>
<th scope="col">Language</th>
<th scope="col">MPI</th>
<th scope="col">MPI +<br />OpenMP 4.5</th>
<th scope="col">Comments</th>
</tr><tr><th colspan="1" rowspan="3" scope="row">IBM XL</th>
<td>C</td>
<td><span class="fixed">mpixlc<br />mpicc</span></td>
<td><span class="fixed">mpixlc-gpu<br />mpicc-gpu</span></td>
<td colspan="1" rowspan="3">The -gpu commands add the flags:<br /><span class="fixed">-qsmp=omp<br />-qoffload</span><br /> </td>
</tr><tr><td scope="row">C++</td>
<td><span class="fixed">mpixlC<br />mpiCC<br />mpic++<br />mpicxx</span></td>
<td><span class="fixed">mpixlC-gpu<br />mpiCC-gpu<br />mpic++-gpu<br />mpicxx-gpu</span></td>
</tr><tr><td scope="row">Fortran</td>
<td><span class="fixed">mpixlf<br />mpifort<br />mpif77<br />mpif90</span></td>
<td><span class="fixed">mpixlf-gpu<br />mpifort-gpu<br />mpif77-gpu<br />mpif90-gpu</span></td>
</tr><tr><th colspan="1" rowspan="2" scope="row">Clang</th>
<td>C</td>
<td><span class="fixed">mpiclang</span></td>
<td><span class="fixed">mpiclang-gpu</span></td>
<td colspan="1" rowspan="2">The -gpu commands add the flags:<br /><span class="fixed">-fopenmp<br />-fopenmp-targets=nvptx64-nvidia-cuda</span></td>
</tr><tr><td scope="row">C++</td>
<td><span class="fixed">mpiclang++</span></td>
<td><span class="fixed">mpiclang++-gpu</span></td>
</tr><tr><th colspan="1" rowspan="3" scope="row">GNU</th>
<td>C</td>
<td><span class="fixed">mpigcc</span></td>
<td><span class="fixed">n/a</span></td>
<td colspan="1" rowspan="3">For OpenMP use the flag: <span class="fixed">-fopenmp</span><br /> </td>
</tr><tr><td scope="row">C++</td>
<td><span class="fixed">mpig++</span></td>
<td><span class="fixed">n/a</span></td>
</tr><tr><td scope="row">Fortran</td>
<td><span class="fixed">mpigfortran</span></td>
<td><span class="fixed">n/a</span></td>
</tr><tr><th colspan="1" rowspan="3" scope="row">PGI</th>
<td>C</td>
<td><span class="fixed">mpipgcc</span></td>
<td><span class="fixed">n/a</span></td>
<td colspan="1" rowspan="3">pgf90 and pgfortran are the same compiler, supporting the Fortran 2003 language specification<br />For OpenMP use the flag: <span class="fixed">-mp</span></td>
</tr><tr><td scope="row">C++</td>
<td><span class="fixed">mpig++</span></td>
<td><span class="fixed">n/a</span></td>
</tr><tr><td scope="row">Fortran</td>
<td><span class="fixed">mpipgf90<br />mpipgfortran</span></td>
<td><span class="fixed">n/a</span></td>
</tr></table><h3><strong>Compiling MPI Applications with CUDA</strong></h3>
<ul><li><span>If you use CUDA C/C++ in your application, the NVIDIA <span class="fixed">nvcc</span> compiler driver is required. </span></li>
<li><span>The nvcc driver should already be in your PATH since a CUDA module is automatically loaded for sierra systems users. </span></li>
<li>Method 1:  Use nvcc to compile CUDA *.cu source files to *.o files. Then use a C/C++ MPI compiler wrapper to compile non-CUDA C/C++ source files and link with the CUDA object files.  Including <span class="fixed">-lcudart</span> runtime library is required. For example:<br /><span class="fixed">nvcc -c  vecAdd.cu<br />mpicxx mpiapp.cpp vecAdd.o  -L/usr/tce/packages/cuda/cuda-10.1.243/lib64 -lcudart -o  mpiapp<br />mpicxx mpiapp.c vecAdd.o  -L/usr/tce/packages/cuda/cuda-10.1.243/lib64 -lcudart -o  mpiapp</span><br /> </li>
<li>Method 2: Use nvcc to compile all files: To invoke nvcc as the actual compiler in your build system and have it use the MPI-aware mpicxx/mpicc compiler for all non-GPU code, use <strong>nvcc -ccbin=mpicxx</strong>.  Note that nvcc is strictly a C++ compiler, not a C compiler. The C++ compiler you obtain will still be the one determined by the compiler module you have loaded. For example:<br /><span class="fixed">nvcc -ccbin=mpicxx mpiapp.cpp vecAdd.cu -o mpiapp<br />nvcc -ccbin=mpicxx mpiapp.c vecAdd.cu -o mpiapp</span><br /> </li>
</ul><h3><strong>Running MPI Jobs</strong></h3>
<ul><li><span class="note-green">Note:</span> Only a very brief summary is provided here.  Please see the <a href="#Running">Running Jobs Section</a> for the many details related to running MPI jobs on Sierra systems.</li>
<li>Running MPI jobs on LC's Sierra systems is very different than other LC clusters.</li>
<li>IBM Platform LSF is used as the workload manager, not SLURM:
<ul><li>LSF syntax is used in batch scripts</li>
<li>LSF commands are used to submit, monitor and interact with jobs</li>
</ul></li>
<li>The MPI job launch commands are:
<ul><li><span class="fixed"><a href="#jsrun">jsrun</a>:</span> native job launch command developed by IBM for the Oak Ridge and Livermore CORAL systems.</li>
<li><span class="fixed"><a href="#lrun">lrun</a>:</span> simplified and binding optimized LC developed alternative to jsrun.</li>
<li><span class="fixed">srun:</span> LC developed job launch command for compatibility with srun on other LC systems.</li>
</ul></li>
<li>Task binding:
<ul><li>The performance of MPI applications can be significantly impacted by the way tasks are bound to cores. </li>
<li>Parallel jobs launched with the jsrun and lrun commands have very different task, thread and GPU bindings.</li>
<li>See the <a href="#Binding">Process, Thread and GPU Binding: js_task_info</a> section for additional information.</li>
</ul></li>
</ul><h3><strong>Documentation</strong></h3>
<ul><li><a href="https://hpc.llnl.gov/sites/default/files/MPI-SpectrumUserGuide_0.pdf" target="_blank">IBM Spectrum MPI User Guide</a> (local)</li>
</ul><h2><a name="OpenMP" id="OpenMP"></a>OpenMP</h2>
<h3><strong>OpenMP Support</strong></h3>
<ul><li>The OpenMP API is supported on Sierra systems for single-node, shared-memory parallel programming in C/C++ and Fortran.</li>
<li>On Sierra systems, <strong><em>the primary motivation for using OpenMP is to take advantage of the GPUs on each node:</em></strong>
<ul><li>OpenMP is used in combination with MPI as usual</li>
<li>On-node: MPI tasks identify computationally intensive sections of code for offloading to the node's GPUs</li>
<li>On-node: Parallel regions are executed on the node's GPUs</li>
<li>Inter-node: Tasks coordinate work across the network using MPI message passing communications</li>
</ul></li>
<li><span class="note-green">Note</span> The ability to perform GPU offloading depends upon the compiler being used - see the table below.</li>
<li>The version of OpenMP support depends upon the compiler used. For example:</li>
</ul><table class="table table-striped table-bordered" summary="OpenMP Support and GPU Offloading for different compilers"><tr><th scope="col">Compiler</th>
<th scope="col">OpenMP Support</th>
<th scope="col">GPU Offloading?</th>
</tr><tr><td>IBM XL C/C++ version 13+</td>
<td>OpenMP 4.5</td>
<td>Yes</td>
</tr><tr><td>IBM XL Fortran version 15+</td>
<td>OpenMP 4.5</td>
<td>Yes</td>
</tr><tr><td>IBM Clang C/C++ version 3.8+</td>
<td>OpenMP 4.5</td>
<td>Yes</td>
</tr><tr><td>GNU version 4.9.3<br />GNU version 6.1+</td>
<td>OpenMP 4.0<br />OpenMP 4.5</td>
<td>No<br />No</td>
</tr><tr><td>PGI version 17+</td>
<td>OpenMP 4.5</td>
<td>No</td>
</tr></table><p>See <a href="https://www.openmp.org/resources/openmp-compilers/" target="_blank">https://www.openmp.org/resources/openmp-compilers/</a> for the latest information.</p>
<h3><strong>Compiling</strong></h3>
<ul><li>The usual compiler flags are used to turn on OpenMP compilation.</li>
<li>GPU offloading currently requires additional flag(s) when supported.</li>
<li><span class="note-green">Note</span> For convenience, LC has created <span class="fixed">*-gpu</span> wrapper scripts which turn on both OpenMP and GPU offloading (IBM XL and Clang only). Simply append <span class="fixed">-gpu</span> to the usual compiler command. For example: <span class="fixed">mpixlc-gpu</span>.</li>
<li>Also for convenience, LC aliases all IBM XL compiler commands to their thread-safe (<span class="fixed"> _r</span> ) command .</li>
<li>The table below summarizes OpenMP compiler flags and wrapper scripts.</li>
</ul><table class="table table-striped table-bordered" summary="Summary of OpenMP compiler flags and wrapper scripts"><tr><th scope="col">Compiler</th>
<th scope="col">OpenMP flag</th>
<th scope="col">GPU offloading flag</th>
<th scope="col">LC <span class="fixed">*-gpu</span> wrappers?</th>
</tr><tr><td>IBM XL</td>
<td><span class="fixed">-qsmp=omp</span></td>
<td><span class="fixed">-qoffload</span></td>
<td>Yes</td>
</tr><tr><td>IBM Clang</td>
<td><span class="fixed">-fopenmp</span></td>
<td><span class="fixed">-fopenmp-targets=nvptx64-nvidia-cuda</span></td>
<td>Yes</td>
</tr><tr><td>GNU</td>
<td><span class="fixed">-fopenmp</span></td>
<td><span class="fixed">n/a</span></td>
<td>No</td>
</tr><tr><td>PGI</td>
<td><span class="fixed">-mp</span></td>
<td><span class="fixed">n/a</span></td>
<td>No</td>
</tr></table><h3><strong>Thread Binding</strong></h3>
<ul><li>The performance of OpenMP applications can be significantly impacted by the way threads are bound to cores.  </li>
<li>Parallel jobs launched with the jsrun and lrun commands have very different task, thread and GPU bindings.</li>
<li>See the <a href="#Binding">Process, Thread and GPU Binding: js_task_info</a> section for additional information.</li>
</ul><h3><strong>More Information</strong></h3>
<ul><li>For non-GPU (host only) OpenMP, the usual programming practices, rules, etc. apply. These are well documented and numerous sources of information and examples are available on the web. Two are listed here:
<ul><li>OpenMP tutorial: <a href="https://hpc.llnl.gov/openmp-tutorial">https://hpc.llnl.gov/openmp-tutorial</a></li>
<li>OpenMP website: <a href="https://openmp.org/" target="_blank">openmp.org</a>. See the <strong>Resources</strong> section.</li>
</ul></li>
<li>OpenMP 4.5+ and GPU offloading are relatively new topics, and online resources are currently limited. A few are provided below.
<ul><li>"<a href="/sites/default/files/S6510.TargetingGPUsWithOpenMP4_5.pdf" target="_blank">Targeting GPUs with OpenMP 4.5 Device Directives.</a>" GPU Technology Conference presentation by James Beyer and Jeff Larkin, NVIDIA. April 2016.</li>
<li>OpenMP 4.5 Examples from the openmp.org website:<br /><a href="https://www.openmp.org/wp-content/uploads/openmp-examples-4.5.0.pdf" target="_blank">https://www.openmp.org/wp-content/uploads/openmp-examples-4.5.0.pdf</a></li>
<li>Presentations and Tutorials from the openmp.org website:<br /><a href="https://www.openmp.org/resources/openmp-presentations/" target="_blank">https://www.openmp.org/resources/openmp-presentations/</a><br /><a href="https://www.openmp.org/resources/tutorials-articles/" target="_blank">https://www.openmp.org/resources/tutorials-articles/</a></li>
</ul></li>
</ul><h2><a name="SysConfig" id="SysConfig"></a>System Configuration and Status Information</h2>
<h3><strong>First Things First</strong></h3>
<ul><li>Before you attempt to run your parallel application, it is important to know a few details about the way the system is configured. This is especially true at LC where every system is configured differently and where things change frequently.</li>
<li>It is also useful to know the status of the machines you intend on using. Are they available or down for maintenance?</li>
<li>System configuration and status information for all LC systems is readily available from the <strong>LC Homepage</strong> and the <strong>MyLC Portal</strong>. Summarized below.</li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-1048" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/lchomepage-jpg">LChomepage.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/LChomepage.jpg"><img alt="LC Homepage screenshot" height="256" width="400" style="height: 256px; width: 400px;" class="media-element file-default" data-delta="100" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/LChomepage-400x256.jpg" /></a>  </div>

  
</div>
</div><br />LC Homepage: <a href="https://hpc.llnl.gov">hpc.llnl.gov</a></div>
<div class="float-left">
<p></p><div class="media media-element-container media-default"><div id="file-1049" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mylchomepage-jpg">MYLChomepage.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/MYLChomepage.jpg"><img alt="MyLC User Portal Screenshot" height="258" width="400" style="height: 258px; width: 400px;" class="media-element file-default" data-delta="59" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/MYLChomepage-400x258.jpg" /></a>  </div>

  
</div>
</div><br />MyLC User Portal: <a href="https://mylc.llnl.gov">mylc.llnl.gov</a>
</div>
<h3><strong>System Configuration Information</strong></h3>
<ul><li>LC Homepage:
<ul><li><a href="https://hpc.llnl.gov/user-portal" target="_blank">hpc.llnl.gov/user-portal</a> (User Portal toggle) ==&gt; Hardware ==&gt; Compute Platforms</li>
<li>Direct link: <a href="https://hpc.llnl.gov/hardware/platforms" target="_blank">https://hpc.llnl.gov/hardware/platforms</a></li>
<li>All production systems appear in a summary table showing basic hardware information.</li>
<li>Diving on a machine's name will take you to a page of detailed hardware and configuration information for that machine.</li>
</ul></li>
<li>MyLC Portal:
<ul><li><a href="https://mylc.llnl.gov/" target="_blank">mylc.llnl.gov</a></li>
<li>Click on a machine name in the "machine status" portlet, or the "my accounts" portlet.</li>
<li>Then select the "details", "topology" and/or "job limits" tabs for detailed hardware and configuration information.</li>
</ul></li>
<li>LC Tutorials:
<ul><li>Located on the LC Homepage under the "Training" menu.</li>
<li>Direct link: <a href="https://hpc.llnl.gov/training/tutorials" target="_blank">https://hpc.llnl.gov/training/tutorials</a></li>
<li>Very detailed hardware information with photos and diagrams is included in the <a href="https://computing.llnl.gov/tutorials/linux_clusters/" target="_blank">Linux Clusters Overview</a>.</li>
</ul></li>
<li>Systems Summary Tables:
<ul><li>Systems Summary Table: <a href="https://hpc.llnl.gov/hardware/platforms" target="_blank">https://hpc.llnl.gov/hardware/platforms</a>. Concise summary of basic hardware information for LC systems.</li>
<li>LC Systems Summary: <a href="https://hpc.llnl.gov/sites/default/files/LC-systems-summary.pdf" target="_blank">https://hpc.llnl.gov/sites/default/files/LC-systems-summary.pdf</a>. Even more concise 1-page summary of LC production systems.</li>
</ul></li>
</ul><h3><strong>System Configuration Commands</strong></h3>
<ul><li>After logging into a machine, there are a number of commands that can be used for determining detailed, real-time machine hardware and configuration information.</li>
<li>A table of some useful commands with example output is provided below. Hyperlinked commands display their man page.</li>
</ul><table class="table table-striped table-bordered" summary="Some Useful System Configuration Commands"><tr><th scope="col">Command</th>
<th scope="col">Description</th>
<th scope="col">Example Output</th>
</tr><tr><td><span class="fixed">news job.lim.<em>machinename</em></span></td>
<td>LC command for displaying system configuration, job limits and usage policies, where machinename is the actual name of the machine.</td>
<td>
<p><button>Example Output % news job.lim._</button></p>
</td>
</tr><tr><td><span class="fixed">lscpu</span></td>
<td>Basic information about the CPU(s), including model, cores, sockets, threads, clock and cache.</td>
<td>
<p><button>Example Output % lscpu</button></p>
</td>
</tr><tr><td><span class="fixed">lscpu -e</span></td>
<td>One line of basic information about the CPU(s), cores, sockets, threads and clock.</td>
<td>
<p><button>Example Output % lscpu -e</button></p>
</td>
</tr><tr><td><span class="fixed">cat /proc/cpuinfo</span></td>
<td>Model and clock information for each thread of each core.</td>
<td>
<p><button>Example Output % cat /proc/cpuinfo</button></p>
</td>
</tr><tr><td><span class="fixed">topo</span></td>
<td>Display a graphical topological map of node hardware.</td>
<td>
<p><button>Example Output % topo</button></p>
</td>
</tr><tr><td><span class="fixed">lstopo --only cores</span></td>
<td>List the physical cores only.</td>
<td>
<p><button>Example Output % lstopo --only cores</button></p>
</td>
</tr><tr><td><span class="fixed">lstopo -v</span></td>
<td>Detailed (verbose) information about a node's hardware components.</td>
<td>
<p><button>Example Output % lstopo -v</button></p>
</td>
</tr><tr><td><span class="fixed">vmstat -s</span></td>
<td>Memory configuration and usage details.</td>
<td>
<p><button>Example Output % vmstat -s</button></p>
</td>
</tr><tr><td><span class="fixed">cat /proc/meminfo</span></td>
<td>
<p>Memory configuration and usage details.</p>
</td>
<td><button>Example Output % cat /proc/meminfo</button></td>
</tr><tr><td><span class="fixed">uname -a<br />distro_version<br />cat /etc/redhat-release<br />cat /etc/toss-release</span></td>
<td>
<p>Display operating system details, version.</p>
<p> </p>
</td>
<td><button>Example Output % uname -a</button></td>
</tr><tr><td><span class="fixed">bdf<br />df -h</span></td>
<td>Show mounted file systems.</td>
<td>
<p><button>Example Output % bdf</button></p>
</td>
</tr><tr><td><span class="fixed">bparams<br />bqueues<br />bhosts<br />lshosts</span></td>
<td>Display LSF system settings and options<br />Display LSF queue information<br />Display information about LSF hosts<br />Display information about LSF hosts
<p>See the <a href="#LSFconfig">LSF Configuration Commands</a> section for additional information.</p>
</td>
<td> </td>
</tr></table><h3><strong>System Status Information</strong></h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1050" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/systemstatusmenu-png">systemStatusMenu.png</a></h2>
    
  
  <div class="content">
    <img alt="System status menu on HPC site" height="323" width="500" class="media-element file-default" data-delta="60" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/systemStatusMenu.png" /></div>

  
</div>
</div></div>
<ul><li>LC Homepage:
<ul><li><a href="https://hpc.llnl.gov/user-portal" target="_blank">hpc.llnl.gov/user-portal</a> (User Portal toggle) - just look on the main page for the System Status links (shown at right).</li>
<li>The same links appear under the <a href="https://hpc.llnl.gov/hardware" target="_blank">Hardware</a> menu.</li>
<li>Unclassified systems only</li>
</ul></li>
<li>MyLC Portal:
<ul><li><a href="https://mylc.llnl.gov/" target="_blank">mylc.llnl.gov</a></li>
<li>Several portlets provide system status information:
<ul><li>machine status</li>
<li>login node status</li>
<li>scratch file system status</li>
<li>enclave status</li>
</ul></li>
<li>Classified MyLC is at: <a href="https://lc.llnl.gov/lorenz/">https://lc.llnl.gov/lorenz/</a></li>
</ul></li>
<li>Machine status email lists:
<ul><li>Provide the most timely status information for system maintenance, problems, and system changes/updates</li>
<li>ocf-status and scf-status cover all machines on the OCF / SCF</li>
<li>Additionally, each machine has its own status list - for example:<br /><strong><a href="mailto:sierra-status@llnl.gov">sierra-status@llnl.gov</a></strong></li>
</ul></li>
<li>Login banner &amp; news items - always displayed immediately after logging in
<ul><li>Login banner includes basic configuration information, announcements and news items. Example login banner HERE.</li>
<li>News items (unread) appear at the bottom of the login banner. For usage, type <span class="fixed">news -h</span>.</li>
</ul></li>
<li>Direct links for systems and file systems status pages:</li>
</ul><table class="table table-striped table-bordered" summary="Direct links for systems and file systems status pages"><tr><th scope="col">Description</th>
<th scope="col">Network</th>
<th scope="col">Links</th>
</tr><tr><td colspan="1" rowspan="3">System status web pages</td>
<td>OCF CZ</td>
<td><a href="https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" target="_blank">https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi</a></td>
</tr><tr><td>OCF RZ</td>
<td><a href="https://rzlc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" target="_blank">https://rzlc.llnl.gov/cgi-bin/lccgi/customstatus.cgi</a></td>
</tr><tr><td>SCF</td>
<td><a href="https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" target="_blank">https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi</a></td>
</tr><tr><td colspan="1" rowspan="4">File Systems status web pages</td>
<td>OCF CZ</td>
<td><a href="https://lc.llnl.gov/fsstatus/fsstatus.cgi" target="_blank">https://lc.llnl.gov/fsstatus/fsstatus.cgi</a></td>
</tr><tr><td>OCF RZ</td>
<td><a href="https://rzlc.llnl.gov/fsstatus/fsstatus.cgi" target="_blank">https://rzlc.llnl.gov/fsstatus/fsstatus.cgi</a></td>
</tr><tr><td>OCF CZ+RZ</td>
<td><a href="https://rzlc.llnl.gov/fsstatus/allfsstatus.cgi" target="_blank">https://rzlc.llnl.gov/fsstatus/allfsstatus.cgi</a></td>
</tr><tr><td>SCF</td>
<td><a href="https://lc.llnl.gov/fsstatus/fsstatus.cgi" target="_blank">https://lc.llnl.gov/fsstatus/fsstatus.cgi</a></td>
</tr></table><h2><a name="Running" id="Running"></a>Running Jobs</h2>
<h3>Overview</h3>
<p><br />A brief summary of running jobs is provided below, with more detail in sections that follow.</p>
<h4><strong>Very Different From Other LC Systems</strong></h4>
<ul><li>Although Sierra systems share a number of similarities with other LC clusters, running jobs is very different.</li>
<li><strong>IBM Spectrum LSF </strong>is used as the Workload Manager instead of Slurm:
<ul><li>Entirely new command set for submitting, monitoring and interacting with jobs.</li>
<li>Entirely new command set for querying the system's configuration, queues, job statistics and accounting information.</li>
<li>New syntax for creating job scripts.</li>
</ul></li>
<li>The <span class="fixed">jsrun</span> command is used to launch jobs instead of Slurm's <span class="fixed">srun</span> command:
<ul><li>Developed by IBM for the LLNL and Oak Ridge CORAL systems.</li>
<li>Command syntax is very different.</li>
<li>New concept of <strong><em>resource sets</em></strong> for defining how a node looks to a job.</li>
</ul></li>
<li>The <strong>lrun</strong> command with simplified syntax can be used instead to launch jobs:
<ul><li>Developed by LC to make job submissions easier for most types of jobs</li>
<li>Actually runs the jsrun command under the hood</li>
</ul></li>
<li>There are both <strong><em>login nodes</em></strong> and <strong><em>launch nodes</em></strong>:
<ul><li>Users login to login nodes, which are shared by other users. Intended for interactive activities such as editing files, submitting batch/interactive jobs, running GUIs, short, non-parallel compiling. Not intended for running production, parallel jobs or long cpu-intensive compiling.</li>
<li>Batch and interactive jobs are both submitted from a login node.</li>
<li>They are then migrated to a launch node where they are managed by LSF. An allocation of compute node(s) is acquired for the job. Launch nodes are shared among user jobs.</li>
<li>Parallel jobs using the<span class="fixed"> jsrun/lrun</span> command will run on the compute node allocation. </li>
<li><span class="note-green">Note:</span> at LC, the first compute node is used a "private launch node" for the job by default:
<ul><li>Shell commands in the job command script are run here</li>
<li>Serial jobs are run here, as are interactive jobs</li>
<li>Intended to prevent overloading of the shared launch nodes</li>
</ul></li>
</ul></li>
</ul><h4><strong>Accounts and Allocations</strong></h4>
<ul><li>In order to run jobs on any LC system, users must have a valid login account.</li>
<li>Additionally, users must have a valid allocation (bank) on the system.</li>
</ul><h4><strong>Queues</strong></h4>
<ul><li>As with other LC systems, compute nodes are divided into queues:
<ul><li><strong><em>pbatch:</em></strong> contains the majority of compute nodes; where most production work is done; larger job size and time limits.</li>
<li><em><strong>pdebug:</strong></em> contains a smaller subset of compute nodes; intended for short, small debugging jobs.</li>
<li>Other queues are often configured for specific purposes.</li>
</ul></li>
<li>Real production work must run in a compute node queue, not on a login or launch node.</li>
<li>Each queue has specific limits that can include:
<ul><li>Default and maximum number of nodes that a job may use</li>
<li>Default and maximum amount of time a job may run</li>
<li>Number of jobs that may run simultaneously</li>
<li>Other limits and restrictions as configured by LC</li>
<li>Queue limits can easily be viewed with the command <span class="fixed">news job.lim.<em>machinename</em></span>.  For example:  <span class="fixed">news job.lim.sierra</span></li>
</ul></li>
</ul><h4><strong>Batch Jobs - General Workflow</strong></h4>
<ol><li>Login to a login node.</li>
<li>Create / prepare executables and associated files.</li>
<li>Create an LSF job script.</li>
<li>Submit the job script to LSF with the <span class="fixed">bsub </span>command. For example:<br /><span class="fixed">bsub &lt; myjobscript</span></li>
<li>LSF will migrate the job to a launch node and acquire the requested allocation of compute nodes from the requested queue. If not specified, the default queue (usually pbatch) will be used.</li>
<li>The <span class="fixed">jsrun/lrun</span> command is used within the job script to launch the job on compute nodes. </li>
<li>Monitor and interact with the job from a login node using the relevant LSF commands.</li>
</ol><h4><strong>Interactive Jobs - General Workflow</strong></h4>
<ol><li>Login to a login node.</li>
<li>Create / prepare executables and associated files.</li>
<li>From the login node command line, request an interactive allocation of compute nodes from LSF with the <span class="fixed">bsub</span> or <span class="fixed">lalloc</span> command. For example, requests 16 nodes, Interactive pseudo-terminal, pdebug queue, running the tcsh shell.:<br /><span class="fixed">bsub -nnodes 16 -Ip -q pdebug /usr/bin/tcsh<br />-or-<br />lalloc 16 -q pdebug</span></li>
<li>LSF will migrate the job to a launch node and acquire the requested allocation of compute nodes from the requested queue. If not specified, the default queue (usually pbatch) will be used.</li>
<li>When ready, an interactive terminal session will begin the first compute node</li>
<li>From here, shell commands, scripts or parallel jobs can be executed:<br />Parallel jobs are launched with the <span class="fixed">jsrun/lrun</span> command from the shell command line or from within a user script and will execute on the allocated compute nodes.</li>
<li>LSF commands can be used to monitor and interact with the job, either from a login node or the compute node</li>
</ol><h3><a name="SummaryCommands" id="SummaryCommands"></a>Summary of Job Related Commands</h3>
<p>The table below summarizes commands commonly used for running jobs. Most of these are discussed further in the sections that follow. For LSF commands, see the man page and the LSF commands documentation for details: <a href="https://www.ibm.com/docs/en/spectrum-lsf/10.1.0" target="_blank">https://www.ibm.com/docs/en/spectrum-lsf/10.1.0</a></p>
<table class="table table-striped table-bordered" summary="Summary of commands commonly used for running jobs"><tr><th scope="col">Command</th>
<th scope="col">Source</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">bhist</span></td>
<td>LSF</td>
<td>Displays historical information about jobs. By default, displays information about your pending, running, and suspended jobs. Some useful options include:<br />-d, -p, -r, -s : show finished (-d), pending (-p), running (-r), suspended (-s) jobs<br />-l : long format listing, maximum details<br />-u username: jobs for specified username<br />-w : wide format listing<br />jobid : use bhist jobid to see information for a specified job</td>
</tr><tr><td><span class="fixed">bhosts</span></td>
<td>LSF</td>
<td>Displays hosts and their static and dynamic resources. Default format is condensed. Marginally useful command for average user. Some useful options include:<br />-l : long format listing, maximum details<br />-X : uncondensed format - one line per host instead of per rack</td>
</tr><tr><td><span class="fixed">bjobs</span></td>
<td>LSF</td>
<td>Displays information about LSF jobs. Numerous options - some useful ones include:<br />-d, -p, -r, -s : show finished (-d), pending (-p), running (-r), suspended (-s) jobs<br />-l: long detailed listing<br />-u username: jobs for specified username<br />-u all: show jobs for all users<br />-X: display actual host names (uncondensed format)<br />jobid : use bhist jobid to see information for a specified job</td>
</tr><tr><td><span class="fixed">bkill</span></td>
<td>LSF</td>
<td>Sends signals to kill, suspend, or resume unfinished jobs. Some useful options include:<br />-b: kill multiple jobs, queued and running<br />-l: display list of supported signals<br />-s signal: sends specified signal<br />jobid: operates on specified jobid</td>
</tr><tr><td><span class="fixed">bmgroup</span></td>
<td>LSF</td>
<td>Show which group nodes belong to (debug, batch, etc).</td>
</tr><tr><td><span class="fixed">bmod</span></td>
<td>LSF</td>
<td>Modify a job’s parameters (e.g. add dependency). Numerous options.</td>
</tr><tr><td><span class="fixed">bparams</span></td>
<td>LSF</td>
<td>Displays information about (over 190) configurable LSF system parameters. Use the -a flag to see all parameters.</td>
</tr><tr><td><span class="fixed">bpeek</span></td>
<td>LSF</td>
<td>Displays the standard output and standard error produced by an unfinished job, up to the time that the command is run.</td>
</tr><tr><td><span class="fixed">bqueues</span></td>
<td>LSF</td>
<td>Displays information about queues. Useful options:<br />-l: long listing with details<br />-r: similar to -l, but also includes fair share scheduling information</td>
</tr><tr><td><span class="fixed">bresume</span></td>
<td>LSF</td>
<td>Resume (re-enable) a suspended job, so it can be scheduled to run</td>
</tr><tr><td><span class="fixed">bslots</span></td>
<td>LSF</td>
<td>Displays slots available and backfill windows available for backfill jobs.</td>
</tr><tr><td><span class="fixed">bstop</span></td>
<td>LSF</td>
<td>Suspend an queued job.</td>
</tr><tr><td><span class="fixed">bsub</span></td>
<td>LSF</td>
<td>Submit a job to LSF for execution. Typically submitted as a job script, though this is not required (interactive prompting mode).</td>
</tr><tr><td><span class="fixed">bugroup</span></td>
<td>LSF</td>
<td>Displays information about user groups. The -l option provides additional information.</td>
</tr><tr><td><span class="fixed">check_sierra_nodes</span></td>
<td>LC</td>
<td>LLNL-specific script to test nodes in allocation</td>
</tr><tr><td><span class="fixed">js_task_info</span></td>
<td>IBM</td>
<td>MPI utility that prints task, thread and GPU binding info for each MPI rank</td>
</tr><tr><td><span class="fixed">jsrun</span></td>
<td>IBM</td>
<td>Primary parallel job launch command. Replaces srun / mpirun found on other systems.</td>
</tr><tr><td><span class="fixed">lacct</span></td>
<td>LC</td>
<td>Displays information about completed jobs. The -h option shows usage information.</td>
</tr><tr><td><span class="fixed">lalloc</span></td>
<td>LC</td>
<td>Allocates nodes interactively and executes a shell or optional command on the first compute node by default. The -h option shows usage information.</td>
</tr><tr><td><span class="fixed">lbf</span></td>
<td>LC</td>
<td>Show backfill slots.  The -h option shows usage information.</td>
</tr><tr><td><span class="fixed">lreport</span></td>
<td>LC</td>
<td>Generates usage report for completed jobs. The -h option shows usage information.</td>
</tr><tr><td><span class="fixed">lrun</span></td>
<td>LC</td>
<td>An LC alternative to the jsrun parallel job launch command. Simpler syntax suitable for most jobs.</td>
</tr><tr><td><span class="fixed">lsclusters</span></td>
<td>LSF</td>
<td>View cluster status and size.</td>
</tr><tr><td><span class="fixed">lsfjobs</span></td>
<td>LC</td>
<td>LC command for displaying LSF job and queue information.</td>
</tr><tr><td><span class="fixed">lshare</span></td>
<td>LC</td>
<td>Display bank allocation and usage information. The -h option shows usage information.</td>
</tr><tr><td><span class="fixed">lshosts</span></td>
<td>LSF</td>
<td>Displays information about hosts - one line each by default. The -l option provides additional details for each host.</td>
</tr><tr><td><span class="fixed">lsid</span></td>
<td>LSF</td>
<td>Display LSF version and copyright information, and the name of the cluster.</td>
</tr><tr><td><span class="fixed">mpibind</span></td>
<td>LC</td>
<td>LLNL-specific bind utility.</td>
</tr><tr><td><span class="fixed">srun</span></td>
<td>LC</td>
<td>Wrapper for the lrun command provided for compatibility with srun command used on other LC systems.</td>
</tr></table><h3><strong><a name="BatchScripts" id="BatchScripts"></a></strong>Batch Scripts and #BSUB / bsub</h3>
<h4><strong><span>LSF Batch Scripts</span></strong></h4>
<ul><li>As with all other LC systems, running batch jobs requires the use of a batch job script:
<ul><li>Plain text file created by the user to describe job requirements, environment and execution logic</li>
<li>Commands, directives and syntax specific to a given batch system</li>
<li>Shell scripting</li>
<li>References to environment and script variables</li>
<li>The application(s) to execute along with input arguments and options</li>
</ul></li>
<li>What makes Sierra systems different is that IBM Spectrum LSF is used as the Workload Manager instead of Slurm:
<ul><li>Batch scripts are required to use LSF <strong>#BSUB</strong> syntax</li>
<li>Shell scripting, environment variables, etc. are the same as other batch scripts</li>
</ul></li>
<li>An example LSF batch script is shown below. The #BSUB syntax is discussed next.</li>
</ul><pre>#!/bin/tcsh

    ### LSF syntax
    #BSUB -nnodes 8                   #number of nodes
    #BSUB -W 120                      #walltime in minutes
    #BSUB -G guests                   #account
    #BSUB -e myerrors.txt             #stderr
    #BSUB -o myoutput.txt             #stdout
    #BSUB -J myjob                    #name of job
    #BSUB -q pbatch                   #queue to use

    ### Shell scripting
    date; hostname
    echo -n 'JobID is '; echo $LSB_JOBID
    cd /p/gscratch1/joeuser/project
    cp ~/inputs/run2048.inp .

    ### Launch parallel executable
    jsrun -n16 -r2 -a20 -g2 -c20 myexec

    echo 'Done'</pre><p> </p>
<ul><li>Usage notes:
<ul><li>The <strong>#BSUB</strong> keyword is case sensitive</li>
<li>The <span class="fixed">jsrun</span> command is used to launch parallel jobs</li>
</ul></li>
</ul><h4>#BSUB / bsub</h4>
<ul><li>Within a batch script, <strong>#BSUB</strong> keyword syntax is used to specify LSF job options.</li>
<li>The <span class="fixed">bsub</span> command is then used to submit the batch script to LSF for execution. For example:<br /><span class="fixed">bsub &lt; mybatchscript</span><br /><span class="note-green">Note </span>the use of input redirection to submit the batch script. This is required.</li>
<li>The exact same options specified by #BSUB in a batch script can be specified on the command line with the <span class="fixed">bsub</span> command. For example:<br /><span class="fixed">bsub -q pdebug &lt; mybatchscript</span></li>
<li>If <span class="fixed">bsub</span> and #BSUB options conflict, the command line option will take precedence.</li>
<li>The table below lists some of the more common #BSUB / bsub options.<br />For other options and more in-depth information, consult the bsub man page and/or the <a href="#R3">LSF documentation</a>.</li>
</ul><table class="table table-striped table-bordered" summary="Common BSUB Options"><tr><th colspan="3" scope="col">Common BSUB Options</th>
</tr><tr><td><strong>Option</strong></td>
<td><strong>Example<br /><em>Can be used with bsub command also</em></strong></td>
<td><strong>Description</strong></td>
</tr><tr><td><span class="fixed">-B</span></td>
<td><span class="fixed">#BSUB -B</span></td>
<td>Send email when job begins</td>
</tr><tr><td><span class="fixed">-b</span></td>
<td><span class="fixed">#BSUB -b 15:00</span></td>
<td>Dispatch the job for execution on or after the specified date and time. - in this case 3pm. Time format is [[[YY:]MM:]DD:]hh:mm</td>
</tr><tr><td><span class="fixed">-cwd</span></td>
<td><span class="fixed">#BSUB -cwd /p/gpfs1/joeuser/</span></td>
<td>Specifies the current working directory for job execution. The default is the directory from where the job was submitted.</td>
</tr><tr><td><span class="fixed">-e</span></td>
<td><span class="fixed">#BSUB -e mystderr.txt<br />#BSUB -e joberrors.%J<br />#BSUB -eo mystderr.txt</span></td>
<td>File into which job stderr will be written. If used, %J will be replaced with the job ID number. If the file exists, it will be appended by default. Use -eo to overwrite. If -e is not used, stderr will be combined with stdout in the stdout file by default.</td>
</tr><tr><td><span class="fixed">-G</span></td>
<td><span class="fixed">#BSUB -G guests</span></td>
<td>At LC this option specifies the account to be used for the job. Required.</td>
</tr><tr><td><span class="fixed">-H</span></td>
<td><span class="fixed">#BSUB -H</span></td>
<td>Holds the job in the PSUSP state when the job is submitted. The job is not scheduled until you tell the system to resume the job using the <span class="fixed">bresume</span> command.</td>
</tr><tr><td><span class="fixed">-i</span></td>
<td><span class="fixed">#BSUB -i myinputfile.txt</span></td>
<td>Gets the standard input for the job from specified file path.</td>
</tr><tr><td><span class="fixed">-Ip</span></td>
<td><span class="fixed">bsub -Ip /bin/tcsh</span></td>
<td>Interactive only. Submits an interactive job and creates a pseudo-terminal when the job starts. See the <a href="#Interactive">Interactive Jobs</a> section for details.</td>
</tr><tr><td><span class="fixed">-J</span></td>
<td><span class="fixed">#BSUB -J myjobname</span></td>
<td>Specifies the name of the job. Default name is the name of the job script.</td>
</tr><tr><td><span class="fixed">-N</span></td>
<td><span class="fixed">#BSUB -N</span></td>
<td>Send email when job ends</td>
</tr><tr><td><span class="fixed">-nnodes</span></td>
<td><span class="fixed">#BSUB -nnodes 128</span></td>
<td>Number of nodes to use</td>
</tr><tr><td><span class="fixed">-o</span></td>
<td><span class="fixed">#BSUB -o myoutput.txt<br />#BSUB -o joboutput.%J<br />#BSUB -oo myoutput.txt</span></td>
<td>File into which job stdout will be written. If used, %J will be replaced with the job ID number. Default output file name is jobid.out. stderr is combined with stdout by default. If the output file already exists, it is appended by default. Use -oo to overwrite.</td>
</tr><tr><td><span class="fixed">-q</span></td>
<td><span class="fixed">#BSUB -q pdebug</span></td>
<td>Specifies the name of the queue to use</td>
</tr><tr><td><span class="fixed">-r<br />-rn</span></td>
<td><span class="fixed">#BSUB -r<br />#BSUB -rn</span></td>
<td>Rerun the job if the system fails. Will not rerun if the job itself fails. Use -rn to never rerun the job.</td>
</tr><tr><td><span class="fixed">-stage</span></td>
<td><span class="fixed">-stage storage=64</span></td>
<td>Used to specify burst buffer options. In the example shown, 64 GB of burst buffer storage is requested.</td>
</tr><tr><td><span class="fixed">-W</span></td>
<td><span class="fixed">#BSUB -W 60</span></td>
<td>Requested maximum walltime - 60 minutes in the example shown.<br />Format is [hours:]minutes, not [[hours:]minutes:]seconds like Slurm</td>
</tr><tr><td><span class="fixed">-w</span></td>
<td><span class="fixed">#BSUB -w ended(22438)</span></td>
<td>Specifies a job dependency - in this case, waiting for jobid 22438 to complete. See the man page and/or documentation for dependency expression options.</td>
</tr><tr><td><span class="fixed">-XF</span></td>
<td><span class="fixed">#BSUB -XF</span></td>
<td>Use X11 forwarding</td>
</tr></table><h4>What Happens After You Submit Your Job?</h4>
<ul><li>As shown previously, the <span class="fixed">bsub</span> command is used to submit your job to LSF from a login node. For example:<br /><span class="fixed">bsub  &lt;  mybatchscript</span></li>
</ul><ul><li>If successful, LSF will migrate and manage your job on a launch node.</li>
<li>An allocation of compute nodes will be acquired for your job in a batch queue - either one specified by you, or the default queue.</li>
<li>The<span class="fixed"> jsrun </span>command is used from within your script to launch your job on the allocation of compute nodes. Your executable then runs on the compute nodes.</li>
<li><span class="note-green">Note: </span>At LC the first compute node is used as your "private launch node" by default.   This is where your job command script commands run.</li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-1051" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/jsrundiagram-png">jsrunDiagram.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/jsrunDiagram.png"><img alt="jsrun chart beginning with the login node and moving to the compute node. " height="129" width="500" style="height: 129px; width: 500px;" class="media-element file-default" data-delta="61" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/jsrunDiagram-500x129.png" /></a>  </div>

  
</div>
</div>
<h4>Environment Variables</h4>
<ul><li>By default, LSF will import most (if not all) of your environment variables so they are available to your job.</li>
<li>If for some reason you are missing environment variables, you can use the #BSUB/bsub <span class="fixed">-env</span> option to specify variables to import. See the man page for details.</li>
<li>Additionally, LSF provides a number of its own environment variables. Some of these may be useful for querying purposes within your batch script. The table below lists a few common ones.</li>
</ul><table class="table table-striped table-bordered" summary="A few common environment variables provided by LSF"><tr><th scope="col">Variable</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">LSB_JOBID</span></td>
<td>The ID assigned to the job by LSF</td>
</tr><tr><td><span class="fixed">LSB_JOBNAME</span></td>
<td>The job's name</td>
</tr><tr><td><span class="fixed">LS_JOBPID</span></td>
<td>The job's process ID</td>
</tr><tr><td><span class="fixed">LSB_JOBINDEX</span></td>
<td>The job's index (if it belongs to a job array)</td>
</tr><tr><td><span class="fixed">LSB_HOSTS</span></td>
<td>The hosts assigned to run the job</td>
</tr><tr><td><span class="fixed">LSB_QUEUE</span></td>
<td>The queue from which the job was dispatched</td>
</tr><tr><td><span class="fixed">LS_SUBCWD</span></td>
<td>The directory from which the job was submitted</td>
</tr></table><ul><li>To see the entire list of LSF environment variables, simply use a command like <span class="fixed">printenv, set</span> or <span class="fixed">setenv </span>(shell dependent) in your batch script, and look for variables that start with <strong>LSB_</strong> or <strong>LS_</strong>.</li>
</ul><h3><a name="Interactive" id="Interactive"></a>Interactive Jobs: bsub and lalloc commands</h3>
<ul><li>Interactive jobs are often useful for quick debugging and testing purposes:
<ul><li>Allow you to acquire an allocation of compute nodes that can be interacted with from the shell command line.</li>
<li>No handing things over to LSF, and then waiting for the job to complete.</li>
<li>Easy to experiment with multiple "on the fly" runs.</li>
</ul></li>
<li>There are two main "flavors" of interactive jobs:
<ul><li><strong>Pseudo-terminal shell</strong> - uses your existing SSH login window</li>
<li><strong>Xterm</strong> - launches a new window using your default login shell</li>
</ul></li>
<li>The LSF <span class="fixed">bsub</span> command, and the LC <span class="fixed">lalloc</span> command can both be used for interactive jobs.</li>
<li>Examples:</li>
</ul><p><strong>Starting a pseudo-terminal interactive job using bsub:</strong><br />From a login node, the bsub command is used to request 4 nodes in an Interactive pseudo-terminal, X11 Forwarding, Wall clock limit of 10 minutes, in a tcsh shell. After the dispatch the interactive session starts on the first compute node (by default). The bjobs -X command is used to display the compute nodes allocated for this job.</p>
<pre>rzansel61% <span class="text-danger">bsub -nnodes 4 -Ip -XF -W 10 /bin/tcsh</span>
Job &lt;206798&gt; is submitted to default queue &lt;pdebug&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on rzansel62&gt;&gt;

rzansel5% <span class="text-danger">bjobs -X</span>
JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
206798  blaise  RUN   pdebug     rzansel61   1*rzansel62 /bin/tcsh  Aug 28 11:53
                                             40*rzansel5
                                             40*rzansel6
                                             40*rzansel29
                                             40*rzansel9</pre><p> </p>
<p><strong>Starting a pseudo-terminal interactive job using lalloc:</strong><br />This same action can be performed more simply using LC's<strong> lalloc</strong> command.  Note that by default, lalloc will use the first compute node as a private launch node. For example:</p>
<pre>sierra4362% <span class="text-danger">lalloc 4</span>
+ exec bsub -nnodes 4 -Is -XF -W 60 -core_isolation 2 /usr/tce/packages/lalloc/lalloc-2.0/bin/lexec
Job &lt;281904&gt; is submitted to default queue &lt;pbatch&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on sierra4370&gt;&gt;
&lt;&lt;Waiting for JSM to become ready ...&gt;&gt;
&lt;&lt;Redirecting to compute node sierra1214, setting up as private launch node&gt;&gt;
sierra1214%</pre><p><strong>Starting an xterm interactive job using bsub:</strong><br />Similar, but opens a new xterm window on the first compute node instead of a tcsh shell in the existing window.<br />The xterm options follow the xterm command.</p>
<pre>sierra4358% <span class="text-danger">bsub -nnodes 4 -XF xterm -sb -ls -fn ergo17 -rightbar</span>
Job &lt;22530&gt; is submitted to default queue &lt;pbatch&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;
sierra4358%
<em>[ xterm running on first compute node appears on screen at this point ]</em></pre><p><span><strong>Starting an xterm interactive job using lalloc:</strong></span><br /><span>Same as previous bsub xterm example, but using lalloc</span></p>
<pre>rzansel61% <span class="text-danger">lalloc 4 xterm</span>
+ exec bsub -nnodes 4 -Is -XF -W 60 -core_isolation 2 /usr/tce/packages/lalloc/lalloc-2.0/bin/lexec xterm
Job &lt;219502&gt; is submitted to default queue &lt;pdebug&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on rzansel62&gt;&gt;
&lt;&lt;Waiting for JSM to become ready ...&gt;&gt;
&lt;&lt;Redirecting to compute node rzansel1, setting up as private launch node&gt;&gt;
<em>[ xterm running on first compute node appears on screen at this point ]</em></pre><ul><li>How it works:
<ul><li>Issuing the <span class="fixed">bsub</span> command from a login node results in control being dispatched to a <strong><em>launch node</em></strong>.</li>
<li>An allocation of compute nodes is acquired. If not specified, the default is one node.</li>
<li>The compute node allocation will be in the default queue, usually pbatch. The desired queue can be explicitly specified with the <strong>bsub -q</strong> or <span class="fixed">lalloc -q</span> option.</li>
<li>When ready, your pseudo-terminal or xterm session will run on the first compute node (default at LC). From there, you can use the <span class="fixed">jsrun</span> command to launch parallel tasks on the compute nodes.</li>
</ul></li>
</ul><ul><li>Usage notes:
<ul><li>Most of the other <span class="fixed">bjob</span> options not shown should work as expected.</li>
<li>For lalloc usage, simple type:  <span class="fixed">lalloc</span></li>
<li>Exiting the pseudo-terminal shell, or the xterm, will terminate the job.</li>
</ul></li>
</ul><h3><a name="lrun" id="lrun"></a>Launching Jobs: the lrun command</h3>
<ul><li>The <span class="fixed">lrun</span> command was developed by LC to make job launching syntax easier for most types of jobs. It can be used as an alternative to the <span class="fixed">jsrun</span> command (discussed next).</li>
<li>Like the jsrun command, it's purpose is similar to srun/mpirun used on other LC clusters, but its syntax is different.</li>
<li>Basic syntax (described in detail below):<br /><br /><span class="fixed">lrun [lrun_options] [jsrun_options(subset)] [executable] [executable_args]</span></li>
</ul><ul><li>lrun options are shown in the table below. Note that the same usage information can be found by simply typing <span class="fixed">lrun</span> when you are logged in.</li>
<li>Notes:
<ul><li>LC also provides an <span class="fixed">srun</span> wrapper for the lrun command for compatibility with the srun command used on other LC systems.</li>
<li>A discussion on which job launch command should be used can be found in the <a href="#quick12">Quickstart Guide section 12</a>.</li>
</ul></li>
</ul><table class="table table-striped table-bordered" summary="lrun options"><tr><th scope="col">Common Options</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">-N</span></td>
<td>Number of nodes within the allocation to use. If used, either the -T or -n option must also be used.</td>
</tr><tr><td><span class="fixed">-T</span></td>
<td>Number of tasks per node. If -N is not specified, all nodes in the allocation are used.</td>
</tr><tr><td>
<p><span class="fixed">-n<br />-p</span></p>
</td>
<td>Number of tasks. If -N is not specified, all nodes in the allocation are used. Tasks are evenly spaced over the number of nodes used.</td>
</tr><tr><td><span class="fixed">-1</span></td>
<td>Used for building on a compute node instead of a launch node. For example: lrun -1 make<br />Uses only 1 task on 1 node of the allocation.</td>
</tr><tr><td><span class="fixed">-M "-gpu"</span></td>
<td>Turns on CUDA-aware Spectrum MPI</td>
</tr><tr><td><strong>Other Options</strong></td>
<td><span class="fixed">--adv_map            </span>Improved mapping but simultaneous runs may be serialized<br /><span class="fixed">--threads=&lt;nthreads&gt; </span>Sets env var OMP_NUM_THREADS to nthreads<br /><span class="fixed">--smt=&lt;1|2|3|4&gt;      </span>Set smt level (default 1), OMP_NUM_THREADS overrides<br /><span class="fixed">--pack               </span>Pack nodes with job steps (defaults to -c 1 -g 0)<br /><span class="fixed">--mpibind=on         </span>Force use mpibind in --pack mode instead of jsrun's bind<br /><span class="fixed">-c &lt;ncores_per_task&gt; </span>Required COREs per MPI task (--pack uses for placement)<br /><span class="fixed">-g &lt;ngpus_per_task&gt;  </span>Required GPUs per MPI task (--pack uses for placement)<br /><span class="fixed">-W &lt;time_limit&gt;      </span>Sends SIGTERM to jsrun after minutes or H:M or H:M:S<br /><span class="fixed">--bind=off           </span>No binding/mpibind used in default or --pack mode<br /><span class="fixed">--mpibind=off        </span>Do not use mpibind (disables binding in default mode)<br /><span class="fixed">--gpubind=off        </span>Mpibind binds only cores (CUDA_VISIBLE_DEVICES unset)<br /><span class="fixed">--core=&lt;format&gt;      </span>Sets both CPU &amp; GPU coredump env vars to &lt;format&gt;<br /><span class="fixed">--core_delay=&lt;secs&gt;  </span>Set LLNL_COREDUMP_WAIT_FOR_OTHERS to &lt;secs&gt;<br /><span class="fixed">--core_cpu=&lt;format&gt;  </span>Sets LLNL_COREDUMP_FORMAT_CPU to &lt;format&gt;<br /><span class="fixed">--core_gpu=&lt;format&gt;  </span>Sets LLNL_COREDUMP_FORMAT_GPU to &lt;format&gt;<br /><span class="fixed">                     </span>where &lt;format&gt; may be core|lwcore|none|core=&lt;mpirank&gt;|lwcore=&lt;mpirank&gt;<br /><span class="fixed">-X &lt;0|1&gt;             </span>Sets --exit_on_error to 0|1 (default 1)<br /><span class="fixed">-v                   </span>Verbose mode, show jsrun command and any set env vars<br /><span class="fixed">-vvv                 </span>Makes jsrun wrapper verbose also (core dump settings)</td>
</tr><tr><td><strong>Additional Information</strong></td>
<td><span class="fixed">JSRUN OPTIONS INCOMPATIBLE WITH LRUN (others should be compatible):<br />  -a, -r, -m, -l, -K, -d, -J (and long versions like --tasks_per_rs, --nrs)<br />Note: -n, -c, -g redefined to have different behavior than jsrun's version.</span>
<p><span class="fixed">ENVIRONMENT VARIABLES THAT LRUN/MPIBIND LOOKS AT IF SET:<br />  MPIBIND_EXE &lt;path&gt;   Sets mpibind used by lrun, defaults to:<br />                       /usr/tce/packages/lrun/lrun-2019.05.07/bin/mpibind10<br />  OMP_NUM_THREADS #    If not set, mpibind maximizes based on smt and cores<br />  OMP_PROC_BIND &lt;mode&gt; Defaults to 'spread' unless set to 'close' or 'master'<br />  MPIBIND &lt;j|jj|jjj&gt;   Sets verbosity level, more j's -&gt; more output</span></p>
<p><span class="fixed">  Spaces are optional in single character options (i.e., -T4 or -T 4 valid)<br />  Example invocation: lrun -T4 js_task_info</span></p></td>
</tr></table><ul><li>Examples - assuming that the total node allocation is 8 nodes (bsub -nnodes 8):</li>
</ul><table class="table table-striped table-bordered"><tr><td><span class="fixed">lrun -N6 -T16 a.out</span></td>
<td>Launches 16 tasks on each of 6 nodes = 96 tasks</td>
</tr><tr><td><span class="fixed">lrun -n128 a.out</span></td>
<td>Launches 128 tasks evenly over 8 nodes</td>
</tr><tr><td><span class="fixed">lrun -T16 a.out</span></td>
<td>Launches 16 tasks one each of 8 nodes = 128 tasks</td>
</tr><tr><td><span class="fixed">lrun -1 make</span></td>
<td>Launches 1 make process on 1 node</td>
</tr></table><h3><a name="jsrun" id="jsrun"></a>Launching jobs: the jsrun Command and Resource Sets</h3>
<h4>jsrun Overview</h4>
<ul><li>The <span class="fixed">jsrun</span> command is the IBM provided parallel job launch command for Sierra systems.</li>
<li>Replaces <span class="fixed">srun</span> and <span class="fixed">mpirun</span> used on other LC systems:
<ul><li>Similar in function, but very different conceptually and in syntax.</li>
<li>Based upon an abstraction called <strong><em>resource sets.</em></strong></li>
</ul></li>
<li>Basic syntax (described in detail below):<br /><span class="fixed">jsrun  <em>[options]  [executable]</em></span></li>
<li>Developed by IBM for the LLNL and Oak Ridge CORAL systems:
<ul><li>Part of the IBM Job Step Manager (JSM) software package for managing a job allocation provided by the resource manager.</li>
<li>Integrated into the IBM Spectrum LSF Workload Manager.</li>
</ul></li>
<li>A discussion on which job launch command should be used can be found in the <a href="#quick12">Quickstart Guide section 12</a>.</li>
</ul><h4>Resource Sets</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1110" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesets01-png-1">resourceSets01.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/resourceSets01_1.png"><img alt="Sierra node " height="268" width="400" style="height: 268px; width: 400px;" class="media-element file-default" data-delta="100" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/resourceSets01_1-400x268.png" /></a>  </div>

  
</div>
</div></div>
<ul><li>A Sierra node consists of the following resources per node - see diagram at right:
<ul><li>40 cores; 20 per socket   <span class="note-green">Note</span> Two cores on each socket are reserved for the operating system, and are therefore not included.</li>
<li>160 hardware threads; 4 per core</li>
<li>4 GPUs; 2 per socket</li>
</ul></li>
<li>In the simplest sense, a resource set describes how a node's resources should look to a job.</li>
<li>A basic resource set definition consists of:
<ul><li>Number of tasks</li>
<li>Number of cores</li>
<li>Number of GPUs</li>
<li>Memory allocation</li>
</ul></li>
<li>Rules:
<ul><li>Described in terms of a single node's resources</li>
<li>Can span sockets on a node</li>
<li>Cannot span multiple nodes</li>
<li>Defaults are used if any resource is not explicitly specified.</li>
</ul></li>
<li>Example Resource Sets:</li>
</ul><table><tr><td><div class="media media-element-container media-wysiwyg"><div id="file-1112" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesetst4c4g1-png-0">resourceSetsT4C4G1.png</a></h2>
    
  
  <div class="content">
    <img alt="1 GPU " height="89" width="205" class="media-element file-wysiwyg" data-delta="100" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSetsT4C4G1_0.png" /></div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-1113" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesetst4c16g2-png-1">resourceSetsT4C16G2.png</a></h2>
    
  
  <div class="content">
    <img alt="Example resource set: 4 tasks, 16 cores, 2 GPUs" height="241" width="203" class="media-element file-default" data-delta="101" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSetsT4C16G2_1.png" /></div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-1114" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesetst16c16g4-png-1">resourceSetsT16C16G4.png</a></h2>
    
  
  <div class="content">
    <img alt="Example resource set: 16 tasks, 16 cores, 4 GPUs, 2 sockets" height="167" width="407" class="media-element file-default" data-delta="102" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSetsT16C16G4_1.png" /></div>

  
</div>
</div></td>
</tr><tr><td>4 tasks ♦ 4 cores ♦ 1 GPU<br />Fits on 1 socket</td>
<td>4 tasks ♦ 16 cores ♦ 2 GPUs<br />Fits on 1 socket</td>
<td>16 tasks ♦ 16 cores ♦ 4 GPUs<br />Requires both sockets</td>
</tr></table><ul><li>After defining the resource set, you need to define:
<ul><li>The number of Nodes required for the job</li>
<li>How many Resource Sets should be on each node</li>
<li>The total number of Resource Sets for the entire job</li>
</ul></li>
<li>These parameters are then provided to the <span class="fixed">jsrun</span> command as options/flags.</li>
<li>Examples with <span class="fixed">jsrun</span> options shown:</li>
</ul><table class="table table-bordered"><tr><td><div class="media media-element-container media-default"><div id="file-1112--2" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesetst4c4g1-png-0">resourceSetsT4C4G1.png</a></h2>
    
  
  <div class="content">
    <img alt="Example resource set: 4 tasks, 4 cores, 1 GPU" height="89" width="205" class="media-element file-default" data-delta="107" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSetsT4C4G1_0.png" /></div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-1116" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesets04-png-1">resourceSets04.png</a></h2>
    
  
  <div class="content">
    <img alt="Example of 2 nodes, 4 resource sets per node, 8 resource sets total" height="200" width="633" class="media-element file-default" data-delta="106" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSets04_1.png" /></div>

  
</div>
</div></td>
</tr><tr><td>Resource Set<br />4 tasks ♦ 4 Cores ♦ 1 GPU<br /><span class="fixed">-a4 -c4 -g1</span></td>
<td>2 nodes<br />4 resource sets per node  ♦  8 resource sets total<br /><span class="fixed">-r4 -n8</span></td>
</tr></table><table class="table table-bordered"><tr><td><div class="media media-element-container media-default"><div id="file-1113--2" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesetst4c16g2-png-1">resourceSetsT4C16G2.png</a></h2>
    
  
  <div class="content">
    <img alt="Example resource set: 4 tasks, 16 cores, 2 GPUs" height="241" width="203" class="media-element file-default" data-delta="108" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSetsT4C16G2_1.png" /></div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-1138" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesets05-png-0">resourceSets05.png</a></h2>
    
  
  <div class="content">
    <img alt="Diagram of resource set" height="200" width="633" class="media-element file-default" data-delta="109" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSets05_0.png" /></div>

  
</div>
</div></td>
</tr><tr><td>Resource Set<br />4 Tasks ♦  16 Cores ♦  2 GPUs<br /><span class="fixed">-a4 -c16 -g2</span></td>
<td>2 nodes<br />2 resource sets per node  ♦  4 resource sets total<br /><span class="fixed">-r4 -n4</span></td>
</tr></table><h4 class="clear-floats"><br />jsrun Options</h4>
<ul><li>The table below describes a number of commonly used jsrun options. See the jsrun man page for details.</li>
<li>Additionally, a very good, and detailed presentation on the jsrun command is available at: <a href="https://www.olcf.ornl.gov/wp-content/uploads/2018/02/SummitJobLaunch.pdf" target="_blank">https://www.olcf.ornl.gov/wp-content/uploads/2018/02/SummitJobLaunch.pdf</a>.</li>
</ul><table class="table table-striped table-bordered" summary="Commonly used jsrun options"><tr><th scope="col">Option (short)</th>
<th scope="col">Option (long)</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">-a</span></td>
<td><span class="fixed">--tasks_per_rs</span></td>
<td>Number of tasks per resource set</td>
</tr><tr><td><span class="fixed">-b</span></td>
<td><span class="fixed">--bind</span></td>
<td>Specifies the binding of tasks within a resource set. Can be <span class="fixed">none</span>, <span class="fixed">rs</span> (resource set), or <span class="fixed">packed:smt#</span>. See the jsrun man page for details.</td>
</tr><tr><td><span class="fixed">-c</span></td>
<td><span class="fixed">--cpu_per_rs</span></td>
<td>Number of CPUs (cores) per resource set.</td>
</tr><tr><td><span class="fixed">-d</span></td>
<td><span class="fixed">--launch_distribution</span></td>
<td>Specifies how task are started on resource sets. Options are cyclic, packed, plane:#. See the man page for details.</td>
</tr><tr><td><span class="fixed">-E<br />-F<br />-D</span></td>
<td><span class="fixed">--env var<br />--env_eval<br />--env_no_propagate</span></td>
<td>Specify how to handle environment variables. See the man page for details.</td>
</tr><tr><td><span class="fixed">-g</span></td>
<td><span class="fixed">--gpu_per_rs</span></td>
<td>Number of GPUs per resource set</td>
</tr><tr><td><span class="fixed">-l</span></td>
<td><span class="fixed">--latency priority</span></td>
<td>Latency Priority. Controls layout priorities. Can currently be cpu-cpu, gpu-cpu, gpu-gpu, memory-memory, cpu-memory or gpu-memory. See the man page for details.</td>
</tr><tr><td><span class="fixed">-n</span></td>
<td><span class="fixed">--nrs</span></td>
<td>Total number of resource sets for the job.</td>
</tr><tr><td><span class="fixed">-M "-gpu"</span></td>
<td><span class="fixed">--smpiargs "-gpu"</span></td>
<td>Turns on CUDA-aware Spectrum MPI</td>
</tr><tr><td><span class="fixed">-m</span></td>
<td><span class="fixed">--memory_per_rs</span></td>
<td>Specifies the number of megabytes of memory (1,048,756 bytes) to assign to a resource set. Use the -S option to view the memory setting.</td>
</tr><tr><td><span class="fixed">-p</span></td>
<td><span class="fixed">--np</span></td>
<td>Number of tasks to start. By default, each task is assigned its own resource set that contains a single CPU.</td>
</tr><tr><td><span class="fixed">-r</span></td>
<td><span class="fixed">--rs_per_host</span></td>
<td>Number of resource sets per host (node)</td>
</tr><tr><td><span class="fixed">-S <em>filename</em></span></td>
<td><span class="fixed">--save_resources</span></td>
<td>Specifies that the resources used for the job step are written to <em>filename</em>.</td>
</tr><tr><td><span class="fixed">-t<br />-o<br />-e<br />-k</span></td>
<td><span class="fixed">--stdio_input<br />--stdio_stdout<br />--stdio_mode<br />--stdio_stderr</span></td>
<td>Specifies how to handle stdio, stdout and stderr. See the man page for details.</td>
</tr><tr><td><span class="fixed">-V</span></td>
<td><span class="fixed">--version</span></td>
<td>Displays the version of jsrun Job Step Manager (JSM).</td>
</tr></table><ul><li>Examples:<br />These examples assume that 40 cores per node are available for user tasks (4 are reserved for the operating system), and each node has 4 GPUs.<br />White space between an option an its argument is optional.</li>
</ul><table class="table table-striped table-bordered" summary="Examples using the jsrun command"><tr><th scope="col">jsrun Command</th>
<th scope="col">Description</th>
<th scope="col">Diagram</th>
</tr><tr><td><span class="fixed">jsrun -p72 a.out</span></td>
<td>72 tasks, no GPUs<br />2 nodes, 40 tasks on node1, 32 tasks on node2</td>
<td><div class="media media-element-container media-default"><div id="file-1118" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesets06-png-0">resourceSets06.png</a></h2>
    
  
  <div class="content">
    <img alt="Diagram of resource set" height="200" width="633" class="media-element file-default" data-delta="108" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSets06_0.png" /></div>

  
</div>
</div></td>
</tr><tr><td><span class="fixed">jsrun -n8 -a1 -c1 -g1 a.out</span></td>
<td>8 resource sets, each with 1 task and 1 GPU<br />2 nodes, 2 tasks per socket</td>
<td><div class="media media-element-container media-default"><div id="file-1119" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesets07-png-0">resourceSets07.png</a></h2>
    
  
  <div class="content">
    <img alt="Diagram of resource set" height="200" width="633" class="media-element file-default" data-delta="109" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSets07_0.png" /></div>

  
</div>
</div></td>
</tr><tr><td><span class="fixed">jsrun -n8 -a1 -c4 -g1 -bpacked:4 a.out</span></td>
<td>8 resource sets each with 1 task with 4 threads (cores) and 1 GPU<br />2 nodes, 2 tasks per socket</td>
<td><div class="media media-element-container media-default"><div id="file-1120" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesets08-png-0">resourceSets08.png</a></h2>
    
  
  <div class="content">
    <img alt="Diagram of resource set" height="200" width="633" class="media-element file-default" data-delta="110" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSets08_0.png" /></div>

  
</div>
</div></td>
</tr><tr><td><span class="fixed">jsrun -n8 -a2 -c2 -g1 a.out</span></td>
<td>8 resource sets each with 2 tasks and 1 GPU<br />2 nodes, 4 tasks per socket</td>
<td><div class="media media-element-container media-default"><div id="file-1121" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesets09-png-0">resourceSets09.png</a></h2>
    
  
  <div class="content">
    <img alt="Diagram of resource set" height="200" width="633" class="media-element file-default" data-delta="111" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSets09_0.png" /></div>

  
</div>
</div></td>
</tr><tr><td><span class="fixed">jsrun -n4 -a1 -c1 -g2 a.out</span></td>
<td>4 resource sets each with 1 task and 2 GPUs<br />2 nodes: 1 task per socket</td>
<td><div class="media media-element-container media-default"><div id="file-1122" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/resourcesets10-png-0">resourceSets10.png</a></h2>
    
  
  <div class="content">
    <img alt="Diagram of resource set" height="200" width="633" class="media-element file-default" data-delta="112" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resourceSets10_0.png" /></div>

  
</div>
</div></td>
</tr></table><h3><a name="Dependencies" id="Dependencies"></a>Job Dependencies</h3>
<h4>#BSUB -w Option</h4>
<ul><li>As with other batch systems, LSF provides a way to place dependencies on jobs to prevent them from running until other jobs have started, completed, etc.</li>
<li>The <span class="fixed">#BSUB -w</span> option is used to accomplish this. The syntax is:<br /><br /><span class="fixed">#BSUB -w  dependency_expression</span></li>
</ul><ul><li>A dependency expression is a logical expression comprised of one or more dependency conditions. It can include relational operators such as:</li>
</ul><p><span class="fixed">    &amp;&amp; (AND)          || (OR)            ! (NOT)<br />    &gt;                 &gt;=                 &lt;<br />    &lt;=                ==                 !=</span></p>
<ul><li>Several dependency examples are shown in the table below:</li>
</ul><table class="table table-striped table-bordered" summary="Job dependency examples"><tr><th scope="col">Example</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">#BSUB -w started(22345)</span></td>
<td>Job will not start until job 22345 starts. Job 22345 is considered to have started if is in any of the following states: USUSP, SSUSP, DONE, EXIT or RUN (with any pre-execution command specified by bsub -E completed)</td>
</tr><tr><td><span class="fixed">#BSUB -w done(22345)<br />#BSUB -w 22345</span></td>
<td>Job will not start until job 22345 has a state of DONE (completed normally). If a job ID is given with no condition, done() is assumed.</td>
</tr><tr><td><span class="fixed">#BSUB -w exit(22345)</span></td>
<td>Job will not start until job 22345 has a state of EXIT (completed abnormally)</td>
</tr><tr><td><span class="fixed">#BSUB -w ended(22345)</span></td>
<td>Job will not start until job 22345 has a state of EXIT or DONE</td>
</tr><tr><td><span class="fixed">#BSUB -w done(22345) &amp;&amp; started(33445)</span></td>
<td>Job will not start until job 22345 has a state of DONE and job 33445 has started</td>
</tr></table><ul><li>Usage notes:
<ul><li>The <span class="fixed">-w</span> option can be used with the <span class="fixed">bsub</span> command, but it is extremely limited because parens and relational operators cannot be included with the command.</li>
<li>LSF requires that valid jobids be specified - can't use non-existent jobids.</li>
<li>To remove dependencies for a job, use the command:  <span class="fixed">bmod -wn  <em>jobid</em></span></li>
</ul></li>
</ul><h4>bjdepinfo Command</h4>
<ul><li>The bjdepinfo command can be used to view job dependency information. More useful than the <span class="fixed">bjobs -l</span> command.</li>
<li>See the bjdepinfo man page and/or the LSF Documentation for details.</li>
<li>Examples are shown below:</li>
</ul><pre>    % <span class="text-danger">bjdepinfo 30290</span>
    JOBID          PARENT         PARENT_STATUS  PARENT_NAME  LEVEL
    30290          30285          RUN            *mmat 500    1


    % <span class="text-danger">bjdepinfo -r3 30290</span>
    JOBID          PARENT         PARENT_STATUS  PARENT_NAME  LEVEL
    30290          30285          RUN            *mmat 500    1
    30285          30271          DONE           *mmat 500    2
    30271          30267          DONE           *mmat 500    3</pre><h3><a name="Monitor" id="Monitor"></a>Monitoring Jobs: lsfjobs, bjobs, bpeek, bhist commands</h3>
<p>LSF provides several commands for monitoring jobs. Additionally LC provides a locally developed command for monitoring jobs called <span class="fixed">lsfjobs</span>.</p>
<h4><a name="lsfjobs" id="lsfjobs"></a>lsfjobs</h4>
<ul><li>LC's lsfjobs command is useful for displaying a summary of queued and running jobs, along with a summary of each queue's usage.</li>
<li>Usage information -  use any of the commands:<span class="fixed"> lsfjobs -h, lsfjobs -help, lsfjobs -man</span></li>
<li>Various options are available for filtering output by user, group, jobid, queue, job state, completion time, etc.</li>
<li>Output can be easily customized and include additional fields of information. Job states are described - over 20 different states possible.</li>
<li>Example output below:</li>
</ul><pre class="left-indent">**********************************
* Host:    - lassen - lassen708  *
* Date:    - 08/26/2019 14:38:34 *
* Cmd:     - lsfjobs             *
**********************************

*********************************************************************************************************************************
* JOBID    SLOTS    PTILE    HOSTS    USER            STATE            PRIO        QUEUE        GROUP    REMAINING        LIMIT *
*********************************************************************************************************************************
  486957       80       40        2    liii3             RUN               -       pdebug      smt4lnn        04:00      2:00:00
  486509      640       40       16    joqqm             RUN               -      standby     hohlfoam        12:00      2:00:00
  487107     1600       40       40    mnss3             RUN               -      pbatch0      wbronze        17:00      1:00:00
  487176     1280       40       32    dirrr211          RUN               -      pbatch0     stanford        25:00      0:40:00
  486908       40       40        1    samuu4            RUN               -      pbatch3        dbalf     11:51:00     12:00:00
  ....
  486910       40       40        1    samuu4            RUN               -      pbatch3        dbalf     11:51:00     12:00:00
  487054       40       40        1    samuu4            RUN               -      pbatch3        dbalf     11:51:00     12:00:00
  -----------------------------------------------------------
  477171    10240       40      256    miss6666       TOOFEW         1413.00      pbatch0      cbronze            -     12:00:00
  -----------------------------------------------------------
  487173      160       40        4    land3211    SLOTLIMIT          600.50      pbatch2         vfib            -      2:00:00
  486770      320       40        8    tamgg4      SLOTLIMIT          200.80      pbatch3     nonadiab            -     12:00:00
  487222       40       40        1    samww2      SLOTLIMIT          200.50      pbatch3        dbalf            -     12:00:00
  -----------------------------------------------------------
  486171       40       40        1    munddd33       DEPEND          200.50      pbatch3      feedopt            -     12:00:00
  487013      640       40       16    joww2          DEPEND           40.50      standby     hohlfoam            -      2:00:00
  -----------------------------------------------------------
  394147      640       40       16    ecqq2344         HELD          401.20       pbatch     exalearn            -      9:00:00
  394162      640       40       16    ecqq2344         HELD          401.10       pbatch     exalearn            -      9:00:00

***************************************************************
* HOST_GROUP       TOTAL   DOWN    RSVD/BUSY   FREE   HOSTS   *
***************************************************************
   batch_hosts        752     15          737      0   lassen[37-680,720-827]
   debug_hosts         36      0           22     14   lassen[1-36]

*****************************************************************************************************
* QUEUE          TOTAL  DOWN   RSVD/BUSY   FREE   DEFAULTTIME      MAXTIME  STATE     HOST_GROUP(S) *
*****************************************************************************************************
   exempt           752    15         737      0          None    Unlimited  Active    batch_hosts
   expedite         752    15         737      0          None    Unlimited  Active    batch_hosts
   pall             788    15         759     14          None    Unlimited  Active    batch_hosts,debug_hosts
   pbatch           752    15         737      0         30:00     12:00:00  Active    batch_hosts
   pbatch0          752    15         737      0         30:00     12:00:00  Active    batch_hosts
   pbatch1          752    15         737      0         30:00     12:00:00  Active    batch_hosts
   pbatch2          752    15         737      0         30:00     12:00:00  Active    batch_hosts
   pbatch3          752    15         737      0         30:00     12:00:00  Active    batch_hosts
   pdebug            36     0          22     14         30:00      2:00:00  Active    debug_hosts
   standby          788    15         759     14          None    Unlimited  Active    batch_hosts,debug_hosts
</pre><h4><a name="bjobs" id="bjobs"></a>bjobs</h4>
<ul><li>Provides a number of options for displaying a range of job information - from summary to detailed.</li>
<li>The table below shows some of the more commonly used options.</li>
<li>See the bjobs man page and/or the LSF Documentation for details.</li>
</ul><table class="table table-striped table-bordered" summary="Commonly used bjobs options"><tr><th scope="col">Command</th>
<th scope="col">Description</th>
<th scope="col">Example</th>
</tr><tr><td><span class="fixed">bjobs</span></td>
<td>Show your currently queued and running jobs</td>
<td>
<p><button>Sample Output % bjobs</button></p>
</td>
</tr><tr><td><span class="fixed">bjobs -u all</span></td>
<td>Show queued and running jobs for all users</td>
<td>
<p><button>Sample Output % bjobs -u all</button></p>
</td>
</tr><tr><td><span class="fixed">bjobs -a</span></td>
<td>Show jobs in all states including recently completed</td>
<td>
<p><button>Sample Output % bjobs -a</button></p>
</td>
</tr><tr><td><span class="fixed">bjobs -d</span></td>
<td>
<p>Show only recently completed jobs</p>
</td>
<td><button>Sample Output % bjobs -d</button></td>
</tr><tr><td><span class="fixed">bjobs -l<br />bjobs -l 22334<br />bjobs -l -u all</span></td>
<td>Show long listing of detailed job information<br />Show long listing for job 22334<br />Show long listing for all user jobs</td>
<td>
<p><button>Sample Output % bjobs -l</button></p>
</td>
</tr><tr><td><span class="fixed">bjobs -o [format string]</span></td>
<td>Specifies options for customized format bjobs output. See the documentation for details.</td>
<td> </td>
</tr><tr><td><span class="fixed">bjobs -p<br />bjobs -p -u all</span></td>
<td>Show pending jobs and reason why<br />Show pending jobs for all users</td>
<td>
<p><button>Sample Output % bjobs -u all -p</button></p>
</td>
</tr><tr><td><span class="fixed">bjobs -r<br />bjobs -r -u all</span></td>
<td>Show running jobs<br />Show running jobs for all users</td>
<td>
<p><button>Sample Output % bjobs -r -u all</button></p>
</td>
</tr><tr><td><span class="fixed">bjobs -X</span></td>
<td>Show host names (uncondensed)</td>
<td>
<p><button>Sample Output % bjobs -X</button></p>
</td>
</tr></table><h4><a name="bpeek" id="bpeek"></a>bpeek</h4>
<ul><li>Allows you to view stdout/stderr of currently running jobs.</li>
<li>Provides several options for selecting jobs by queue, name, jobid.</li>
<li>See the bpeek man page and/or LSF documentation for details.</li>
<li>Examples below:</li>
</ul><table class="table table-striped table-bordered" summary="Examples of common bpeek options"><tr><th scope="col">Command</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">bpeek 27239</span></td>
<td>Show output from jobid 27239</td>
</tr><tr><td><span class="fixed">bpeek -J myjob</span></td>
<td>Show output for most recent job named "myjob"</td>
</tr><tr><td><span class="fixed">bpeek -f</span></td>
<td>Shows output of most recent job by looping with the command <span class="fixed">tail -f</span>. When the job is done, the bpeek command exits.</td>
</tr><tr><td><span class="fixed">bpeek -q</span></td>
<td>Displays output of the most recent job in the specified queue.</td>
</tr></table><h4><a name="bhist" id="bhist"></a>bhist</h4>
<ul><li>By default, displays information about your pending, running, and suspended jobs.</li>
<li>Also provides options for displaying information about recently completed jobs, and for filtering output by job name, queue, user, group, start-end times, and more.</li>
<li>See the bhist man page and/or LSF documentation for details.</li>
<li>Example below - shows running, queued and recently completed jobs:</li>
</ul><pre>% <span class="text-danger">bhist -a</span>
    Summary of time in seconds spent in various states:
    JOBID   USER    JOB_NAME  PEND    PSUSP   RUN     USUSP   SSUSP   UNKWN   TOTAL
    27227   user22  run.245   2       0       204     0       0       0       206
    27228   user22  run.247   2       0       294     0       0       0       296
    27239   user22  runtest   4       0       344     0       0       0       348
    27240   user22  run.248   2       0       314     0       0       0       316
    27241   user22  runtest   1       0       313     0       0       0       314
    27243   user22  run.249   13      0       1532    0       0       0       1545
    27244   user22  run.255   0       0       186     0       0       0       186
    27245   user22  run.267   1       0       15      0       0       0       16
    27246   user22  run.288   2       0       12      0       0       0       14</pre><h4>Job States</h4>
<ul><li>LSF job monitoring commands display a job's state. The most commonly seen ones are shown in the table below.</li>
</ul><table class="table table-striped table-bordered" summary="Common job states displayed by LSF job monitoring commands"><tr><th scope="col">State</th>
<th scope="col">Description</th>
</tr><tr><td>DONE</td>
<td>Job completed normally</td>
</tr><tr><td>EXIT</td>
<td>Job completed abnormally</td>
</tr><tr><td>PEND</td>
<td>Job is pending, queued</td>
</tr><tr><td>PSUSP</td>
<td>Job was suspended (either by the user or an administrator) while pending</td>
</tr><tr><td>RUN</td>
<td>Job is running</td>
</tr><tr><td>SSUSP</td>
<td>Job was suspended by the system after starting</td>
</tr><tr><td>USUSP</td>
<td>Job was suspended (either by the user or an administrator) after starting</td>
</tr></table><h3><strong><a name="Suspend" id="Suspend"></a></strong>Suspending / Resuming Jobs: bstop, bresume commands</h3>
<h4>bstop and bresume Commands</h4>
<ul><li>LSF provides support for user-level suspension and resumption of running and queued jobs.</li>
<li>However, at LC, the <span class="fixed">bstop</span> command is used to suspend queued jobs only. <span class="note-green">Note:</span>this is different from the LSF default behavior and documentation, which allows suspension of running jobs.</li>
<li>Queued jobs that have been suspended will show a <strong>PSUSP</strong> state</li>
<li>The <span class="fixed">bresume</span> command is used to resume suspended jobs.</li>
<li>Jobs can be specified by jobid, host, job name, group, queue and other criteria. In the examples below, jobid is used.</li>
<li>See the bstop man page, bresume man page and/or LSF documentation for details.</li>
<li>Examples below:</li>
</ul><p><strong>Suspend a queued job, and then resume</strong></p>
<pre>    % <span class="text-danger">bjobs</span>
    JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
    31411   user22  PEND  pdebug     sierra4360              bmbtest    Apr 13 12:11

    % <span class="text-danger">bstop 31411</span>
    Job &lt;31411&gt; is being stopped

    % <span class="text-danger">bjobs</span>
    JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
    31411   user22  PSUSP pdebug     sierra4360              bmbtest    Apr 13 12:11

    % <span class="text-danger">bresume 31411</span>
    Job &lt;31411&gt; is being resumed

    % <span class="text-danger">bjobs</span>
    bjobs
    JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
    31411   user22  RUN   pdebug     sierra4360  1*launch_ho bmbtest    Apr 13 12:11
                                                 400*debug_hosts</pre><h3><a name="Modify" id="Modify"></a>Modifying Jobs: bmod command</h3>
<h4>bmod Command</h4>
<ul><li>The <span class="fixed">bmod</span> command is used to modify the options of a previously submitted job.</li>
<li>Simply use the desired bsub option with <span class="fixed">bmod</span>, providing a new value. For example, to modify the wallclock time for jobid 22345:</li>
</ul><p><span class="fixed">   bmod -W 500 22345&gt;</span></p>
<ul><li>You can modify all options for a pending job, even if the corresponding bsub command option was not specified. This comes in handy in case you forgot an option when the job was originally submitted.</li>
<li>You can also "reset" options to their original or default values by appending a lowercase n to the desired option (no whitespace). For example to reset the queue to the original submission value:<br /><span class="fixed">bmod -qn 22345</span></li>
<li>For running jobs, there are very few, if any, useful options that can be changed.</li>
<li>See the bmod man page and/or LSF documentation for details.</li>
<li>The <span class="fixed">bhist -l</span> command can be used to view a history of which job parameters have been changed - they appear near the end of the output. For example:</li>
</ul><pre>% <span class="text-danger">bhist -l 31788</span>

    ...[previous output omitted]

    Fri Apr 13 14:10:20: Parameters of Job are changed:
        Output file change to : /g/g0/user22/lsf/
        User group changes to: guests
        run limit changes to : 55.0 minutes;
    Fri Apr 13 14:13:40: Parameters of Job are changed:
        Job queue changes to : pbatch
        Output file change to : /g/g0/user22/lsf/
        User group changes to: guests;
    Fri Apr 13 14:30:08: Parameters of Job are changed:
        Job queue changes to : standby
        Output file change to : /g/g0/user22/lsf/
        User group changes to: guests;

    ...[following output omitted]</pre><p> </p>
<h3><a name="Signal" id="Signal"></a>Signaling / Killing Jobs: bkill command</h3>
<h4>bkill Command</h4>
<ul><li>The <span class="fixed">bkill</span> command is used to both terminate jobs and to send signals to jobs.</li>
<li>Similar to the kill command found in Unix/Linux operating systems - can be used to send various signals (not just SIGTERM and SIGKILL) to jobs.</li>
<li>Can accept both numbers and names for signals.</li>
<li>In additional to jobid, jobs can be identified by queue, host, group, job name, user, and more.</li>
<li>For a list of accepted signal names, run <span class="fixed">bkill -l</span></li>
<li>See the bkill man page and/or LSF documentation for details.<br />For general details on Linux signals see <a href="http://man7.org/linux/man-pages/man7/signal.7.html" target="_blank">http://man7.org/linux/man-pages/man7/signal.7.html</a>.</li>
<li>Examples:</li>
</ul><table class="table table-bordered table-striped" summary="Examples of bkill command"><tr><th scope="col">Command</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">bkill 22345<br />bkill 34455 24455</span></td>
<td>Force a job(s) to stop by sending SIGINT, SIGTERM, and SIGKILL. These signals are sent in that order, so users can write applications such that they will trap SIGINT and/or SIGTERM and exit in a controlled manner.</td>
</tr><tr><td><span class="fixed">bkill -s HUP 22345</span></td>
<td>Send SIGHUP to job 22345. <span class="note-green">Note</span> When specifying a signal by name, omit SIG from the name.</td>
</tr><tr><td><span class="fixed">bkill -s 9 22345</span></td>
<td>Send signal 9 to job 22345</td>
</tr><tr><td><span class="fixed">bkill -s STOP -q pdebug</span></td>
<td>Send a SIGSTOP signal to the most recent job in the pdebug queue</td>
</tr></table><h3><a name="CUDA-aware" id="CUDA-aware"></a>CUDA-aware MPI</h3>
<ul><li>CUDA-aware MPI allows GPU buffers (allocated with cudaMalloc) to be used directly in MPI calls. Without CUDA-Aware MPI data must be copied manually to/from a CPU buffer (using cudaMemcpy) before/after passing data in MPI calls. For example:</li>
</ul><table class="table table-bordered"><tr><th>
<p>Without CUDA-aware MPI - need to copy data between GPU and CPU memory before/after MPI send/receive operations.</p>
</th>
<th>
<p>With CUDA-aware MPI - data is transferred directly to/from GPU memory by MPI send/receive operations.</p>
</th>
</tr><tr><td>
<pre>//MPI rank 0
cudaMemcpy(sendbuf_h,sendbuf_d,size,cudaMemcpyDeviceToHost);
MPI_Send(sendbuf_h,size,MPI_CHAR,1,tag,MPI_COMM_WORLD);

//MPI rank 1
MPI_Recv(recbuf_h,size,MPI_CHAR,0,tag,MPI_COMM_WORLD, &amp;status);
cudaMemcpy(recbuf_d,recbuf_h,size,cudaMemcpyHostToDevice);

</pre></td>
<td>
<pre>//MPI rank 0
MPI_Send(sendbuf_d,size,MPI_CHAR,1,tag,MPI_COMM_WORLD);

//MPI rank 1
MPI_Recv(recbuf_d,size,MPI_CHAR,0,tag,MPI_COMM_WORLD, &amp;status);</pre></td>
</tr></table><ul><li>IBM Spectrum MPI on CORAL systems is CUDA-aware. However, users are required to "turn on" this feature using a run-time flag with lrun or jsrun. For example:<br /><span class="fixed">lrun -M "-gpu"<br />jsrun -M "-gpu"</span></li>
<li><span class="note-green">Caveat:</span> Do NOT use the MPIX_Query_cuda_support() routine or the preprocessor constant MPIX_CUDA_AWARE_SUPPORT to determine if Spectrum MPI is CUDA-aware.  This routine has either been removed from the IBM implementation, or will always return false (older versions).  </li>
<li>Additional Information:
<ul><li>An Introduction to CUDA-Aware MPI:  <a href="https://devblogs.nvidia.com/introduction-cuda-aware-mpi/" target="_blank">https://devblogs.nvidia.com/introduction-cuda-aware-mpi/</a></li>
<li>MPI Status Updates and Performance Suggestions: <a href="https://lc.llnl.gov/confluence/display/SIERRA/Sierra+Systems+User+Meeting?preview=/633381124/640520291/2019.05.09.MPI_UpdatesPerformance.Karlin.pdf" target="_blank"> 2019.05.09.MPI_UpdatesPerformance.Karlin.pdf</a></li>
</ul></li>
</ul><h3><a name="Binding" id="Binding"></a>Process, Thread and GPU Binding: js_task_info</h3>
<ul><li>Application performance can be <em><strong>significantly impacted</strong></em> by the way MPI tasks and OpenMP threads are bound to cores and GPUs.</li>
<li><span class="note-red">Important:</span> The binding behaviors of <span class="fixed">lrun</span> and <span class="fixed">jsrun</span> are very different, and not obvious to users.  The jsrun command in particular often requires careful consideration in order to obtain optimal bindings.</li>
<li>The <span class="fixed">js_task_info</span> utility provides an easy way to see exactly how tasks and threads are being bound. Simply run <span class="fixed">js_task_info</span> with lrun or jsrun as you would your application.</li>
<li>The <span class="fixed">lrun -v</span> flag shows the actual jsrun command that is used "under the hood".  The <span class="fixed">-vvv</span> flag can be used with both lrun and jsrun to see additional details, including environment variables.</li>
<li>Several examples, using 1 node, are shown below. Note that each thread on an SMT4 core counts as a "cpu"  (4*44 cores = 176 cpus) in the output, and that the first 8 "cpus" [0-7] are reserved for core isolation.</li>
</ul><pre>% <span class="text-danger">lrun -n4 js_task_info</span>
Task 0 ( 0/4, 0/4 ) is bound to cpu[s] 8,12,16,20,24,28,32,36,40,44 on host lassen2 with OMP_NUM_THREADS=10 and with OMP_PLACES={8},{12},{16},{20},{24},{28},{32},{36},{40},{44} and CUDA_VISIBLE_DEVICES=0
Task 1 ( 1/4, 1/4 ) is bound to cpu[s] 48,52,56,60,64,68,72,76,80,84 on host lassen2 with OMP_NUM_THREADS=10 and with OMP_PLACES={48},{52},{56},{60},{64},{68},{72},{76},{80},{84} and CUDA_VISIBLE_DEVICES=1
Task 3 ( 3/4, 3/4 ) is bound to cpu[s] 136,140,144,148,152,156,160,164,168,172 on host lassen2 with OMP_NUM_THREADS=10 and with OMP_PLACES={136},{140},{144},{148},{152},{156},{160},{164},{168},{172} and CUDA_VISIBLE_DEVICES=3
Task 2 ( 2/4, 2/4 ) is bound to cpu[s] 96,100,104,108,112,116,120,124,128,132 on host lassen2 with OMP_NUM_THREADS=10 and with OMP_PLACES={96},{100},{104},{108},{112},{116},{120},{124},{128},{132} and CUDA_VISIBLE_DEVICES=2

% <span class="text-danger">lrun -n4 --smt=4 -v js_task_info</span>
+ export MPIBIND+=.smt=4
+ exec /usr/tce/packages/jsrun/jsrun-2019.05.02/bin/jsrun --np 4 --nrs 1 -c ALL_CPUS -g ALL_GPUS -d plane:4 -b none -X 1 /usr/tce/packages/lrun/lrun-2019.05.07/bin/mpibind10 js_task_info
Task 0 ( 0/4, 0/4 ) is bound to cpu[s] 8-47 on host lassen2 with OMP_NUM_THREADS=40 and with OMP_PLACES={8},{9},{10},{11},{12},{13},{14},{15},{16},{17},{18},{19},{20},{21},{22},{23},{24},{25},{26},{27},{28},{29},{30},{31},{32},{33},{34},{35},{36},{37},{38},{39},{40},{41},{42},{43},{44},{45},{46},{47} and CUDA_VISIBLE_DEVICES=0
Task 1 ( 1/4, 1/4 ) is bound to cpu[s] 48-87 on host lassen2 with OMP_NUM_THREADS=40 and with OMP_PLACES={48},{49},{50},{51},{52},{53},{54},{55},{56},{57},{58},{59},{60},{61},{62},{63},{64},{65},{66},{67},{68},{69},{70},{71},{72},{73},{74},{75},{76},{77},{78},{79},{80},{81},{82},{83},{84},{85},{86},{87} and CUDA_VISIBLE_DEVICES=1
Task 2 ( 2/4, 2/4 ) is bound to cpu[s] 96-135 on host lassen2 with OMP_NUM_THREADS=40 and with OMP_PLACES={96},{97},{98},{99},{100},{101},{102},{103},{104},{105},{106},{107},{108},{109},{110},{111},{112},{113},{114},{115},{116},{117},{118},{119},{120},{121},{122},{123},{124},{125},{126},{127},{128},{129},{130},{131},{132},{133},{134},{135} and CUDA_VISIBLE_DEVICES=2
Task 3 ( 3/4, 3/4 ) is bound to cpu[s] 136-175 on host lassen2 with OMP_NUM_THREADS=40 and with OMP_PLACES={136},{137},{138},{139},{140},{141},{142},{143},{144},{145},{146},{147},{148},{149},{150},{151},{152},{153},{154},{155},{156},{157},{158},{159},{160},{161},{162},{163},{164},{165},{166},{167},{168},{169},{170},{171},{172},{173},{174},{175} and CUDA_VISIBLE_DEVICES=3

% <span class="text-danger">jsrun -p4 js_task_info</span>
Task 0 ( 0/4, 0/4 ) is bound to cpu[s] 8-11 on host lassen2 with OMP_NUM_THREADS=4 and with OMP_PLACES={8:4}
Task 1 ( 1/4, 1/4 ) is bound to cpu[s] 12-15 on host lassen2 with OMP_NUM_THREADS=4 and with OMP_PLACES={12:4}
Task 2 ( 2/4, 2/4 ) is bound to cpu[s] 16-19 on host lassen2 with OMP_NUM_THREADS=4 and with OMP_PLACES={16:4}
Task 3 ( 3/4, 3/4 ) is bound to cpu[s] 20-23 on host lassen2 with OMP_NUM_THREADS=4 and with OMP_PLACES={20:4}

% <span class="text-danger">jsrun -r4 -c10 -a1 -g1 js_task_info</span>
Task 0 ( 0/4, 0/4 ) is bound to cpu[s] 8-11 on host lassen2 with OMP_NUM_THREADS=4 and with OMP_PLACES={8:4} and CUDA_VISIBLE_DEVICES=0
Task 1 ( 1/4, 1/4 ) is bound to cpu[s] 48-51 on host lassen2 with OMP_NUM_THREADS=4 and with OMP_PLACES={48:4} and CUDA_VISIBLE_DEVICES=1
Task 2 ( 2/4, 2/4 ) is bound to cpu[s] 96-99 on host lassen2 with OMP_NUM_THREADS=4 and with OMP_PLACES={96:4} and CUDA_VISIBLE_DEVICES=2
Task 3 ( 3/4, 3/4 ) is bound to cpu[s] 136-139 on host lassen2 with OMP_NUM_THREADS=4 and with OMP_PLACES={136:4} and CUDA_VISIBLE_DEVICES=3

% <span class="text-danger">jsrun -r4 -c10 -a1 -g1 -b rs js_task_info</span>
Task 0 ( 0/4, 0/4 ) is bound to cpu[s] 8-47 on host lassen2 with OMP_NUM_THREADS=40 and with OMP_PLACES={8:4},{12:4},{16:4},{20:4},{24:4},{28:4},{32:4},{36:4},{40:4},{44:4} and CUDA_VISIBLE_DEVICES=0
Task 1 ( 1/4, 1/4 ) is bound to cpu[s] 48-87 on host lassen2 with OMP_NUM_THREADS=40 and with OMP_PLACES={48:4},{52:4},{56:4},{60:4},{64:4},{68:4},{72:4},{76:4},{80:4},{84:4} and CUDA_VISIBLE_DEVICES=1
Task 2 ( 2/4, 2/4 ) is bound to cpu[s] 96-135 on host lassen2 with OMP_NUM_THREADS=40 and with OMP_PLACES={96:4},{100:4},{104:4},{108:4},{112:4},{116:4},{120:4},{124:4},{128:4},{132:4} and CUDA_VISIBLE_DEVICES=2
Task 3 ( 3/4, 3/4 ) is bound to cpu[s] 136-175 on host lassen2 with OMP_NUM_THREADS=40 and with OMP_PLACES={136:4},{140:4},{144:4},{148:4},{152:4},{156:4},{160:4},{164:4},{168:4},{172:4} and CUDA_VISIBLE_DEVICES=3</pre><h3><a name="Diagnostics" id="Diagnostics"></a>Node Diagnostics: check_sierra_nodes</h3>
<ul><li>This LC utility allows you to check for bad nodes within your allocation before launching your actual job. For example:</li>
</ul><pre><span>sierra4368% check_sierra_nodes
STARTED: 'jsrun -r 1 -g 4 test_sierra_node -mpi -q' at Thu Aug 23 15:48:14 PDT 2018
SUCCESS: Returned 0 (all, including MPI, tests passed) at Thu Aug 23 15:48:22 PDT 2018</span></pre><ul><li>The last line will start with SUCCESS if no bad nodes were found and the return code will be 0.</li>
<li>Failure messages should be reported to the LC Hotline.</li>
<li><span class="note-green">Note:</span> this diagnostic and other detailed "health checks" are run after every batch allocation, so routine use of this test has been deprecated. For additional details, see the <a href="#quick19">discussion in the Quickstart Guide</a>. </li>
</ul><h3><a name="BurstBuffer" id="BurstBuffer"></a>Burst Buffer Usage</h3>
<h4>Overview</h4>
<ul><li>A burst buffer is a fast and intermediate storage layer positioned between the front-end computing processes and the back-end storage systems. </li>
<li>The goal of a burst buffer is to improve application I/O performance and reduce pressure on the parallel file system.</li>
<li>Example use: applications that write checkpoints; faster than writing to disk; computation can resume more quickly while burst buffer data is asynchronously moved to disk.</li>
<li>For Sierra systems, and the Ray Early Access system, the burst buffer is implemented as a 1.6 TB SSD (Solid State Drive) storage device local to each compute node. This drive takes advantage of NVMe over fabrics technologies, which allows remote access to the data without causing interference to an application running on the compute node itself.</li>
<li>Sierra's burst buffer hardware is covered in the <a href="#NVMe">NVMe PCIe SSD (Burst Buffer)</a> section of this tutorial.</li>
<li>The node-local burst buffer space on sierra, lassen and rzansel compute nodes is managed by the LSF scheduler:
<ul><li>Users may request a portion of this space for use by a job.</li>
<li>Once a job is running, the burst buffer space appears as a file system mounted under <strong>$BBPATH</strong>.</li>
<li>Users can then access $BBPATH as any other mounted file system.</li>
<li>Users may also stage-in and stage-out files to/from burst buffer storage.</li>
<li>In addition, a shared-namespace filesystem (called BSCFS) can be spun up across the disparate storage devices. This allows users to write a shared file across the node-local storage devices.</li>
</ul></li>
<li>On the ray Early Access system, the node-local SSD is simply mounted as <span class="fixed">/l/nvme</span> on the compute nodes, and is not managed by LSF.  It can be used as any other node-local file system for working with files. Additional information for using the burst buffer on ray can be found at: <a href="https://lc.llnl.gov/confluence/display/CORALEA/Ray+Burst+Buffers+and+dbcast" target="_blank">https://lc.llnl.gov/confluence/display/CORALEA/Ray+Burst+Buffers+and+dbcast</a> (internal wiki).</li>
</ul><h4>Requesting Burst Buffer Storage For a Job</h4>
<ul><li>Applies to sierra, lassen and rzansel, not ray</li>
<li>Simply add the <span class="fixed">-stage storage=<em>#gigabytes</em></span><span> flag to your <span class="fixed">bsub</span> or <span class="fixed">lalloc</span> command. Some examples are shown below:<br /><br /><span class="fixed">bsub -nnodes 4 -stage storage=64 -Is bash         </span>Requests 4 nodes with 64 GB storage each, interactive bash shell<br /><span class="fixed">lalloc 4 -stage storage=64                        </span>Equivalent using lalloc<br /><span class="fixed">bsub -stage storage=64  &lt; jobscript               </span>Requests 64 GB storage per node using a batch script</span><br /> </li>
<li>For LSF batch scripts, you can use the <span class="fixed">#BSUB -stage storage=64</span> syntax in your script instead of on the bsub command line.</li>
<li>Allocating burst buffer space typically requires additional time for bsub/lalloc.</li>
<li><span class="note-green">Note:</span> As of Sep 2019, the maximum amount of storage that can be requested is 1200 GB (subject to change).  Requesting more than this will cause jobs to hang in the queue. In the future, LC plans to implement immediate rejection of a job if it requests storage above the limit.</li>
</ul><h4>Using the Burst Buffer Storage Space</h4>
<ul><li>Applies to sierra, lassen, rzansel, not ray</li>
<li>Once LSF has allocated the nodes for your job, the node-local storage space can be accessed as any other mounted file system.  </li>
<li>For convenience, the path to your node-local storage is set as the <strong>$BBPATH</strong> environment variable.</li>
<li>You can <span class="fixed">cd, cp, ls, rm, mv, vi,</span> etc. files in $BBPATH as normal for other file systems. </li>
<li>Your programs can conduct I/O to files in $BBPATH as well.</li>
<li>Example:<br /><pre>% <span class="text-danger">lalloc 1 -qpdebug -stage storage=64</span>
+ exec bsub -nnodes 1 -qpdebug -stage storage=64 -Is -XF -W 60 -core_isolation 2 /usr/tce/packages/lalloc/lalloc-2.0/bin/lexec
Job &lt;517170&gt; is submitted to queue &lt;pdebug&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on lassen710&gt;&gt;
&lt;&lt;Waiting for JSM to become ready ...&gt;&gt;
&lt;&lt;Redirecting to compute node lassen21, setting up as private launch node&gt;&gt;
% <span class="text-danger">echo $BBPATH</span>
/mnt/bb_1d2e8a9f19a8c5dedd3dd9a373b70cc9
% <span class="text-danger">df -h $BBPATH</span>
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/bb-bb_35   64G  516M   64G   1% /mnt/bb_1d2e8a9f19a8c5dedd3dd9a373b70cc9
% <span class="text-danger">touch $BBPATH/testfile</span>
% <span class="text-danger">cd $BBPATH</span>
% <span class="text-danger">pwd</span>
/mnt/bb_1d2e8a9f19a8c5dedd3dd9a373b70cc9
% <span class="text-danger">ls -l</span>
total 0
-rw------- 1 user22 user22 0 Sep  6 15:00 testfile</pre></li>
</ul><ul><li>For parallel jobs, each task sees the burst buffer mounted as $BBPATH local to its node.  A simple parallel usage example using 1 task on each of 2 nodes is shown below.
<pre>% <span class="text-danger">cat testscript</span>
#!/bin/tcsh
setenv myrank $OMPI_COMM_WORLD_RANK
setenv node `hostname`
echo "Rank $myrank using burst buffer $BBPATH on $node"
echo "Rank $myrank copying input file to burst buffer"
cp $cwd/input.$myrank  $BBPATH/
echo "Rank $myrank doing work..."
cat $BBPATH/input.$myrank &gt; $BBPATH/output.$myrank
echo -n "Rank $myrank burst buffer shows: "
ls -l $BBPATH
echo "Rank $myrank copying output file to GPFS"
cp $BBPATH/output.$myrank /p/gpfs1/$USER/output/
echo "Rank $myrank done."

% <span class="text-danger">lrun -n2 testscript</span>
Rank 0 using burst buffer /mnt/bb_811dfc9bc5a6896a2cbea4f5f8087212 on rzansel3
Rank 0 copying input file to burst buffer
Rank 0 doing work...
Rank 0 burst buffer shows: total 128
-rw------- 1 user22 user22 170 Sep 10 12:49 input.0
-rw------- 1 user22 user22 170 Sep 10 12:49 output.0
Rank 0 copying output file to GPFS
Rank 0 done.
Rank 1 using burst buffer /mnt/bb_811dfc9bc5a6896a2cbea4f5f8087212 on rzansel5
Rank 1 copying input file to burst buffer
Rank 1 doing work...
Rank 1 burst buffer shows: total 128
-rw------- 1 user22 user22 76 Sep 10 12:49 input.1
-rw------- 1 user22 user22 76 Sep 10 12:49 output.1
Rank 1 copying output file to GPFS
Rank 1 done.

% <span class="text-danger">ls -l /p/gpfs1/user22/output</span>
total 2
-rw------- 1 user22 user22 170 Sep  6 15:53 output.0
-rw------- 1 user22 user22  76 Sep  6 15:53 output.1
</pre></li>
</ul><h4>Staging Data to/from Burst Buffer Storage</h4>
<ul><li>LSF can automatically move a job's data files in-to and out-of the node-local storage devices. This is achieved through the integration of LSF with IBM's burst buffer software.  The two options are:
<ul><li><span class="fixed"><strong>bbcmd </strong></span>command line tool, typically employed in user scripts.</li>
<li><span class="fixed"><strong>BBAPI</strong></span> C-library API consisting of subroutines called from user source code.</li>
</ul></li>
<li>There are 4 possible "phases" of data movement relating to a single job allocation:
<ol><li>Stage-in or pre-stage of data: Before an application begins on the compute resources, files are moved from the parallel file system into the burst buffer. The file movement is triggered by a user script with bbcmd commands which has been registered with LSF.</li>
<li>Data movement during the compute allocation: While the application is running, asynchronous data movement can take place between the burst buffer and parallel file system. This movement can be initiated via the C-library routines or via the command line tool.</li>
<li>Stage-out or post-stage of data: After the application has completed using the compute resources (but before the burst buffer has been de-allocated), files are moved from the burst buffer to the parallel file system. The file movement is triggered by a user script with bbcmd commands which has been registered with LSF.</li>
<li>Post-stage finalization: After the stage-out of files has completed, a user script may be called. This allows users to perform book-keeping actions after the data-movement portion of their job has completed. This is done through a user supplied script which is registered with LSF.</li>
</ol></li>
<li>Example workflow using the bbcmd interface:
<ul><li>Create a stage-in script with bbcmd commands for moving data from the parallel file system to the burst buffer. Make it executable. Also create a corresponding text file that lists the files to be transferred.</li>
<li>Create stage-out script with bbcmd commands for moving data from the burst buffer to the parallel file system. Make it executable. Also create a corresponding text file that lists the files to be transferred.</li>
<li>Create a post-stage script and make it executable.</li>
<li>Create an LSF job script as usual</li>
<li>Register your stage-in/stage-out scripts with LSF:  This is done by submitting your LSF job script with <span class="fixed">bsub</span> using the <span class="fixed">-stage <em>&lt;sub-arguments&gt;</em></span> flag.  The sub-arguments are separated by colons, and can include:
<ul><li><span class="fixed">storage=<em>#gigabytes</em></span></li>
<li><span class="fixed">in=<em>path-to-stage-in-script</em></span></li>
<li><span class="fixed">out=<em>path-to-stage-out-script1, path-to-stage-out-script2</em></span></li>
</ul></li>
<li>Alternatively, you can specify the <span class="fixed">-stage <em>&lt;sub-arguments&gt;</em> </span>flag in your LSF job script using the <span class="fixed">#BSUB</span> syntax.</li>
<li>Example: requests 256 GB of storage; stage-in.sh is the user stage-in script, stage-out1.sh is the user stage-out script, stage-out2.sh is the user post-stage finalization script. <br /><br /><span class="fixed">bsub -stage "<span class="text-danger">storage=</span>256:<span class="text-danger">in=</span>/p/gpfs1/user22/stage-in.sh:<span class="text-danger">out=</span>/p/gpfs1/user22/stage-out1.sh<span class="text-danger">,</span>/p/gpfs1/user22/stage-out2.sh"</span><br /> </li>
<li>Notes for stage-out, post-stage scripts: The <span class="fixed">out=</span><em><span class="fixed">path-to-stage-out-script1,path-to-stage-out-script2</span></em> option specifies 2 separate user-created stage-out scripts separated by a comma. The first script is run after the compute allocation has completed, but while the data on the burst buffer may still be accessed. The second script is run after the burst buffer has been de-allocated. If a stage-out1 script is not needed, the argument syntax would be <span class="fixed">out=,<em>path-to-stage-out-script2</em></span>. The full path to the scripts should be specified and the scripts must be marked as executable.</li>
</ul></li>
<li>Stage-in / stage-out scripts and file lists:  examples coming soon</li>
</ul><h4>BBAPI C-library API</h4>
<ul><li>This IBM provided, C-library API provides routines for using Sierra systems burst buffers.</li>
<li>Requires modification of source code.</li>
<li>More information can be found at: <a href="https://lc.llnl.gov/confluence/display/SIERRA/API+Documentation" target="_blank">https://lc.llnl.gov/confluence/display/SIERRA/API+Documentation</a> (internal wiki)</li>
</ul><h4><strong>BSCFS:</strong></h4>
<ul><li>This IBM provided, C-library API enables an application to write a single, shared, non-overlapping file using the node local burst buffers as cache.</li>
<li>Requires modification of source code.</li>
<li>More information can be found at: <a href="https://lc.llnl.gov/confluence/display/SIERRA/API+Documentation" target="_blank">https://lc.llnl.gov/confluence/display/SIERRA/API+Documentation</a> (internal wiki)</li>
</ul><h2><a name="BanksJobUsage" id="BanksJobUsage"></a>Banks, Job Usage and Job History Information</h2>
<p>Several commands are available for users to query their banks, job usage and job history information.  These are described below.<br />Additional, general information about allocations and banks can be found at:</p>
<ul><li><a href="https://computing.llnl.gov/tutorials/moab/#Banks" target="_blank">Banks</a> and <a href="https://computing.llnl.gov/tutorials/moab/#FairShare" target="_blank">Fair Share Job Scheduling</a> sections of the Moab and Slurm tutorial</li>
<li><a href="#Accounts" target="_blank">Accounts, Allocations and Banks</a> section of this tutorial</li>
</ul><h3>lshare</h3>
<ul><li>This is the most useful command for obtaining bank allocation and usage information on sierra and lassen where real banks are implemented.</li>
<li>Not currently used on rzansel, rzmanta, ray or shark where "guests" is shared by all users.</li>
<li>Provides detailed bank allocation and usage information for the entire bank hierarchy (tree) down to the individual user level.</li>
<li>LC developed wrapper command.</li>
<li>For usage information simply enter <span class="fixed">lshare -h</span></li>
<li>Example output below:</li>
</ul><pre>% <span class="text-danger">lshare -T cmetal</span>
Name                 Shares   Norm Usage      Norm FS
   cmetal              3200        0.003        0.022
    cbronze            2200        0.003        0.022
    cgold               700        0.000        0.022
    csilver             300        0.000        0.022

% <span class="text-danger">lshare -v -t cmetal</span>
Name                 Shares  Norm Shares        Usage   Norm Usage      Norm FS       Priority Type
   cmetal              3200        0.003      14243.0        0.003        0.022      81055.602 Bank
    cbronze            2200        0.002      14243.0        0.003        0.022      55725.727 Bank
      bbeedd11            1        0.000          0.0        0.000        0.022        100.000 User
      bvveer32            1        0.000          0.0        0.000        0.022        100.000 User
      ...
      sbbnrrrt            1        0.000          0.0        0.000        0.022        100.000 User
      shewwqq             1        0.000          0.0        0.000        0.022        100.000 User
      turrrr93            1        0.000          0.0        0.000        0.022        100.000 User
    cgold               700        0.001          0.0        0.000        0.022      70000.000 Bank
    csilver             300        0.000          0.0        0.000        0.022      30000.000 Bank</pre><h3>lsfjobs</h3>
<ul><li>The LC developed <span class="fixed">lsfjobs</span> command provides several options for showing job history:
<ul><li><span class="fixed">-c</span> shows job history for the past 1 day</li>
<li><span class="fixed">-d</span> shows job history for the specified number of days; must be used with the -c option</li>
<li><span class="fixed">-C</span> shows completed jobs within a specified time range</li>
</ul></li>
<li>Usage information -  use any of the commands: <span class="fixed">lsfjobs -h, lsfjobs -help, lsfjobs -man</span></li>
<li>Example below:</li>
</ul><pre>% <span class="text-danger">lsfjobs -c -d 7</span>

                                      -- STARTING:2019/08/22 13:40                   ENDING:2019/08/29 13:40 --

   JOBID       HOSTS USER        QUEUE     GROUP        STARTTIME          ENDTIME    TIMELIMIT         USED        STATE   CCODE REASON

   48724           1 user22    pbatch1        lc   15:14:27-08/26   15:15:49-08/26        03:00        01:22    Completed       - -
   48725           1 user22    pbatch1        lc   15:15:18-08/26   15:16:27-08/26        03:00        01:10    Completed       - -
   48725           1 user22    pbatch1        lc   15:16:13-08/26   15:19:33-08/26        03:00        03:20   Terminated     140 TERM_RUNLIMIT
   48726           1 user22    pbatch1        lc   15:20:20-08/26   15:21:00-08/26        03:00        00:40    Completed       - -
   ...
   49220           1 user22    pbatch2        lc   09:49:07-08/29   09:51:06-08/29        10:00        01:58   Terminated     255 TERM_CHKPNT
   49221           1 user22    pbatch2        lc   09:51:49-08/29   09:53:10-08/29        10:00        01:18   Terminated     255 TERM_CHKPNT</pre><h3>bjobs</h3>
<ul><li>The LSF <span class="fixed">bjobs</span> command provides the following options for job history information:
<ul><li><span class="fixed">-d</span> shows recently completed jobs</li>
<li><span class="fixed">-a</span> additionally shows jobs in all other states</li>
<li><span class="fixed">-l</span> can be used with -a and -d to show detailed information for each job</li>
</ul></li>
<li>The length of job history kept is configuration dependent.</li>
<li>See the man page for details.</li>
<li>Example below:</li>
</ul><pre>% <span class="text-danger">bjobs -d</span>
JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
487249  user22  DONE  pbatch1    lassen708   1*launch_ho *bin/lexec Aug 26 15:14
                                             40*batch_hosts
487254  user22  DONE  pbatch1    lassen708   1*launch_ho /bin/tcsh  Aug 26 15:15
                                             40*batch_hosts
487258  user22  EXIT  pbatch1    lassen708   1*launch_ho /bin/tcsh  Aug 26 15:16
                                             40*batch_hosts
...
492205  user22  EXIT  pbatch2    lassen708   1*launch_ho *ho 'done' Aug 29 09:48
                                             40*batch_hosts
492206  user22  DONE  pbatch2    lassen708   1*launch_ho *ho 'done' Aug 29 09:49
                                             40*batch_hosts
492210  user22  EXIT  pbatch2    lassen708   1*launch_ho *ho 'done' Aug 29 09:51
                                             40*batch_hosts</pre><h3>bhist</h3>
<ul><li>The LSF <span class="fixed">bhist</span> command provides the following options for job history information:
<ul><li><span class="fixed">-d</span> shows recently completed jobs</li>
<li><span class="fixed">-C</span> <span class="fixed">start_time,end_time</span> shows jobs completed within a specified date range. Time format is specified <span class="fixed">yyyy/mm/dd/HH:MM,</span><span class="fixed">yyyy/mm/dd/HH:MM</span> (no spaces permitted)</li>
<li><span class="fixed">-a</span> additionally shows jobs in all other states</li>
<li><span class="fixed">-l</span> can be used with -a and -d to show detailed information for each job</li>
</ul></li>
<li>The length of job history kept is configuration dependent.</li>
<li>See the man page for details.</li>
<li><span class="note-green">Note:</span> Users can only see their own usage. Elevated privileges are required to see other users, groups.</li>
<li>Example below:</li>
</ul><pre>% <span class="text-danger">bhist -d</span>

Summary of time in seconds spent in various states:
JOBID   USER    JOB_NAME  PEND    PSUSP   RUN     USUSP   SSUSP   UNKWN   TOTAL
487249  user22  *n/lexec  2       0       82      0       0       0       84      
487254  user22  *in/tcsh  2       0       70      0       0       0       72      
...      
492206  user22  * 'done'  2       0       118     1       0       0       121     
492210  user22  * 'done'  2       0       78      3       0       0       83</pre><h3>lacct</h3>
<ul><li>The LC developed <span class="fixed">lacct</span> command shows job history information. Several options are available. </li>
<li>Usage information - use the command: <span class="fixed">lacct -h</span></li>
<li><span class="note-green">Note:</span> Users can only see their own usage. Elevated privileges are required to see other users, groups.</li>
<li>May take a few minutes to run</li>
<li>Examples below:</li>
</ul><pre>% <span class="text-danger">lacct -s 05/01-00:00 -e 08/30-00:00</span>
JobID        User         Group         Nodes Start                 Elapsed
312339       user22       lc                1 2019/06/04-12:58      1:00:56
330644       user22       lc                1 2019/06/19-14:07      1:00:02
...
491036       user22       lc                1 2019/08/28-13:16      0:00:57
492210       user22       lc                1 2019/08/29-09:51      0:01:57

% <span class="text-danger">lacct -s 05/01-00:00 -e 08/30-00:00 -v</span>
JobID        User         Group        Project       Nodes Submit           Start            End                   Elapsed Hosts
312339       user22       lc           default           1 2019/06/04-12:58 2019/06/04-12:58 2019/06/04-13:59      1:00:56 lassen10
330644       user22       lc           default           1 2019/06/19-14:07 2019/06/19-14:07 2019/06/19-15:07      1:00:02 lassen32
...
491036       user22       lc           default           1 2019/08/28-13:16 2019/08/28-13:16 2019/08/28-13:17      0:00:57 lassen739
492210       user22       lc           default           1 2019/08/29-09:51 2019/08/29-09:51 2019/08/29-09:53      0:01:57 lassen412</pre><h3>lreport</h3>
<ul><li>The LC developed <span class="fixed">lreport</span> command provides a concise job usage summary for your jobs. </li>
<li>Usage information - use the command: <span class="fixed">lreport -h </span></li>
<li><span class="note-green">Note:</span> Users can only see their own usage. Elevated privileges required to see other users, groups.</li>
<li>May take a few minutes to run</li>
<li>Example below - shows usage, in minutes, since May 1st current year:</li>
</ul><pre>% <span class="text-danger">lreport -s 05/01-00:01 -e 08/30-00:01 -t min</span>
user(nodemin)               total
user22                       2312
TOTAL                        2312</pre><h3>bugroup</h3>
<ul><li>This is a marginally useful, native LSF command with several options.</li>
<li>Can be used to list banks and bank members. </li>
<li>Does not show allocation and usage information.</li>
<li>See the man page for details.</li>
</ul><h2><a name="LSF" id="LSF"></a>LSF - Additional Information</h2>
<h3><a name="R3" id="R3"></a>LSF Documentation</h3>
<ul><li>Most of the commonly used LSF syntax and commands have been covered in the previous sections.</li>
<li>For additional detailed information, users can consult several sources of LSF documentation, listed below.</li>
<li><a href="https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/New%20IBM%20Platform%20LSF%20Wiki/page/LSF%20documentation" target="_blank">IBM Spectrum LSF online documentation</a></li>
<li><a href="https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/lsf_welcome/lsf_welcome.html" target="_blank">IBM Knowledge Center LSF documentation</a></li>
<li>LC's LSF documents located at: <a href="https://hpc.llnl.gov/banks-jobs/running-jobs" target="_blank">https://hpc.llnl.gov/banks-jobs/running-jobs</a>. Includes:
<ul><li>Batch System Primer</li>
<li>LSF User Manual</li>
<li>LSF Quick Start Guide</li>
<li>LSF Commands</li>
<li>Batch System Cross-Reference</li>
<li>Slurm srun versus IBM jsrun</li>
</ul></li>
</ul><h3><a name="LSFconfig" id="LSFconfig"></a>LSF Configuration Commands</h3>
<ul><li>LSF provides several commands that can be used to display configuration information, such as:
<ul><li>LSF system configuration parameters:<span class="fixed"> bparams</span></li>
<li>Job queues: <span class="fixed">bqueues</span></li>
<li>Batch hosts: <span class="fixed">bhosts</span> and <span class="fixed">lshosts</span></li>
</ul></li>
<li>These commands are described in more detail below.</li>
</ul><h4>bparams Command</h4>
<ul><li>This command can be used to display the many configuration options and settings for the LSF system. Currently over 180 parameters.</li>
<li>Probably of most interest to LSF administrators/managers.</li>
<li>Examples:</li>
<li>See the bparams man page and/or LSF documentation for details.</li>
</ul><h4>bqueues Command</h4>
<ul><li>This command can be used to display information about the LSF queues</li>
<li>By default, returns one line of information for each queue.</li>
<li>Provides several options, including a long listing <span class="fixed">-l</span>.</li>
<li>Examples:</li>
</ul><pre>    % <span class="text-danger">bqueues</span>
    QUEUE_NAME      PRIO STATUS          MAX JL/U JL/P JL/H NJOBS  PEND   RUN  SUSP
    pall             60  Open:Active       -    -    -    -     0     0     0     0
    expedite         50  Open:Active       -    -    -    -     0     0     0     0
    pbatch           25  Open:Active       -    -    -    - 32083     0 32083     0
    exempt           25  Open:Active       -    -    -    -     0     0     0     0
    pdebug           25  Open:Active       -    -    -    -     0     0     0     0
    pibm             25  Open:Active       -    -    -    -     0     0     0     0
    standby           1  Open:Active       -    -    -    -     0     0     0     0</pre><p>Long listing format:</p>
<ul><li>See the bqueues man page and/or LSF documentation for details.</li>
</ul><h4>bhosts Command</h4>
<ul><li>This command can be used to display information about LSF hosts.</li>
<li>By default, returns a one line summary for each host group.</li>
<li>Provides several options, including a long listing <span class="fixed">-l</span>.</li>
<li>Examples:</li>
</ul><pre>    %<span class="text-danger"> bhosts</span>
    HOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV
    batch_hosts        ok              -  45936  32080  32080      0      0      0
    debug_hosts        unavail         -   1584      0      0      0      0      0
    ibm_hosts          ok              - 132286      0      0      0      0      0
    launch_hosts       ok              -  49995      3      3      0      0      0
    sierra4372         closed          -      0      0      0      0      0      0
    sierra4373         unavail         -      0      0      0      0      0      0</pre><p>Long listing format:</p>
<ul><li>See the bhosts man page and/or LSF documentation for details.</li>
</ul><h4>lshosts Command</h4>
<ul><li>This is another command used for displaying information about LSF hosts.</li>
<li>By default, returns a one line of information for every LSF host.</li>
<li>Provides several options, including a long listing <span class="fixed">-l</span>.</li>
<li>Examples:</li>
</ul><pre>    % <span class="text-danger">lshosts</span>
    HOST_NAME      type    model  cpuf ncpus maxmem maxswp server RESOURCES
    sierra4372  LINUXPP   POWER9 250.0    32 251.5G   3.9G    Yes (mg)
    sierra4373  UNKNOWN   UNKNOWN  1.0     -      -      -    Yes (mg)
    sierra4367  LINUXPP   POWER9 250.0    32 570.3G   3.9G    Yes (LN)
    sierra4368  LINUXPP   POWER9 250.0    32 570.3G   3.9G    Yes (LN)
    sierra4369  LINUXPP   POWER9 250.0    32 570.3G   3.9G    Yes (LN)
    sierra4370  LINUXPP   POWER9 250.0    32 570.3G   3.9G    Yes (LN)
    sierra4371  LINUXPP   POWER9 250.0    32 570.3G   3.9G    Yes (LN)
    sierra1     LINUXPP   POWER9 250.0    44 255.4G      -    Yes (CN)
    sierra10    LINUXPP   POWER9 250.0    44 255.4G      -    Yes (CN)
    ...
    ...</pre><p>Long listing format:</p>
<ul><li>See the lshosts man page and/or LSF documentation for details.</li>
</ul><h2><a name="MathLibs2" id="MathLibs2"></a>Math Libraries</h2>
<h3>ESSL</h3>
<ul><li>IBM's Engineering and Scientific Subroutine Library (ESSL) is a collection of high-performance subroutines providing a wide range of highly optimized mathematical functions for many different scientific and engineering applications, including:
<ul><li>Linear Algebra Subprograms</li>
<li>Matrix Operations</li>
<li>Linear Algebraic Equations  Eigensystem Analysis</li>
<li>Fourier Transforms</li>
<li>Sorting and Searching  Interpolation</li>
<li>Numerical Quadrature</li>
<li>Random Number Generation</li>
</ul></li>
<li>Location: the ESSL libraries are available through modules. Use the module avail command to see what's available, and then load the desired module. For example:
<p> </p>
<pre><span>% <span class="text-danger">module avail essl</span></span>
<span>    ------------------------- /usr/tcetmp/modulefiles/Core -------------------------
       </span>essl/sys-default    essl/6.1.0    essl/6.1.0-1   essl/6.2 (D)

<span>    % <span class="text-danger">module load essl/6.1.0-1</span></span>

<span>    % <span class="text-danger">module list</span></span>
<span>    Currently Loaded Modules:
   </span>  1) xl/2019.<span>02.07   2) spectrum-mpi/rolling-release   3) cuda/9.2.148   4) StdEnv   5) essl/6.1.0-1</span></pre></li>
</ul><ul><li>Version 6.1.0 Supports POWER9 systems sierra, lassen, and rzansel.</li>
<li>Version 6.2 supports CUDA 10</li>
<li>Environment variables will be set when you load the module of choice.  Use them with the following options during compile and link:<br /><br />For XL, GNU, and PGI: <br /><span class="fixed">-I${ESSLHEADERDIR} -L${ESSLLIBDIR64} -R${ESSLLIBDIR64} -lessl</span>
<p><br />For clang:<br /><span class="fixed">-I${ESSLHEADERDIR} -L${ESSLLIBDIR64} -Wl,-rpath,${ESSLLIBDIR64} -lessl</span><br /><br /><span class="note-green"><span>Note:</span><span class="fixed"> </span></span>If you don't use the  -R or -Wl,-rpath option you may end up dynamically linking to the libraries in /lib64 at runtime which may not be the version you thought you linked with.</p>
</li>
<li>The following libraries are available:<br /><span class="fixed">libessl.so</span>   - non-threaded<br /><span class="fixed">libesslsmp.so</span>   - threaded<br /><span class="fixed">libesslsmpcuda.so</span>  - subset of functions supporting cuda<br /><span class="fixed">liblapackforessl.so</span> - provides LAPACK functions not available in the ESSL libraries.<br /> </li>
<li>Additional XL libraries are also required, even when using other compilers:<br /><span class="fixed">XLLIBDIR="/usr/tce/packages/xl/xl-2019.08.20/alllibs"          </span><span># or the most recent/recommended version</span><br /><span class="fixed">-L${XLLIBDIR} -R${XLLIBDIR} -lxlfmath -lxlf90_r -lm            </span># add -lxlsmp when using -lesslsmp or -lesslsmpcuda<br /> </li>
<li>When using the <span class="fixed">-lesslsmpcuda</span>  library for CUDA add the following:<br /><span class="fixed">CUDALIBDIR="/usr/tce/packages/cuda/cuda-10.1.168/lib64"        </span><span># or the most recent/recommended version</span><br /><span class="fixed">-L${CUDALIBDIR} -R${CUDALIBDIR} -lcublas -lcudart</span><br /> </li>
<li>CUDA support:  The <span class="fixed">-lesslsmpcuda</span> library contains GPU-enabled versions of the following subroutines:<br /><table class="table table-bordered"><tr><td>
<pre>Matrix Operations
SGEMM, DGEMM, CGEMM, and ZGEMM
SSYMM, DSYMM, CSYMM, ZSYMM, CHEMM, and ZHEMM
STRMM, DTRMM, CTRMM, and ZTRMM
SSYRK, DSYRK, CSYRK, ZSYRK, CHERK, and ZHERK
SSYR2K, DSYR2K, CSYR2K, ZSYR2K, CHER2K, and ZHER2K

Fourier Transforms
SCFTD and DCFTD
SRCFTD and DRCFTD
SCRFTD and DCRFTD

Linear Least Squares
SGEQRF, DGEQRF, CGEQRF, and ZGEQRF
SGELS, DGELS, CGELS, and ZGELS</pre></td>
<td>
<pre>Dense Linear Algebraic Equations
SGESV, DGESV, CGESV, and ZGESV
SGETRF, DGETRF, CGETRF, and ZGETRF
SGETRS, DGETRS, CGETRS, and ZGETRS
SGETRI, DGETRI, CGETRI, and ZGETRI  ( new in 6.2 )
SPPSV, DPPSV, CPPSV, and ZPPSV
SPPTRF, DPPTRF, CPPTRF, and ZPPTRF
SPPTRS, DPPTRS, CPPTRS, and ZPPTRS
SPOSV, DPOSV, CPOSV, and ZPOSV
SPOTRF, DPOTRF, CPOTRF, and ZPOTRF
SPOTRS, DPOTRS, CPOTRS, and ZPOTRS
SPOTRI, DPOTRI, CPOTRI, and ZPOTRI ( new in 6.2 )</pre></td>
</tr></table></li>
</ul><ul><li>Coverage for BLAS, LAPACK and SCALAPACK functions:
<ul><li>A subset of the functions contained in ESSL are tuned replacements for some of the functions provided in the BLAS and LAPACK libraries. </li>
<li><span class="note-green">Note:</span> There are no ESSL substitutes for SCALAPACK functions.</li>
<li>BLAS: The following functions are NOT available in ESSL:  <span class="fixed">dcabs1 dsdot lsame scabs1 sdsdot xerbla_array</span></li>
<li>LAPACK: a list of functions available in<a href="/sites/default/files/essl-lapack.txt"> ESSL is available HERE</a></li>
<li>All other LAPACK functions not in ESSL are available in the separate library  <strong>liblapackforessl.so</strong></li>
<li>See the ESSL documentation for details.</li>
</ul></li>
<li>Documentation - select the appropriate version:
<ul><li>Once you've loaded the essl module, you can use man pages to view documentation for selected functions. Example:  man dgemm</li>
<li>ESSL 5.5 Guide and Reference: <a href="https://publib.boulder.ibm.com/epubs/pdf/a2322688.pdf" target="_blank">https://publib.boulder.ibm.com/epubs/pdf/a2322688.pdf</a></li>
<li>ESSL 6.1 Guide and Reference:  <a href="https://www.ibm.com/support/knowledgecenter/SSFHY8_6.1/reference/essl_reference_pdf.pdf?view=kc" target="_blank">https://www.ibm.com/support/knowledgecenter/SSFHY8_6.1/reference/essl_reference_pdf.pdf?view=kc</a></li>
<li>ESSL 6.2 Guide and Reference: <a href="https://www.ibm.com/support/knowledgecenter/SSFHY8_6.2/reference/essl_reference_pdf.pdf?view=kc" target="_blank">https://www.ibm.com/support/knowledgecenter/SSFHY8_6.2/reference/essl_reference_pdf.pdf?view=kc</a></li>
<li>In the "Guide and Reference" document, some useful references include:
<ul><li>Chapter 5 for compile examples</li>
<li>Appendix B for a list of LAPACK functions supported by ESSL and a mechanism to use LAPACK with ESSL</li>
<li>For CUDA, search for a section labeled  "Using the ESSL SMP CUDA Library"</li>
</ul></li>
</ul></li>
</ul><h3>IBM's Mathematical Acceleration Subsystem (MASS) Libraries</h3>
<ul><li>The IBM XL C/C++ and XL Fortran compilers are shipped with a set of Mathematical Acceleration Subsystem (MASS) libraries for high-performance mathematical computing.</li>
<li>The libraries consist of tuned mathematical intrinsic functions (sin, pow, log, tan, cos, sqrt, etc.).</li>
<li>Typically provide significant performance improvement over the standard system math library routines.</li>
<li>Three different versions are available:
<ul><li>Scalar - libmass.a</li>
<li>Vector - libmassv.a</li>
<li>SIMD - libmass_simdp8.a (POWER8) and libmass_simdp9.a (POWER9)</li>
</ul></li>
<li>Location: /opt/ibm/xlmass/version#</li>
<li>Documentation:
<ul><li><a href="https://www.ibm.com/support/home/product/W511326D80541V01/Mathematical_Acceleration_Subsystem" target="_blank">IBM Mathematical Acceleration Subsystem (MASS) website</a></li>
<li>C/C++: <a href="https://www.ibm.com/support/knowledgecenter/en/SSXVZZ_13.1.6/com.ibm.compilers.linux.doc/proguide.pdf" target="_blank">Chapter 9 of the Optimization and Programming Guide</a></li>
<li>Fortran: <a href="https://www.ibm.com/support/knowledgecenter/SSAT4T_15.1.6/com.ibm.compilers.linux.doc/proguide.pdf" target="_blank">Chapter 8 of the Optimization and Programming Guide</a></li>
<li>Quickstart online documentation:  For <a href="https://www.ibm.com/support/pages/using-mass-libraries-linux-little-endian" target="_blank">Linux Little Endian</a>    Note that this document shows POWER8 examples - just substitute POWER9 options where applicable for Sierra systems.</li>
</ul></li>
<li>How to use:
<ul><li>Automatic through compiler options</li>
<li>Explicit by including MASS routines in your source code</li>
</ul></li>
<li>Automatic usage:
<ul><li>Compile using any of these sets of compiler options:<br /><table class="table"><tr><th scope="col">C/C++</th>
<th scope="col">Fortran</th>
</tr><tr><td>
<pre>-qhot -qignerrno -qnostrict
-qhot -qignerrno -qstrict=nolibrary
-qhot -O3
-O4
-O5</pre></td>
<td>
<pre>-qhot -qnostrict
-qhot -O3 -qstrict=nolibrary
-qhot -O3
-O4
-O5</pre></td>
</tr></table></li>
</ul></li>
</ul><ul><li>The IBM XL compilers will automatically attempt to vectorize calls to system math functions by using the equivalent MASS vector functions</li>
<li>If the vector function can't be used, then the compiler will attempt to use the scalar version of the function</li>
<li>Does not apply to the SIMD library functions</li>
</ul><ul><li>Explicit usage:
<ul><li>Familiarize yourself with the MASS routines by consulting the relevant IBM documentation</li>
<li>Include selected MASS routines in your source code</li>
<li>Include the relevant mass*.h in your source files (see MASS documentation)</li>
<li>Link with the required MASS library/libraries - no Libpath needed.<br /><span class="fixed">-lmass               </span>Scalar Library<br /><span class="fixed">-lmassv              </span>Vector Library<br /><span class="fixed">-lmass_simdp8        </span>SIMD Library - POWER8<br /><span class="fixed">-lmass_simdp9        </span>SIMD Library - POWER9
<p><br />For example:<br /><span class="fixed">xlc myprog.c -o myprog -lmass -lmassv<br />xlf myprog.f -o myprog -lmass -lmassv<br />mpixlc myprog.c -o myprog -lmass_simdp9<br />mpixlf90 myprog.f -o myprog -lmass_simdp9</span></p>
</li>
</ul></li>
</ul><ul><li>It's also possible to use libmass.a scalar library for some functions and the normal math library libm.a for other functions. See the Optimization and Programming Guide for details.</li>
<li><span class="note-green">Note:</span> The MASS functions must run with the default rounding mode and floating-point exception trapping settings.</li>
</ul><h3>NETLIB: BLAS, LAPACK, ScaLAPACK, CBLAS, LAPACKE</h3>
<ul><li>This set of  libraries available from netlib provide routines that are standard building blocks for performing basic vector and matrix operations (BLAS), routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations, eigenvalue problems, and singular value problems (LAPACK), and a library of high-performance linear algebra routines for parallel distributed memory machines that solve dense and banded linear systems, least squares problems, eigenvalue problems, and singular value problems. (ScaLPACK).</li>
<li>The BLAS, LAPACK, ScaLAPACK, CBLAS, LAPACKE libraries are all available through the common lapack module:
<ul><li>Loading any lapack module will load all of its associated libraries</li>
<li>It is not necessary to match the Lapack version with the XL compiler version you are using.</li>
<li>Example: showing available lapack modules, loading the default lapack module, loading an alternate lapack module.<br /><pre>% <span class="text-danger">ml avail lapack</span>
lapack/3.8.0-gcc-4.9.3    lapack/3.8.0-xl-2018.08.24    lapack/3.8.0-xl-2018.11.26    lapack/3.8.0-xl-2019.06.12   lapack/3.8.0-xl-2019.08.20 (L,D)

% <span class="text-danger">ml load lapack</span>

% <span class="text-danger">ml load lapack/3.8.0-gcc-4.9.3</span></pre></li>
</ul></li>
</ul><ul><li><span>The environment variable LAPACK_DIR will be set to the directory containing the archive (.a) and shared object (.so) files.  The LAPACK_DIR will also be added to the LD_LIBRARY_PATH environment variable so you find the appropriate version at runtime.  The environment variable LAPACK_INC will be set to the directory containing the header files. </span>
<pre>% <span class="text-danger">echo $LAPACK_DIR</span>
/usr/tcetmp/packages/lapack/lapack-3.8.0-xl-2018.08.20/lib

% <span class="text-danger">ls $LAPACK_DIR</span>
libblas.a   libcblas.a   liblapack.a   liblapacke.a   libscalapack.a   libblas_.a  liblapack_.a
libblas.so  libcblas.so  liblapack.so  liblapacke.so  libscalapack.so  libblas_.so  liblapack_.so</pre></li>
<li>Compile and link flags:
<ul><li>Select those libraries that your code uses</li>
<li>The -Wl,-rpath,${LAPACK_DIR} explicitly adds  ${LAPACK_DIR} to the runtime library search path (rpath) within the executable.<br /><br /><span class="fixed">-I${LAPACK_INC} -L${LAPACK_DIR}  -Wl,-rpath,${LAPACK_DIR}  -lblas -llapack -lscalapack -lcblas -llapacke</span><br /> </li>
</ul></li>
<li>Portability between Power9 (lassen, rzansel, sierra) and Power8 (ray, rzmanta, shark) systems:
<ul><li>Behind the scenes, there are actually 2 separately optimized XL versions of the libraries. One labeled for P9 and the other for P8.</li>
<li>The modules access the appropriate version using symbolic links.</li>
<li>Using the generic version provided by the module will allow for portability between system types and still obtain optimum performance for the platform being run on.</li>
</ul></li>
<li>Dealing with "undefined references" to BLAS or LAPACK functions during link:
<ul><li>This is a common symptom of a long-standing issue with function naming conventions which has persisted during the evolution of fortran standards, the interoperability between fortran, C, and C++, and the implementation of features provided by various compiler vendors. Some history and details can be viewed at the following links:<br /><a href="http://www.math.utah.edu/software/c-with-fortran.html#routine-naming" target="_blank">http://www.math.utah.edu/software/c-with-fortran.html#routine-naming</a><br /><a href="https://stackoverflow.com/questions/18198278/linking-c-with-blas-and-lapack" target="_blank">https://stackoverflow.com/questions/18198278/linking-c-with-blas-and-lapack</a></li>
<li>The issue boils down to a mismatch in function names, either referenced by code or provided by libraries, with or without trailing underscores (_).</li>
<li>The error messages are of the form:<br />&lt;source_file&gt;:&lt;line_number&gt; undefined reference to `&lt;func&gt;_'<br />&lt;library_file&gt;: undefined reference to `&lt;func&gt;_'</li>
<li>Examples:<br />lapack_routines.cxx:93: undefined reference to `zgtsv_'<br />../SRC/libsuperlu_dist.so.6.1.1: undefined reference to `ztrtri_'                              &lt;= this actually uncovered an omission in a superlu header file.<br />.../libpetsc.so: undefined reference to `dormqr'</li>
<li>The solution is to either choose the right library or alter the name referenced in the code.</li>
</ul></li>
<li>Selecting the right library:
<ul><li>You'll see by examining the module list, two flavors of these libraries are provided: GNU and IBM XL.</li>
<li>By default, GNU Fortran appends an underscore to external names so the functions in the gcc versions have trailing underscores (ex. dgemm_).</li>
<li>By default the IBM XL does not append trailing underscores.</li>
<li>The recommendation is to use the IBM XL compilers and an XL version of the lapack libraries, and then resolve the references to functions with trailing underscores by either of these methods:
<ul><li>If you can't avoid the use of GNU gfortran, you could either link with the GCC lapack library, or use the compiler option <span class="fixed">-fnounderscoring</span> then link with the XL lapack library.</li>
<li>If your code or libraries reference functions with trailing underscores, or a mix of both, use or add the following XL libraries to the list: <span class="fixed">-lblas_ -llapack_</span>
<p>Note the trailing underscores. These libraries provide trailing-underscore versions of all the functions that are provided in the primary -lblas and -llapack libraries.</p>
</li>
</ul></li>
</ul></li>
<li>Altering the names referenced in the source code:  if you have control over the source code, you can try using the following options:
<ul><li>GNU gfortran option <span class="fixed">-fnounderscoring</span> to look for external functions without the trailing underscore.</li>
<li>IBM XL option <span class="fixed">-qextname&lt;=name&gt;</span> to append trailing underscores to all or specifically named global entities.</li>
<li>Using #define to redefine the names, controlled by a compiler define option (ie. -DNo_ or -DAdd_ )
<p><span class="fixed">#ifdef No_<br />#define dgemm_  dgemm<br />#endif</span></p>
</li>
</ul></li>
<li>Documentation:<br /><a href="http://www.netlib.org/blas/" target="_blank">http://www.netlib.org/blas/</a><br /><a href="http://www.netlib.org/lapack/" target="_blank">http://www.netlib.org/lapack/</a><br /><a href="http://www.netlib.org/scalapack/" target="_blank">http://www.netlib.org/scalapack/</a><br /><a href="https://www.netlib.org/lapack/lapacke.html" target="_blank">https://www.netlib.org/lapack/lapacke.html</a></li>
</ul><h3>FFTW</h3>
<ul><li>Fastest Fourier Transform in the West.</li>
<li>The FFTW libraries are available through modules:   <span class="fixed">ml load fftw</span></li>
<li>The module will set the following environment variables: <span class="fixed"> LD_LIBRARY_PATH,  FFTW_DIR</span></li>
<li>Use the following compiler/linker options:   <span class="fixed"> -I${FFTW_DIR}/include -L${FFTW_DIR}/lib -R${FFTW_DIR}/lib  -lfftw3</span></li>
<li>The libraries were built using the gcc C compiler and xlf fortran compiler. The function symbols in the libraries do not have trailing underscores. It is recommended that you do NOT use gfortran to build and link your codes with the FFTW libraries so that you avoid any issues with functions with trailing underscores that can not be found.</li>
<li>The libraries include: single and double precision, mpi, omp, and threads.</li>
<li>Website: <a href="http://fftw.org" target="_blank">http://fftw.org</a></li>
</ul><h3>PETSc</h3>
<ul><li>Portable, Extensible Toolkit for Scientific Computation</li>
<li>Provides a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. It supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU parallelism.</li>
<li>To view available versions, use the command:  <span class="fixed">ml avail petsc</span></li>
<li>Load the desired version using <span class="fixed">ml load <em>modulename</em></span>. This will set the <span class="fixed">PETSC_DIR</span> environment variable and put the <span class="fixed">${PETSC_DIR}/bin</span> directory in your PATH.</li>
<li>Online documentation available at: <a href="https://www.mcs.anl.gov/petsc/" target="_blank">https://www.mcs.anl.gov/petsc/</a></li>
<li>GPU support documentation available at: <a href="https://www.mcs.anl.gov/petsc/features/gpus.html" target="_blank">https://www.mcs.anl.gov/petsc/features/gpus.html</a></li>
</ul><h3>GSL - GNU Scientific Library</h3>
<ul><li>Provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. There are over 1000 functions in total with an extensive test suite.</li>
<li>To view available versions, use the command:  <span class="fixed">ml avail gsl</span><br />Load the desired version using <span class="fixed">ml load modulename.  </span>This will set the following environment variables:  <span class="fixed">LD_LIBRARY_PATH,  GSL_DIR</span></li>
<li>Use the following compiler/linker options:    <span class="fixed">-I${GSL_DIR}/include -L${GSL_DIR}/lib -R${GSL_DIR}/lib  -lgsl</span></li>
<li>Online documentation available at: <a href="https://www.gnu.org/software/gsl/" target="_blank">https://www.gnu.org/software/gsl/</a></li>
</ul><h3>NVIDIA CUDA Tools</h3>
<ul><li>The CUDA toolkit comes with several math libraries, which are described in the CUDA toolkit documentation. These are intended to be replacements for existing CPU math libraries that execute on the GPU, without requiring the user to explicitly write any GPU code. Note that the GPU-based IBM ESSL routines mentioned above are built on libraries like cuBLAS and in certain cases may take better advantage of the CPU and multiple GPUs together (specifically on the CORAL EA systems) than a pure CUDA program would.</li>
<li><a href="https://docs.nvidia.com/cuda/cublas/index.html" target="_blank">cuBLAS</a> provides drop-in replacements for Level 1, 2, and 3 BLAS routines. In general, wherever a BLAS routine was being used, a cuBLAS routine can be applied instead. Note that cuBLAS stores data in a column-major format for Fortran compatibility. See here for an example code using cuBLAS. The Six Ways to SAXPY blog post describes how to perform SAXPY using a number of approaches and one of them is cuBLAS. cuBLAS also provides a set of extensions that perform BLAS-like operations. In particular, one of interest may be the batched routines for LU decomposition, which are optimized for small matrix operations, like 100x100 or smaller (they will not perform well on large matrices). NVIDIA has blog posts describing how to use the batched routine in CUDA C and CUDA Fortran.</li>
<li><a href="https://docs.nvidia.com/cuda/cusparse/index.html" target="_blank">cuSPARSE</a> provides a set of operations for sparse matrix operations (in particular, sparse matrix-vector multiply, for example). cuSPARSE is capable of representing data in multiple formats for compatibility with other libraries, for example the compressed sparse row (CSR) format. As with cuBLAS, these are intended to be drop-in replacements for other libraries when you are computing on NVIDIA GPUs.</li>
<li><a href="https://docs.nvidia.com/cuda/cusolver/index.htm" target="_blank">cuSOLVER</a> is a higher level package built on cuBLAS and cuSPARSE that is intended to provide LAPACK-like operations. The documentation provides some examples of usage.</li>
<li><a href="https://docs.nvidia.com/cuda/cufft/index.html" target="_blank">cuFFT</a> provides FFT operations as replacements for programs that were using existing CPU libraries. The documentation includes a table indicating how to convert from FFTW to cuFFT, and a description of the FFTW interface to cuFFT.</li>
<li><a href="https://docs.nvidia.com/cuda/curand/index.html" target="_blank">cuRAND</a> is a set of tools for pseudo-random number generation.</li>
<li><a href="https://docs.nvidia.com/cuda/thrust/index.html" target="_blank">Thrust</a> provides a set of STL-like templated libraries for performing common parallel operations without explicitly writing GPU code. Common operations include sorting, reductions, saxpy, etc. It also allows you to define your own functional transformation to apply to the vector.</li>
<li><a href="https://nvlabs.github.io/cub/" target="_blank">CUB</a>, like Thrust, provides a set of tools for doing common collective CUDA operations like reductions and scans so that programmers do not have to implement it themselves. The algorithms are individually tuned for each NVIDIA architecture. CUB supports operations at the warp-wide, block-wide, or kernel-wide level. CUB is generally intended to be integrated within an existing CUDA C++ project, whereas Thrust is a much more general, higher level approach. Consequently Thrust will usually be a bit slower than CUB in practice, but is easier to program with, especially in a project that is just beginning its port to GPUs. Note that CUB is not an official NVIDIA product, although it is supported by NVIDIA employees.</li>
</ul><h2><a name="Debug" id="Debug"></a>Debugging</h2>
<h3><a name="TotalView" id="TotalView"></a>TotalView</h3>
<p><strong>Overview</strong></p>
<ul><li>TotalView is a sophisticated and powerful tool used for debugging and analyzing both serial and parallel programs. It is especially popular for debugging HPC applications.</li>
<li>TotalView provides source level debugging for serial, parallel, multi-process, multi-threaded, accelerator/GPU and hybrid applications written in C/C++ and Fortran.</li>
<li>Both a graphical user interface and command line interface are provided. Advanced, memory debugging tools and the ability to perform "replay" debugging are two additional features.</li>
<li>TotalView is supported on all LC platforms including Sierra and CORAL EA systems.</li>
<li>The default version of TotalView should be in your path automatically:</li>
<li>To view all available versions: <span class="fixed">module avail totalview</span></li>
<li>To load a different version: <span class="fixed">module load <em>module_name</em></span></li>
<li>For details on using modules: <a href="https://hpc.llnl.gov/software/modules-and-software-packaging" target="_blank">https://hpc.llnl.gov/software/modules-and-software-packaging</a>.</li>
<li>Only a few quickstart summaries are provided here - please see the More Information section below for details.</li>
</ul><h4>Interactive Debugging</h4>
<ol start="1"><li>To debug a parallel application interactively, you will first need to acquire an allocation of compute nodes. This can be done by using the LSF <span class="fixed">bsub</span> command or the LC <span class="fixed">lalloc</span> command. Examples for both are shown below.
<table class="table table-striped table-bordered" summary="Examples for debugging a parallel application interactively"><tr><th scope="row">bsub</th>
<td><span class="fixed">bsub -nnodes 2 -W 60 -Is -XF /usr/bin/tcsh</span></td>
<td>Request 2 nodes for 60 minutes, interactive shell with X11 forwarding, using the tcsh login shell. Default account and queue (pbatch) are used since they are not explicitly specified.</td>
</tr><tr><th scope="row">bsub</th>
<td><span class="fixed">bsub -nnodes 2 -W 60 -Is -XF -q pdebug /usr/bin/tcsh</span></td>
<td>Same as above but using the pdebug queue instead of the default pbatch queue</td>
</tr><tr><th scope="row">lalloc</th>
<td><span class="fixed">lalloc 2<br />lalloc 2 -q pdebug</span></td>
<td>LC equivalents - same as above but less verbose</td>
</tr></table></li>
</ol><ol start="2"><li>While your allocation is being setup, you will see messages similar to those below.<br /><table class="table table-striped table-bordered" summary="Examples of bsub and lalloc messages you will see during setup"><tr><th scope="col">bsub</th>
<th scope="col">lalloc</th>
</tr><tr><td><span class="fixed">% bsub -nnodes 2 -W 60 -Is -XF /usr/bin/tcsh<br />Job &lt;70544&gt; is submitted to default queue &lt;pbatch&gt;.<br />&lt;&lt;ssh X11 forwarding job&gt;&gt;<br />&lt;&lt;Waiting for dispatch ...&gt;&gt;<br />&lt;&lt;Starting on lassen710&gt;&gt;</span><br /> </td>
<td><span class="fixed">% lalloc 2<br />+ exec bsub -nnodes 2 -Is -XF -W 60 -core_isolation 2<br />/usr/tce/packages/lalloc/lalloc-2.0/bin/lexec<br />Job &lt;70542&gt; is submitted to default queue &lt;pbatch&gt;.<br />&lt;&lt;ssh X11 forwarding job&gt;&gt;<br />&lt;&lt;Waiting for dispatch ...&gt;&gt;<br />&lt;&lt;Starting on lassen710&gt;&gt;<br />&lt;&lt;Redirecting to compute node lassen263,<br />setting up as private launch node&gt;&gt;</span></td>
</tr></table></li>
<li>Launch your application under totalview: this can be done by using the LC <span class="fixed">lrun</span> command or the IBM <span class="fixed">jsrun</span> command. Examples for both are shown below.<br /><table class="table table-striped table-bordered" summary="Examples of lrun and jsrun commands"><tr><th scope="row">lrun</th>
<td><span class="fixed">totalview lrun -a -N2 -T2 a.out<br />totalview --args lrun -N2 -T2 a.out</span></td>
<td>Launches your parallel job with 2 nodes and 2 tasks on each node</td>
</tr><tr><th scope="row">jsrun</th>
<td><span class="fixed">totalview jsrun -a -n2 -a2 -c40 a.out<br />totalview --args jsrun -n2 -a2 -c40 a.out</span></td>
<td>Same as above, but using jsrun syntax: 2 resource sets with each one using 2 processes and a full node (40 CPUs)</td>
</tr></table></li>
<li>Eventually, the totalview Root and Process windows will appear, as shown in <strong>(1)</strong> below. At this point, totalview has loaded the jsrun or lrun job launcher program. You will need to GO the program in order for it to continue and load your parallel application on your allocated compute nodes.</li>
<li>After your parallel application has been loaded onto the compute nodes, totalview will inform you of this and ask you if the program should be stopped as shown in <strong>(2)</strong> below. In most cases the answer is Yes so you can set breakpoints, etc. Notice that the program name is lrun&lt;bash&gt;&lt;jsrun&gt;&lt;jsrun&gt; (or something similar). This is because there is a chain of execs before your application is run, and TotalView could not fit the full chain into this dialogue box.</li>
<li>When your job is ready for debugging, you will see your application's source code in the Process Window, and the parallel processes in the Root Window as shown in <strong>(3)</strong> below. You may now debug your application using totalview.  <em>(click images for larger version)</em></li>
</ol><div class="float-left"><strong>1<strong>.</strong>) </strong><em><div class="media media-element-container media-default"><div id="file-1862" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/totalview01-300x305-png">totalview01-300x305.png</a></h2>
    
  
  <div class="content">
    <img alt="totalview Root and Process windows " height="305" width="300" class="media-element file-default" data-delta="105" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/totalview01-300x305.png" /></div>

  
</div>
</div></em></div>
<div class="float-left"><strong>2.)</strong><em> <div class="media media-element-container media-default"><div id="file-1863" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/totalview02-300x150-png">totalview02-300x150.png</a></h2>
    
  
  <div class="content">
    <img alt="totalview window" height="150" width="300" class="media-element file-default" data-delta="106" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/totalview02-300x150.png" /></div>

  
</div>
</div></em></div>
<div class="float-left">3.) <div class="media media-element-container media-default"><div id="file-1864" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/totalview03-300x305-png">totalview03-300x305.png</a></h2>
    
  
  <div class="content">
    <img alt="Process Window" height="305" width="300" class="media-element file-default" data-delta="107" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/totalview03-300x305.png" /></div>

  
</div>
</div></div>
<h4>Attaching to a Running Parallel Job</h4>
<ol><li>Find where the job's <span class="fixed">jsrun</span> job manager process is running. This is usually the first compute node in the job's node list.</li>
<li>The<span class="fixed"> bjobs -X</span> and <span class="fixed">lsfjobs -v</span> commands can be used to show the job's node list.</li>
<li>Start totalview on a login node, or else rsh directly to the node where the jobs' jsrun process is running and start totalview there: <span class="fixed">totalview &amp;</span></li>
<li>If you choose to rsh directly to the node, skip to step 5.</li>
<li>After totalview starts, select "A running program" from the "Start a Debugging Session" dialog window, as shown in <strong>(1)</strong> below.</li>
<li>When the "Attach to running program(s)" window appears, click on the <strong>H+</strong> button to add the name of the host where the jobs' jsrun process is running. Enter the node's name in the "Add Host" dialog box and click OK, as shown in <strong>(2)</strong> below.</li>
<li>After totalview connects to the node, you should see the jsrun process in the process list. Select it, and click "Start Session" as shown in <strong>(3)</strong> below.</li>
<li>Totalview will attach to the job and the totalview Root and Process windows will appear to allow you to begin debugging the running job.  <em>(click images for larger version)</em><br /> </li>
</ol><div class="float-left"><strong>1.) </strong><div class="media media-element-container media-default"><div id="file-1869" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/totalview04-png-0">totalview04.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/totalview04_0.png"><img alt="start a debugging session" height="220" width="218" style="height: 220px; width: 218px;" class="media-element file-default" data-delta="105" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/totalview04_0-218x220.png" /></a>  </div>

  
</div>
</div></div>
<div class="float-left"><strong>2.)</strong> <div class="media media-element-container media-default"><div id="file-1870" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/totalview05-png-0">totalview05.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/totalview05_0.png"><img alt="add host dialog" height="220" width="300" style="height: 220px; width: 300px;" class="media-element file-default" data-delta="106" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/totalview05_0-300x220.png" /></a>  </div>

  
</div>
</div></div>
<div class="float-left"><strong>3.) </strong><div class="media media-element-container media-default"><div id="file-1871" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/totalview06-png-0">totalview06.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/totalview06_0.png"><img alt="start session dialog" height="219" width="300" style="height: 219px; width: 300px;" class="media-element file-default" data-delta="107" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/totalview06_0-300x219.png" /></a>  </div>

  
</div>
</div></div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<div class="clear-floats"> </div>
<h4 class="clear-floats">Debugging GPU Code on Sierra</h4>
<ul><li>TotalView supports GPU debugging on Sierra systems:
<ul><li>CUDA with NVIDIA NVCC compiler</li>
<li>OpenMP target regions with IBM XL and CLANG compilers</li>
</ul></li>
<li>NVIDIA CUDA recommended compiler options:
<ul><li><span class="fixed">-O0 -g -G -arch sm_60</span> : to generate GPU DWARF and avoid just-in-time (JIT) compilation for improved performance.</li>
<li><span class="fixed">-dlink</span> : reduce number of GPU ELF images when linking GPU object files into a large image; improves performance.</li>
</ul></li>
<li>IBM XL recommended compiler options:
<ul><li><span class="fixed">-O0 -g -qsmp=omp:noopt -qfullpath -qoffload </span>: generate debug information, no optimization, openMP with offloading. Should be sufficient for most applications.</li>
<li><span class="fixed">-qnoinline -Xptxas -O0 -Xllvm2ptx -nvvm-compile-options=-opt=0</span> : may be necessary for heavily templated codes, or if previous compile options result in "odd" code motion.</li>
</ul></li>
<li>Clang recommended compiler options:
<ul><li><span class="fixed">-fopenmp -fopenmp-targets=nvptx64-nvidia-cuda --cuda-noopt-device-debug</span> : enable openMP offloading for NVIDIA GPUs; no optimization with cuda device debug generation.</li>
</ul></li>
<li>For the most part, the basics of running GPU-enabled applications under TotalView are similar to those of running other applications. However, there are unique GPU features and usage details, which are discussed in the "More Information" links below (the TotalView CORAL Update in particular).</li>
</ul><h4>More Information</h4>
<ul><li>LC Tutorial: <a href="https://hpc.llnl.gov/training/tutorials/totalview-tutorial">https://hpc.llnl.gov/training/tutorials/totalview-tutorial</a></li>
<li>Sierra systems usage notes: <a href="https://lc.llnl.gov/confluence/display/SIERRA/TotalView" target="_blank">https://lc.llnl.gov/confluence/display/SIERRA/TotalView</a></li>
<li><a href="/sites/default/files/2018.10.24.TotalViewCORALupdate.pdf">TotalView CORAL Update</a> </li>
<li>Vendor website: <a href="https://www.roguewave.com/" target="_blank">https://www.roguewave.com/</a></li>
</ul><h3><a name="STAT" id="STAT"></a>STAT</h3>
<h4>Overview</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1853" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/stat01-png">stat01.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/stat01.png"><img alt="Example of a STAT 2D spatial graph" height="300" width="500" style="height: 300px; width: 500px;" class="media-element file-default" data-delta="105" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/stat01-500x300.png" /></a>  </div>

  
</div>
</div></div>
<ul><li>The Stack Trace Analysis Tool (STAT) gathers and merges stack traces from a parallel application's processes.</li>
<li>Primarily intended to attach to a hung job, and quickly identify where the job is hung. The output from STAT consists of 2D spatial and 3D spatial-temporal graphs. These graphs encode calling behavior of the application processes in the form of a prefix tree. Example of a STAT 2D spatial graph shown on right (click to enlarge).</li>
<li>Graph nodes are labeled by function names. The directed edges show the calling sequence from caller to callee and are labeled by the set of tasks that follow that call path. Nodes that are visited by the same set of tasks are assigned the same color.</li>
<li>STAT is also capable of gathering stack traces with more fine-grained information, such as the program counter or the source file and line number of each frame.</li>
<li>STAT has demonstrated scalability over 1,000,000 MPI tasks and its logarithmic scaling characteristics position it well for even larger systems.</li>
<li>STAT is supported on most LC platforms, including Linux, Sierra/CORAL EA, and BG/Q. It works for Message Passing Interface (MPI) applications written in C, C++, and Fortran and supports threads.</li>
<li>The default version of STAT should be in your path automatically:</li>
<li>To view all available versions: <span class="fixed">module avail stat</span></li>
<li>To load a different version: <span class="fixed">module load <em>module_name</em></span></li>
<li>For details on using modules: <a href="https://hpc.llnl.gov/software/modules-and-software-packaging" target="_blank">https://hpc.llnl.gov/software/modules-and-software-packaging</a>.</li>
</ul><h4>Quickstart</h4>
<ul><li>Only a brief quickstart summary is provided here - please see the More Information section below for details.</li>
</ul><ol><li>In a typical usage case, you have already launched a job which appears to be hung. You would then use STAT to debug the job.</li>
<li>First, find where the job's<span class="fixed"> jsrun</span> job manager process is running. This is usually the first compute node in the job's node list.<br />The<span class="fixed"> bjobs -X </span>and<span class="fixed"> lsfjobs -v</span> commands can be used to show the job's node list.</li>
<li>Start STAT using the <span class="fixed">stat-gui</span> command on a login node, or else rsh directly to the node where the jobs' jsrun process is running and start <span class="fixed">stat-gui</span> there.<br />If you choose to rsh directly to the node, skip to step 5.</li>
<li>Two STAT windows will appear. In the "Attach" window, enter the name of the compute node where your jsrun process is running, and then click "Search Remote Host" as shown in <strong>(1)</strong> below.</li>
<li>STAT will then display the jsrun process running on the first compute node. Make sure it is selected and then click "Attach", as shown in <strong>(2)</strong> below.</li>
<li>A 2D graph of your job's merged stack traces will appear, as shown in <strong>(3)</strong> below. You can now use STAT to begin debugging your job. See the "More Information" section below for links to STAT debugging details.  <em>(click images for larger version)</em><br /><table class="table table-bordered"><tr><td><div class="media media-element-container media-default"><div id="file-1854" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/stat02-png">stat02.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/stat02.png"><img alt="Example of STAT &amp;#039;Attach&amp;#039; window with &amp;#039;Search Remote Host&amp;#039; click option" height="225" width="300" style="height: 225px; width: 300px;" class="media-element file-default" data-delta="106" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/stat02-300x225.png" /></a>  </div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-1855" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/stat03-png">stat03.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/stat03.png"><img alt="Example of STAT display: make sure to select the jsrun process and then click &amp;#039;Attach&amp;#039;" height="225" width="300" style="height: 225px; width: 300px;" class="media-element file-default" data-delta="107" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/stat03-300x225.png" /></a>  </div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-1853--2" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/stat01-png">stat01.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/stat01.png"><img alt="Example of a STAT 2D graph of your job&amp;#039;s merged stack traces" height="180" width="300" style="height: 180px; width: 300px;" class="media-element file-default" data-delta="108" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/stat01-300x180.png" /></a>  </div>

  
</div>
</div></td>
</tr><tr><td><strong>(1)</strong></td>
<td><strong>(2)</strong></td>
<td><strong>(3)</strong></td>
</tr></table></li>
</ol><h4>More Information</h4>
<ul><li>STAT User Guide: <a href="https://github.com/LLNL/STAT/blob/develop/doc/userguide/stat_userguide.pdf" target="_blank">https://github.com/LLNL/STAT/blob/develop/doc/userguide/stat_userguide.pdf</a></li>
<li>LC web pages: <a href="https://hpc.llnl.gov/software/development-environment-software/stat-stack-trace-analysis-tool" target="_blank">https://hpc.llnl.gov/software/development-environment-software/stat-stack-trace-analysis-tool</a></li>
<li>Sierra usage notes: <a href="https://lc.llnl.gov/confluence/pages/viewpage.action?pageId=544145673" target="_blank">https://lc.llnl.gov/confluence/pages/viewpage.action?pageId=544145673</a> (internal LC wiki)</li>
<li>STAT man page or help menu: <span class="fixed">man stat-gui</span> or <span class="fixed">stat-gui -h</span> (note: you may need to<span class="fixed"> module load stat</span> to get your MANPATH properly set).</li>
</ul><h3><a name="CoreFiles" id="CoreFiles"></a>Core Files</h3>
<h4>Overview</h4>
<ul><li>TotalView can be used to debug core files. This topic is discussed in detail at: <a href="https://computing.llnl.gov/tutorials/totalview/part2.html#CoreFile" target="_blank">https://computing.llnl.gov/tutorials/totalview/part2.html#CoreFile</a>.</li>
<li>For Sierra systems, there are also hooks in place that inform jsrun to dump core files for GPU or CPU exceptions.</li>
<li>These core files can be full core files or lightweight core files.</li>
<li>LC has created options that can be used with the <span class="fixed">jsrun</span> and <span class="fixed">lrun</span> commands to specify core file generation and format. Use the --help flag to view. For example: <br /><table class="table table-bordered"><tr><td>
<pre>% <span class="text-danger">jsrun --help</span>

<em>&lt;snip&gt;</em>

LLNL-specific jsrun enhancements from wrapper:
  --core=&lt;format&gt;       Sets both CPU &amp; GPU coredump env vars to &lt;format&gt;
  --core_cpu=&lt;format&gt;   Sets LLNL_COREDUMP_FORMAT_CPU to &lt;format&gt;
  --core_gpu=&lt;format&gt;   Sets LLNL_COREDUMP_FORMAT_GPU to &lt;format&gt;
    where &lt;format&gt; may be core|lwcore|none|core=&lt;mpirank&gt;|lwcore=&lt;mpirank&gt;
  --core_delay=&lt;secs&gt;   Set LLNL_COREDUMP_WAIT_FOR_OTHERS to &lt;secs&gt;
  --core_kill=&lt;target&gt;  Set LLNL_COREDUMP_KILL to &lt;target&gt;
    where &lt;target&gt; may be task|step|job (defaults to task)</pre></td>
<td>
<pre>% <span class="text-danger">lrun --help</span>

<em>&lt;snip&gt;</em>

--core=&lt;format&gt;      Sets both CPU &amp; GPU coredump env vars to &lt;format&gt;
  --core_delay=&lt;secs&gt;  Set LLNL_COREDUMP_WAIT_FOR_OTHERS to &lt;secs&gt;
  --core_cpu=&lt;format&gt;  Sets LLNL_COREDUMP_FORMAT_CPU to &lt;format&gt;
  --core_gpu=&lt;format&gt;  Sets LLNL_COREDUMP_FORMAT_GPU to &lt;format&gt;
       where &lt;format&gt;  may be core|lwcore|none|core=&lt;mpirank&gt;|lwcore=&lt;mpirank&gt;</pre></td>
</tr></table></li>
<li>LC has also the <span class="fixed">stat-core-merger</span> utility that can be used to merge and view these core files using STAT.
<ul><li>For usage information, simply type <span class="fixed">stat-core-merge</span></li>
<li>Example: <br /><pre>% <span class="text-danger">stat-core-merger -x a.out -c core.*</span>
merging 3 trace files
066%... done!
outputting to file "STAT_merge.dot" ...done!
outputting to file "STAT_merge_1.dot" ...done!
View the outputted .dot files with `STATview`

% <span class="text-danger">stat-view STAT_merge.dot STAT_merge_1.dot</span></pre></li>
</ul></li>
</ul><h2><a name="Performance" id="Performance"></a>Performance Analysis Tools</h2>
<p>For information on available Performance Analysis Tools, please see the following sources:</p>
<p>Development Environment Software:  <a href="https://hpc.llnl.gov/software/development-environment-software" target="_blank">https://hpc.llnl.gov/software/development-environment-software</a></p>
<p>Code Development Tools on LC's Confluence Wiki: <a href="https://lc.llnl.gov/confluence/display/SIERRA/Code+Development+Tools" target="_blank">https://lc.llnl.gov/confluence/display/SIERRA/Code+Development+Tools</a>  (requires authentication)</p>
<h2><a name="Evaluation" id="Evaluation"></a>Tutorial Evaluation Form</h2>
<p><span>We welcome your evaluation and comments on this tutorial.</span></p>
<p><strong>Please complete the <a href="https://hpc.llnl.gov/training/tutorials/evaluation-form" target="_blank">online evaluation form</a></strong></p>
<p>Thank you!</p>
<h2><a name="References" id="References"></a>References &amp; Documentation</h2>
<ul><li>Author: Blaise Barney, Lawrence Livermore National Laboratory.</li>
<li>Ray cluster photos: Randy Wong, Sandia National Laboratories.</li>
<li>Sierra cluster photos: Adam Bertsch and Meg Epperly, Lawrence Livermore National Laboratory.</li>
</ul><h3>Livermore Computing General Documentation</h3>
<ul><li>Livermore Computing user web pages: <a href="https://hpc.llnl.gov" target="_blank">https://hpc.llnl.gov</a></li>
<li>MyLC Livermore Computing user portal: <a href="https://mylc.llnl.gov" target="_blank">mylc.llnl.gov</a></li>
<li>Livermore Computing tutorials: <a href="https://hpc.llnl.gov/training/tutorials" target="_blank">https://hpc.llnl.gov/training/tutorials</a></li>
</ul><h3>CORAL Early Access systems, POWER8, NVIDIA Pascal</h3>
<ul><li><a href="https://www.redbooks.ibm.com/abstracts/redp5405.html?Open" target="_blank">"IBM Power System S822LC for High Performance Computing Introduction and Technical Overview"</a> IBM Redpaper publication REDP-5405-00 by Alexandre Bicas Caldeira, Volker Haug, Scott Vetter. September, 2016.</li>
<li><a href="https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf" target="_blank">"NVIDIA Tesla P100"</a>. NVIDIA Whitepaper. 2016.</li>
<li><a href="https://docs.nvidia.com/cuda/#axzz4ev5JbmWC" target="_blank">NVIDIA CUDA Toolkit</a> documentation</li>
<li>CORAL Early Access systems user information (internal LC wiki):  <a href="https://lc.llnl.gov/confluence/display/CORALEA/CORAL+EA+Systems" target="_blank">https://lc.llnl.gov/confluence/display/CORALEA/CORAL+EA+Systems</a></li>
</ul><h3>Sierra systems, POWER9, NVIDIA Volta</h3>
<ul><li><a href="https://www.redbooks.ibm.com/redpieces/abstracts/redp5472.html?Open" target="_blank">IBM Power System AC922 Introduction and Technical Overview</a>. IBM Redbook publication REDP-5472-00 by Alexandre Bicas Caldeira. March 2018.</li>
<li><a href="https://www.redbooks.ibm.com/redbooks/pdfs/sg248280.pdf" target="_blank">"Implementing an IBM High-Performance Computing Solution on IBM Power System S822LC"</a>. IBM Redbook publication SG24-8280-00. July 2016.</li>
<li><a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" target="_blank">"NVIDIA Tesla V100 GPU Architecture"</a>. NVIDIA Whitepaper. August 2017.</li>
<li><a href="https://docs.nvidia.com/cuda/#axzz4ev5JbmWC" target="_blank">NVIDIA CUDA Toolkit</a> documentation</li>
<li>Sierra systems user information (internal LC wiki):  <a href="https://lc.llnl.gov/confluence/display/SIERRA/Sierra+Systems" target="_blank">https://lc.llnl.gov/confluence/display/SIERRA/Sierra+Systems</a></li>
</ul><h3>LSF Documentation</h3>
<ul><li><a href="https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/New%20IBM%20Platform%20LSF%20Wiki/page/LSF%20documentation" target="_blank">IBM Spectrum LSF online documentation</a></li>
<li><a href="https://www.ibm.com/docs/en/spectrum-lsf/10.1.0" target="_blank">IBM Knowledge Center LSF documentation</a></li>
<li>LC's LSF documents located at: <a href="https://hpc.llnl.gov/banks-jobs/running-jobs" target="_blank">https://hpc.llnl.gov/banks-jobs/running-jobs</a></li>
</ul><h3>Compilers and MPI Documentation</h3>
<ul><li>XLC/C++: Select the relevant version of Little Endian documents at <a href="https://www-01.ibm.com/support/docview.wss?uid=swg27036675" target="_blank">https://www-01.ibm.com/support/docview.wss?uid=swg27036675</a></li>
<li>XLF: Select the relevant version of Little Endian documents at <a href="https://www-01.ibm.com/support/docview.wss?uid=swg27036672" target="_blank">https://www-01.ibm.com/support/docview.wss?uid=swg27036672</a></li>
<li>IBM White Paper "Code Optimization with the IBM XL compilers on Power Architectures": <a href="https://www-01.ibm.com/support/docview.wss?uid=swg27005174&amp;aid=1" target="_blank">https://www-01.ibm.com/support/docview.wss?uid=swg27005174&amp;aid=1</a></li>
<li>IBM MPI Spectrum Documentation at: <a href="https://www.ibm.com/docs/en/spectrum-lsf/10.1.0" target="_blank">https://www.ibm.com/docs/en/spectrum-lsf/10.1.0</a><br />Quick start guide, release notes, installation, user guide and more.</li>
<li>GNU compiler online documentation at: <a href="https://gcc.gnu.org/onlinedocs/" target="_blank">https://gcc.gnu.org/onlinedocs/</a></li>
<li>PGI Compilers - select OpenPOWER docs: <a href="https://www.pgroup.com/resources/docs/" target="_blank">https://www.pgroup.com/resources/docs/</a></li>
</ul><h2><a name="Quickstart" id="Quickstart"></a>Appendix A:  Quickstart Guide</h2>
<p><span>This section provides both a "Lightning-quick" and "Detailed" Quickstart Guide.  For more information, see the relevant sections in the full tutorial.</span></p>
<h3>Lightning-quick Quickstart Guide</h3>
<ol><li>If you cannot find what you need on these pages, the LC Hotline &lt;<a href="mailto:lc-hotline@llnl.gov">lc-hotline@llnl.gov</a>&gt;,  925-422-4531, can help!</li>
<li>Use <span class="fixed">lsfjobs</span> to find the state of the job queue.</li>
<li>Use <span class="fixed">news job.lim.<em>&lt;machinename&gt;</em></span> to see the job queue limits for a machine. For example on sierra:  <span class="fixed">news job.lim.sierra</span></li>
<li>Use<span class="fixed"> lalloc <em>&lt;number of nodes&gt;</em></span> to get an interactive allocation and a shell on the first allocated compute node. For example,  allocate 2 nodes for 30 minutes in the pdebug queue:<br /><span class="fixed">lalloc 2 -W 30 -q pdebug</span></li>
<li>Use <span class="fixed">bsub -nnodes <em>&lt;number of nodes&gt;</em> myscript</span> to run a batch job script on the first allocated compute node</li>
<li>Query your bank usage with command: <span class="fixed">lshare -u <em>&lt;user_name&gt;</em></span> on Lassen or Sierra (not on Rzansel, Ray, Rzmanta or Shark)</li>
<li>Always build with and use the default MPI (<span class="fixed">spectrum-mpi/rolling-release</span>) unless specifically told otherwise. </li>
<li>Running jobs using <span class="fixed">lrun</span> is recommended (but <span class="fixed">jsrun</span> and the <span class="fixed">srun</span> emulator are the other options): Syntax:<br /><span class="fixed">lrun -n <em>&lt;ntasks&gt;</em>|-T <em>&lt;ntasks_per_node&gt;</em> [-N <em>&lt;nnodes&gt;</em>] [ <em>many more options</em>] &lt;app&gt; [app-args]</span></li>
<li>Run <span class="fixed">lrun</span> with no args for detailed help.  Add <span class="fixed">-v</span> to see the jsrun invocation that lrun generates.</li>
<li>The easy way to use <span class="fixed">lrun</span> is to specify tasks per node with the <span class="fixed">-T</span> option and let lrun figure out the number of ranks from the allocation. For example: <span class="fixed">lrun -T4 hello.exe</span> will run 4 ranks in a 1 node allocation and 16 tasks evenly distributed on a 4 node allocation</li>
<li><span class="fixed">lrun -T1 hostname | sort</span> gets you the list of nodes you were allocated</li>
<li>Use the<span class="fixed"> -M "-gpu"</span> option to use GPUDirect with device or managed memory buffers. No CUDA API calls (including cudaMallocManaged) are permitted before the MPI_Init call or you may get the wrong answer!</li>
<li>Don't build big codes on a login or launch node (basically don't slam any node with other users on it).  Use <span class="fixed">bsub</span> or <span class="fixed">lalloc</span> to get a dedicated compute node before running <span class="fixed">make -j</span>.  </li>
<li>The<span class="fixed"> -m "launch_hosts sierra24"</span> option of bsub requests a particular node or nodes (compute node sierra24 in this case)</li>
<li>To submit a 2048 node job to the pbatch queue with core isolation and 4 ranks per node for 24 hours:<br /><span class="fixed">bsub -nnodes 2048 -W 24:00 -G pbronze -core_isolation 2 -q pbatch lrun -T4 &lt;executable&gt; &lt;args&gt;</span></li>
<li>You can check your node(s) using <span class="fixed">check_sierra_nodes</span> (but you are unlikely to find bad nodes at this point)</li>
<li>Use <span class="fixed">lrun --smt=4 &lt;options&gt;</span> to use 4 hardware threads per core.</li>
</ol><h2>Detailed Quickstart Guide</h2>
<h3>Table of Contents</h3>
<ol><li><a href="#quick01">How to get help from an actual human</a></li>
<li><a href="#quick02">If direct ssh to LASSEN or SIERRA fails, login from somewhere inside LLNL first</a></li>
<li><a href="#quick03">First time LASSEN/SIERRA/RZANSEL users should verify their default bank and ssh key setup first</a></li>
<li><a href="#quick04">Use  lsfjobs to see machine state</a><br /><a href="#quick04">Use news job.lim.&lt;machinename&gt; to see queue limits</a></li>
<li><a href="#quick05">Allocate interactive nodes with lalloc</a></li>
<li><a href="#quick06">Known issue running on the first backend compute node (12 second X11 GUI startup, Error initializing RM connection, --stdio_stderr --stdio_stdout broken)</a></li>
<li><a href="#quick07">How to start a 'batch xterm' on CORAL</a></li>
<li><a href="#quick08">Disabling core isolation with bsub -core_isolation 0 (and the one minute node state change)</a></li>
<li><a href="#quick09">The occasional one minute bsub startup and up to five minute bsub teardown times seen in lsfjobs output</a></li>
<li><a href="#quick10">Batch scripts with bsub and a useful bsub scripts trick</a></li>
<li><a href="#quick11">How to run directly on the shared batch launch node instead of the first compute node</a></li>
<li><a href="#quick12">Should MPI jobs be launched with lrun, jsrun, the srun emulator, mpirun, or flux?</a></li>
<li><a href="#quick13">Running MPI jobs with lrun (recommended)</a></li>
<li><a href="#quick14">Examples of using lrun to run MPI jobs</a></li>
<li><a href="#quick15">How to see which compute nodes you were allocated</a></li>
<li><a href="#quick16">CUDA-aware MPI and Using Managed Memory MPI buffers</a></li>
<li><a href="#quick17">MPI Collective Performance Tuning</a></li>
<li><a href="#quick18">(Deprecated) Documentation for using jsrun directly (multiple threads per core examples)</a></li>
<li><a href="#quick19">(Deprecated) Use 'check_sierra_nodes' to verify you have good nodes</a></li>
<li><a href="#quick20">(Deprecated) How to manually run a detailed passphrase-less ssh key check</a></li>
</ol><h3><br /><a name="quick01" id="quick01"></a>1. How to get help from an actual human</h3>
<p><br />If something is not working right on any machine (CORAL or otherwise), your best bet is to contact the Livermore Computing Hotline, Hours: M-F: 8A-12P,1-4:45P, Email: <a href="mailto:lc-hotline@llnl.gov">lc-hotline@llnl.gov</a>, Phone: 925-422-4531.    For those rare CORAL error messages that ask you to contact the Sierra development environment point of contact John Gyllenhaal (<a href="mailto:gyllen@llnl.gov">gyllen@llnl.gov</a>, (925) 424-5485), please contact John Gyllenhaal and also cc the LC Hotline to track the issues. </p>
<h3><a name="quick02" id="quick02"></a>2. If direct ssh to LASSEN or SIERRA fails, login from somewhere inside LLNL first</h3>
<p><br />We believe you can now login directly to LASSEN (from the internet) and SIERRA (on the SCF network) but if that does not work, tell us!   A workaround is to login to oslic (for LASSEN) or cslic (for SIERRA) first.   As of Aug 2019, RZANSEL can be accessed directly without the need to go through rzgw.llnl.gov first. LANL and Sandia users should start from an iHPC node. Authentication is with your LLNL username and RZ PIN + Token.</p>
<h3><a name="quick03" id="quick03"></a>3. First time LASSEN/SIERRA/RZANSEL users should verify their default bank and ssh key setup first</h3>
<p><br />The two issues new CORAL users typically encounter are 1) not having a compute bank set up or 2) having incompatible ssh keys copied from another machine.   Running the following lalloc command (with a short time limit to allow fast scheduling) will check both and verify you can run an MPI job:</p>
<pre>$ <span class="fixed">lalloc 1 -W 3 check_sierra_nodes</span>
&lt;potentially a bunch of messages about setting up your ssh keys&gt;
+ exec bsub -nnodes 1 -W 3 -Is -XF -core_isolation 2 /usr/tce/packages/lalloc/lalloc-2.0/bin/lexec check_sierra_nodes
Job &lt;389127&gt; is submitted to default queue &lt;pbatch&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;   <em>&lt;--This indicates have bank and ssh keys setup correctly, can hit Control-C if machine really busy</em>
&lt;&lt;Starting on lassen710&gt;&gt;
&lt;&lt;Waiting for JSM to become ready ...&gt;&gt;
&lt;&lt;Redirecting to compute node lassen449, setting up as private launch node&gt;&gt;
STARTED: 'jsrun -r 1 -g 4 test_sierra_node -mpi -q' at Mon Jul 22 14:19:42 PDT 2019
SUCCESS: Returned 0 (all, including MPI, tests passed) at Mon Jul 22 14:19:46 PDT 2019  <em>&lt;--MPI worked for you, you are all set!</em>
logout</pre><p>If you don't have a compute bank set up, you will get a message to contact your computer coordinator:</p>
<pre>+ exec bsub -nnodes 1 -Is -W 60 -core_isolation 2 /usr/tce/packages/lalloc/lalloc-2.0/bin/lexec
You do not have a default group (bank). <em>&lt;--This indicates bank PROBLEM</em>
Please specify a bank with -G option or contact your computer coordinator to request a bank.
A list of computer coordinators is available at https://myconfluence.llnl.gov/pages/viewpage.action?spaceKey=HPCINT&amp;title=Computer+Coordinators
or through the "my info" portlet at https://lc.llnl.gov/lorenz/mylc/mylc.cgi
Request aborted by esub. Job not submitted.</pre><p>If you have passphrases on your ssh keys, you will see something like:</p>
<pre>==&gt; Ah ha! ~/.ssh/id_rsa encrypted with passphrase, likely the problem!
    Highly recommend using passphrase-less keys on LC to minimize issues

<span class="text-danger">Error: Passphrase-less ssh keys not set up properly for LC CORAL clusters</span>  <em>&lt;--This indicates ssh keys PROBLEM</em>
       You can remove an existing passphrase by running 'ssh-keygen -p',
         selecting your ssh key (i.e., .ssh/id_rsa), entering your current passphrase,
         and hitting enter for your new passphrase. 
       lalloc/lrun/bsub/jsrun will likely fail with mysterious errors</pre><p>Typically removing an existing passphrase by running <span class="fixed">ssh-keygen -p</span>, selecting your ssh key (i.e., .ssh/id_rsa), entering your current passphrase, and hitting enter for your new passphrase will solve the problem.    Otherwise contact John Gyllenhaal (<a href="mailto:gyllen@llnl.gov">gyllen@llnl.gov</a>, 4-5485) and cc the LC Hotline <a href="mailto:lc-hotline@llnl.gov">lc-hotline@llnl.gov</a> for help with ssh key setup.    Having complicated .ssh/config setups can also break ssh keys.</p>
<h3><a name="quick04" id="quick04"></a>4. Use <span class="fixed-light">lsfjobs</span> to see machine state<br />Use <span class="fixed-light">news job.lim.&lt;machinename&gt;</span> to see queue limits</h3>
<p><br />Use  the <span class="fixed">lsfjobs</span> command to see what is running, what is queued and what is available on the machine. See the <a href="#lsfjobs">lsfjobs</a> section for details.</p>
<pre>sierra4368$  lsfjobs
&lt;snip&gt;
*******************************************************************************************************
* QUEUE           NODE GROUP      Total   Down    Busy   Free  NODES                                  *
*******************************************************************************************************
   -               debug_hosts        36      1       0     35  sierra[361-396]
   -               batch_hosts       871     14     212    645  sierra[397-531,533-612,631-684,703-720,1081-1170,1189-1440,1819-2060]
&lt;snip&gt;</pre><div>As with all LC clusters, you can quickly view the job queue limits for a given machine by using the command  <strong>news  job.lim.<em>&lt;machinename&gt;</em></strong>.  For example, on sierra:  <strong>news  job.lim.sierra.</strong>
<p>Queue limits are also available on the web via the MyLC Portal:</p>
<ul><li>mylc.llnl.gov</li>
<li>Click on a machine name in the "machine status" portlet, or the "my accounts" portlet.</li>
<li>Then select the "details", "topology" and/or "job limits" tabs for detailed hardware and configuration information.</li>
</ul><p>Common queue limits include the maximum number of nodes, maximum time limit, maximum number of running jobs, etc.  Limits are subject to change, and are different for every cluster.</p></div>
<h3><br /><a name="quick05" id="quick05"></a>5. Allocate interactive nodes with lalloc</h3>
<p><br />Use the LLNL-specific <span class="fixed">lalloc</span> bsub wrapper script to facilitate interactive allocations on CORAL and CORAL EA systems.   The first and only required argument is the number of nodes you want followed by optional bsub arguments to pick queue, length of the allocation, etc.  <span class="note-green">Note:</span>by default, all Sierra systems and CORAL EA systems use lalloc/2.0, which uses 'lexec' to place the shell for the interactive allocation on the first compute node of the allocation.   </p>
<p>The lalloc script prints out the exact bsub line used.   For example, 'lalloc 2' will give you 2 nodes with those listed defaults:</p>
<pre>lassen708{gyllen}2: lalloc 2
+ exec bsub -nnodes 2 -Is -XF -W 60 -G guests -core_isolation 2 /usr/tce/packages/lalloc/lalloc-2.0/bin/lexec
Job &lt;3564&gt; is submitted to default queue &lt;pbatch&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on lassen710&gt;&gt;
&lt;&lt;Redirecting to compute node lassen90, setting up as private launch node&gt;&gt;</pre><p>Run 'lalloc' with no arguments for usage info. Here is the current usage info for lalloc/2.0 as of 7/19/19:</p>
<pre>Usage: lalloc #nodes &lt;--shared-launch&gt; &lt;--quiet&gt; &lt;supported bsub opts&gt; &lt;command&gt;
Allocates nodes interactively on LLNL's CORAL and CORAL EA systems
and executes a shell, or the optional &lt;command&gt;, on the first compute node
(which is set up as a private launch node) instead of a shared launch node

lalloc specific options:
--shared-launch Use shared launch node instead of a private launch node
--quiet Suppress bsub and lalloc output (except on errors)

Supported bsub options:
-W minutes Allocation time in minutes (default: 60)
-q queue Queue to use (default: system default queue)
-core_isolation # Cores per socket used for system processes (default: 2)
-G group Bsub fairshare scheduling group (former default: guests)
-Is|-Ip|-I&lt;x&gt; Interactive job mode (default: -Is)
-XF X11 forwarding (default if DISPLAY set)
-stage "bb_opts" Burst buffer options such as "storage=2"
-U reservation Bsub reservation name

Example usage:
lalloc 2 (Gives interactive shell with 2 nodes and above defaults)
lalloc 1 make -j (Run parallel make on private launch node)
lalloc 4 -W 360 -q pbatch lrun -n 8 ./parallel_app -o run.out

Please report issues or missing bsub options you need supported to
John Gyllenhaal (gyllen@llnl.gov, 4-5485)</pre><h3><a name="quick06" id="quick06"></a>6. Known issue running on the first backend compute node (12 second X11 GUI startup, Error initializing RM connection, --stdio_stderr --stdio_stdout broken)</h3>
<p><br />As of Aug 2019, there are three known issues with running on the first backend node (the new default for bsub and lalloc).   One workaround is to use --shared-launch to land on the shared launch node (but please don't slam this node with builds, etc.).</p>
<p>1) Some MPI errors cause allocation daemons to die, preventing future lrun/jsrun invocations from working (gives messages like: Could not find the contact information for the JSM daemon. and: Error initializing RM connection. Exiting.).   You must exit the lalloc shell and do another lalloc to get a working allocation.   As of February 2019, this is a much rarer problem but we still get some reports of issues when hitting Control-C.   Several fixes for these problems are expected in the September 2019 update.</p>
<p>2) The lrun/jsrun options --stdio_stderr --stdio_stdout options don't work at all on the backend nodes.  Either don't use them or use --shared-launch to run lrun and jsrun on the launch node.   Expected to be fixed in September 2019 update.</p>
<p>3) Many X11 GUI programs (gvim, memcheckview, etc.) have a 12 second delay the first time they are invoked.   Future invocations in the same allocation work fine.   Sometimes, the allocation doesn't exit properly after typing 'exit' until control-C is hit.   This is caused the the startup of dbus-daemon, which is commonly used by graphics programs.   We are still exploring solutions to this.</p>
<h3><a name="quick07" id="quick07"></a>7. How to start a 'batch xterm' on CORAL</h3>
<p><br />You can run commands interactively with lalloc (like xterm) and you can make lalloc silent with the --quiet option.   So an easy way to start a 'batch xterm' is:</p>
<p><span class="fixed">lalloc 1 -W 60 --quiet xterm -sb &amp;</span></p>
<p>Your allocation will go away when the xterm is exited.   Your xterm will go away when the allocation ends.</p>
<h3><a name="quick08" id="quick08"></a>8. Disabling core isolation with bsub -core_isolation 0 (and the one minute node state change)</h3>
<p><br />As of February 2019, '-core_isolation 2' is the default behavior if -core_isolation is not specified on the bsub line.  This isolates all the system processes (including GPFS daemons) to 4 cores per node (2 per socket).    With 4 cores per node dedicated for system processes, we believe there should be relatively little impact on GPFS performance (except perhaps if you are running the ior benchmark).   You may explicitly disable core isolation by specifying '-core_isolation 0' on the bsub or lalloc line but we don't recommend it .</p>
<h3><a name="quick09" id="quick09"></a>9. The occasional one minute bsub startup and up to five minute bsub teardown times seen in lsfjobs output</h3>
<p><br />When bsub allocation starts (lsfjobs shows state as 'running'), the core_isolation mode state is checked against the requested mode.   If the node modes are different, it takes about 1 minute to set up the nodes(s) in the new core_isolation mode.   So if the previous user of one or nodes used a different core_isolation setting than your run, you will get a mysterious 1 minute delay before your job actually starts running.    This is why we recommend everyone stay with the default -core_isolation 2 setting.</p>
<p>After the bsub allocation ends, we run more than 50 node health checks before returning the node for use in a new allocation.   These tests require all running user processes to terminate first and if the user processes are writing to disk over the network, it sometimes takes a few minutes for them to terminate.    We have a 5 minute timeout waiting for tasks to end before we give up and drain the node for a sysadmin to look at.   This is why it is not uncommon to have to wait 15 to 120 seconds before all the nodes for an allocation are actually released.</p>
<h3><a name="quick10" id="quick10"></a>10. Batch scripts with bsub and a useful bsub scripts trick</h3>
<p><br />The only way to submit batch jobs is 'bsub'.    You may specify a bsub script at the end of the bsub command line, put a full command on the end of the bsub command line, or pipe a bsub script into stdin.   As of June 2019 (on LASSEN and RZANSEL only), this script will run on the first compute node of your allocation (see next section for more details).</p>
<p>For example, a batch shell script can be submitted via:</p>
<p><span class="fixed">bsub -nnodes 32 -W 360 myapp.bsub</span></p>
<p>or equivalently</p>
<p><span class="fixed">bsub -nnodes 32 -W 360 &lt; myapp.bsub</span></p>
<p>In both cases, additional bsub options may be specified in the script via one or more '#BSUB &lt;list of bsub options&gt;' lines.</p>
<p>It is often useful to have a script that submits bsub scripts for you.    It is often convenient to use the 'cat &lt;&lt; EOF'  trick to embed the bsub script you wish to pipe in to stdin in your script.     Here is an example of this technique:</p>
<pre>sierra4359{gyllen}52: cat do_simple_bsub
#!/bin/sh
cat &lt;&lt; EOF | bsub -nnodes 32 -W 360
#!/bin/bash <em>&lt;-- optionally set shell language, bash default</em>
#BSUB -core_isolation 2 -G guests -J "MYJOB1"
cd ~/debug/hasgpu
lrun -T 4 ./mpihasgpu arg1 arg2
EOF

sierra4359{gyllen}53: ./do_simple_bsub
Job &lt;143505&gt; is submitted to default queue .</pre><h3><a name="quick11" id="quick11"></a>11. How to run directly on the shared batch launch node instead of the first compute node</h3>
<p><br />As of June 2019, LASSEN's and RZANSEL's bsub by default runs your bsub script on the first compute node (like SLURM does), to prevent users from accidentally slamming and crashing the shared launch node.     Although it is no longer the default behavior, you are welcome to continue to use the shared launch node to launch jobs (but please don't build huge codes on the shared launch node or the login nodes).    To get access back to the shared launch node, use the new LLNL-specific option '--shared-launch' with either bsub or lalloc.    To force the use of the first compute node, use '--private-launch' with either bsub or lalloc. </p>
<h3><a name="quick12" id="quick12"></a>12. Should MPI jobs be launched with lrun, jsrun, the srun emulator, mpirun, or flux?</h3>
<p><br />The CORAL contract required IBM to develop a new job launcher (jsrun) with a long list of powerful new features to support running regression tests, UQ runs, and very complex job launch configurations that was missing in SLURM's srun, the job launcher on all of LLNL's other supercomputers.   IBM's jsrun delivered all the power we required at the cost of a more complex interface that is very different than the interface for SLURM's srun.     This more complex jsrun interface makes a lot of sense if you need all of its power (and the complexity is unavoidable), but many of our user's use cases do not need all this power.    For this reason, 'lrun' was written by LLNL as a wrapper over jsrun to provide an srun-like interface to jsrun that captures perhaps 95% of the use cases.   Later, LLNL wrote a 'srun' emulator that provides an exact srun interface (for a common subset of srun options) that captures perhaps 80% of the our user's use cases (and uses lrun and thus jsrun under the covers).   In parallel, LLNL also developed flux, a powerful new job scheduler that has a different portable solution for all those features missing in SLURM and it can run on all LLNL supercomputers.   Lastly, the old 'mpirun' command still exists but is mostly broken and should be not be used unless you have a truly compelling need to do so .</p>
<p><strong>Recommendations:</strong></p>
<p>Use 'lrun' for almost all use cases.   It does very good default binding and layout of runs, including for regression tests (use the --pack, -c, and -g options) and UQ (use the -N option) runs.  The lrun command defaults to a node-schedule mode (unless --pack option used), unlike jsrun and srun, so simultaneous job steps will not share nodes by default (which is typically what you want for UQ).   In '--pack' mode (regression test mode), uses jsrun's enhanced binding algorithm (designed for regression tests) instead of mpibind.</p>
<p>Use 'jsrun' only if you need complete control of MPI task placement/resources or if you want to run the same bsub script on ORNL's SUMMIT cluster (or other non-LLNL CORAL clusters).   The jsrun command defaults to core-scheduled mode (like srun does), so concurent jobs will shared nodes unless the specified resource constraints prevent it.</p>
<p>Use flux (contact the flux team) if you want a regression test or UQ solution that can run on all LLNL supercomputers, not just CORAL.  The 'flux' system has a scalable python interface for submitting a large number of jobs with exactly the layout desired that is portable to all LLNL machines (and eventually all schedulers).</p>
<p>Use 'srun' if you want an actual srun interface for regression tests or straightforward one-at-a-time runs.   Not a good match for UQ runs (non-trivial to prevent overlapping simultaneous job steps on the same node) and srun will punt if you use an unsupported options (emulator does not support manual placement options, use jsrun for that).  The srun command defaults to core-scheduled mode and using mpibind (uses lrun with --pack --mpibind=on by default), so simultaneous job steps will share nodes by default.</p>
<p>Do NOT use 'mpirun' unless one of the above solutions does not work and you really know what you are doing (takes &gt; 100 character of options ot make mpirun work right on CORAL and not crash the machine).    Some science run users use mpirun combined with flux, so mpirun is allowed on compute nodes but will not run by default on login or launch nodes.</p>
<h3><a name="quick13" id="quick13"></a>13. Running MPI jobs with 'lrun' (recommended)</h3>
<p><br />In most cases (as detailed above) we recommend you use the LC-written 'lrun' wrapper for jsrun, instead of using jsrun directly to launch jobs on the backend compute nodes.   By default, lrun uses node-scheduling (job steps will not share nodes) unlike jsrun or the srun emulator, which is good for single runs and UQ runs.   If you wish to run multiple simultaneous  job steps on the same nodes for regression tests, use the --pack option and specify cpus-per-task and gpus-per-task with -c and -g.   If you wish to use multiple threads per core, use the --smt option or specify the desired number of threads with OMP_NUM_THREADS.     Running lrun no arguments will give the following help text (as of July 2019):</p>
<pre>Usage: lrun -n &lt;ntasks&gt; | -T &lt;ntasks_per_node&gt; | -1 \
[-N &lt;nnodes&gt;] [--adv_map] [--threads=&lt;nthreads&gt;] [--smt=&lt;1|2|3|4&gt;] \
[--pack] [-c &lt;ncores_per_task&gt;] [-g &lt;ngpus_per_task&gt;] \
[-W &lt;time_limit&gt; [--bind=off] [--mpibind=off|on] [--gpubind=off] \
[--core=&lt;format&gt;] [--core_delay=&lt;secs&gt;] \
[--core_gpu=&lt;format&gt;] [--core_cpu=&lt;format&gt;] \
[-X &lt;0|1&gt;] [-v] [-vvv] [&lt;compatible_jsrun_options&gt;] \
&lt;app&gt; [app-args]

Launches a job step in a LSF node allocation with a srun-like interface.
By default the resources for the entire node are evenly spread among MPI tasks.
Note: for 1 task/node, only one socket is bound to unless --bind=off used.
Multiple simultaneous job steps may now be run in allocation for UQ, etc.
Job steps can be packed tightly into nodes with --pack for regression testing.

AT LEAST ONE OF THESE LRUN ARGUMENTS MUST BE SPECIFIED FOR EACH JOB STEP:
-n &lt;ntasks&gt; Exact number of MPI tasks to launch
-T &lt;ntasks_per_node&gt; Layout ntasks/node and if no -n arg, use to calc ntasks
-1 Run serial job on backend node (e.g. lrun -1 make)
-1 expands to '-N 1 -n 1 -X 0 --mpibind=off'

OPTIONAL LRUN ARGUMENTS:
-N &lt;nnodes&gt; Use nnodes nodes of allocation (default use all nodes)
--adv_map Improved mapping but simultaneous runs may be serialized
--threads=&lt;nthreads&gt; Sets env var OMP_NUM_THREADS to nthreads
--smt=&lt;1|2|3|4&gt; Set smt level (default 1), OMP_NUM_THREADS overrides
--pack Pack nodes with job steps (defaults to -c 1 -g 0)
--mpibind=on Force use mpibind in --pack mode instead of jsrun's bind
-c &lt;ncores_per_task&gt; Required COREs per MPI task (--pack uses for placement)
-g &lt;ngpus_per_task&gt; Required GPUs per MPI task (--pack uses for placement)
-W &lt;time_limit&gt; Sends SIGTERM to jsrun after minutes or H:M or H:M:S
--bind=off No binding/mpibind used in default or --pack mode
--mpibind=off Do not use mpibind (disables binding in default mode)
--gpubind=off Mpibind binds only cores (CUDA_VISIBLE_DEVICES unset)
--core=&lt;format&gt; Sets both CPU &amp; GPU coredump env vars to &lt;format&gt;
--core_delay=&lt;secs&gt; Set LLNL_COREDUMP_WAIT_FOR_OTHERS to &lt;secs&gt;
--core_cpu=&lt;format&gt; Sets LLNL_COREDUMP_FORMAT_CPU to &lt;format&gt;
--core_gpu=&lt;format&gt; Sets LLNL_COREDUMP_FORMAT_GPU to &lt;format&gt;
where &lt;format&gt; may be core|lwcore|none|core=&lt;mpirank&gt;|lwcore=&lt;mpirank&gt;
-X &lt;0|1&gt; Sets --exit_on_error to 0|1 (default 1)
-v Verbose mode, show jsrun command and any set env vars
-vvv Makes jsrun wrapper verbose also (core dump settings)

JSRUN OPTIONS INCOMPATIBLE WITH LRUN (others should be compatible):
-a, -r, -m, -l, -K, -d, -J (and long versions like --tasks_per_rs, --nrs)
Note: -n, -c, -g redefined to have different behavior than jsrun's version.

ENVIRONMENT VARIABLES THAT LRUN/MPIBIND LOOKS AT IF SET:
MPIBIND_EXE &lt;path&gt; Sets mpibind used by lrun, defaults to:
/usr/tce/packages/lrun/lrun-2019.05.07/bin/mpibind10
OMP_NUM_THREADS # If not set, mpibind maximizes based on smt and cores
OMP_PROC_BIND &lt;mode&gt; Defaults to 'spread' unless set to 'close' or 'master'
MPIBIND &lt;j|jj|jjj&gt; Sets verbosity level, more j's -&gt; more output

Spaces are optional in single character options (i.e., -T4 or -T 4 valid)
Example invocation: lrun -T4 js_task_info

Written by Edgar Leon and John Gyllenhaal at LLNL.
Please report problems to John Gyllenhaal (gyllen@llnl.gov, 4-5485)</pre><h3><br /><a name="quick14" id="quick14"></a>14. Examples of using lrun to run MPI jobs</h3>
<p><br />JSM includes the utility program 'js_task_info' that provides great binding and mapping info, but it is quite verbose.  Much of the output below is replaced with '...' for readability.</p>
<p>If you have a 16 node allocation, you can restrict the nodes lrun uses with the -N &lt;nodes&gt; option, for example, on one node:</p>
<pre>$ <span class="text-danger">lrun -N 1 -n 4 js_task_info | &amp; sort</span>
Task 0 ... cpu[s] 0,4,... on host sierra1301 with OMP_NUM_THREADS=10 and with OMP_PLACES={0},{4},... and CUDA_VISIBLE_DEVICES=0
Task 1 ... cpu[s] 40,44,... on host sierra1301 with OMP_NUM_THREADS=10 and with OMP_PLACES={40},{44},... and CUDA_VISIBLE_DEVICES=1
Task 2 ... cpu[s] 88,92,... on host sierra1301 with OMP_NUM_THREADS=10 and with OMP_PLACES={88},{92},... and CUDA_VISIBLE_DEVICES=2
Task 3 ... cpu[s] 128,132,... on host sierra1301 with OMP_NUM_THREADS=10 and with OMP_PLACES={128},{132},... and CUDA_VISIBLE_DEVICES=3</pre><p>All these examples do binding, since --nolbind was not specified.</p>
<pre>$ <span class="text-danger">lrun -N 3 -n 6 js_task_info | &amp; sort</span>
Task 0 ... cpu[s] 0,4,... on host sierra1301 with OMP_NUM_THREADS=20 and with OMP_PLACES={0},{4},... and CUDA_VISIBLE_DEVICES=0 1
Task 1 ... cpu[s] 88,92,... on host sierra1301 with OMP_NUM_THREADS=20 and with OMP_PLACES={88},{92},... and CUDA_VISIBLE_DEVICES=2 3
Task 2 ... cpu[s] 0,4,... on host sierra1302 with OMP_NUM_THREADS=20 and with OMP_PLACES={0},{4},... and CUDA_VISIBLE_DEVICES=0 1
Task 3 ... cpu[s] 88,92,... on host sierra1302 with OMP_NUM_THREADS=20 and with OMP_PLACES={88},{92},... and CUDA_VISIBLE_DEVICES=2 3
Task 4 ... cpu[s] 0,4,... on host sierra1303 with OMP_NUM_THREADS=20 and with OMP_PLACES={0},{4},... and CUDA_VISIBLE_DEVICES=0 1
|Task 5 ... cpu[s] 88,92,... on host sierra1303 with OMP_NUM_THREADS=20 and with OMP_PLACES={88},{92},... and CUDA_VISIBLE_DEVICES=2 3</pre><p>If you don’t specify -N&lt;nodes&gt;, it will spread things across your whole allocation, unlike the default behavior for jsrun:</p>
<p><span class="fixed">$ lrun  -p6 js_task_info | sort</span></p>
<p>You can specify -T &lt;tasks_per_nodes&gt; instead of -p&lt;tasks&gt;:</p>
<p><span class="fixed">$ lrun -N2 -T4 js_task_info | sort</span></p>
<h3><a name="quick15" id="quick15"></a>15. How to see which compute nodes you were allocated</h3>
<p><br />See what compute nodes you were actually allocated using lrun -T1 :</p>
<pre>$ lrun -T1 hostname | sort
sierra361
sierra362
&lt;snip&gt;</pre><p>NOTE: To ssh to the first backend node, use 'lexec'.   Sshing directly does not set up your environment properly for running lrun or jsrun.</p>
<h3><a name="quick16" id="quick16"></a>16. CUDA-aware MPI and Using Managed Memory MPI buffers</h3>
<p><br />CUDA-aware MPI allows GPU buffers (allocated with cudaMalloc) to be used directly in MPI calls. Without CUDA-Aware MPI data must be copied manually to/from a CPU buffer (using cudaMemcpy) before/after passing data in MPI calls. For example:</p>
<p>Without CUDA-aware MPI - need to copy data between GPU and CPU memory before/after MPI send/receive operations.</p>
<pre><span>//MPI rank 0</span>

cudaMemcpy(sendbuf_h,sendbuf_d,size,cudaMemcpyDeviceToHost);
MPI_Send(sendbuf_h,size,MPI_CHAR,1,tag,MPI_COMM_WORLD);

//MPI rank 1
MPI_Recv(recbuf_h,size,MPI_CHAR,0,tag,MPI_COMM_WORLD, &amp;status);
cudaMemcpy(recbuf_d,recbuf_h,size,cudaMemcpyHostToDevice);
With CUDA-aware MPI - data is transferred directly to/from GPU memory by MPI send/receive operations.</pre><p>With CUDA-aware MPI - data is transferred directly to/from GPU memory by MPI send/receive operations.</p>
<pre>//MPI rank 0
MPI_Send(sendbuf_d,size,MPI_CHAR,1,tag,MPI_COMM_WORLD);

//MPI rank 1
MPI_Recv(recbuf_d,size,MPI_CHAR,0,tag,MPI_COMM_WORLD, &amp;status);</pre><p>IBM Spectrum MPI on CORAL systems is CUDA-aware. However, users are required to "turn on" this feature using a run-time flag with lrun or jsrun. For example:</p>
<p><span class="fixed">lrun -M "-gpu"</span></p>
<p><span class="fixed">jsrun -M "-gpu"</span></p>
<p>Caveat:  Do NOT use the MPIX_Query_cuda_support() routine or the preprocessor constant MPIX_CUDA_AWARE_SUPPORT to determine if MPI is CUDA-aware.  IBM Spectrum MPI will always return false.</p>
<p>Additional Information:</p>
<p>An Introduction to CUDA-Aware MPI:  <a href="https://devblogs.nvidia.com/introduction-cuda-aware-mpi/" target="_blank">https://devblogs.nvidia.com/introduction-cuda-aware-mpi/</a><br />MPI Status Updates and Performance Suggestions:  <a href="https://lc.llnl.gov/confluence/display/SIERRA/Sierra+Systems+User+Meeting?preview=/633381124/640520291/2019.05.09.MPI_UpdatesPerformance.Karlin.pdf">2019.05.09.MPI_UpdatesPerformance.Karlin.pdf</a></p>
<h3><br /><a name="quick17" id="quick17"></a>17. MPI Collective Performance Tuning</h3>
<p><br />MPI collective performance on sierra may be improved by using the Mellanox HCOLL and SHARP functionality, both of which are now enabled by default.  Current benchmarking indicates that using HCOLL can reduce collective latency 10-50% for message sizes larger than 2KiB, while using SHARP can reduce collective latency 50-66% for message sizes up to 2 KiB.  Best performance is observed when using both HCOLL and SHARP.   As of Aug 2018, we believe we do the below by default for users but the mpiP info below may be useful for tuning parameters further for your application. </p>
<ul><li>To enable HCOLL functionality, pass the following flags to your jsrun command:<br /><br />-M "-mca coll_hcoll_enable 1 -mca coll_hcoll_np 0 -mca coll ^basic -mca coll ^ibm -HCOLL -FCA"</li>
</ul><ul><li>To enable SHARP functionality, also pass the following flags to your jsrun command:<br /><br />-E HCOLL_SHARP_NP=2 -E HCOLL_ENABLE_SHARP=2</li>
</ul><ul><li>If you wish ensure that SHARP is being used by your job, set the HCOLL_ENABLE_SHARP environment variable to 3, and your job will fail if it cannot use SHARP.  Your job will generate messages similar to:<br /><br />[sierra2545:94746:43][common_sharp.c:292:comm_sharp_coll_init] SHArP: Fallback is disabled. exiting ...</li>
</ul><ul><li>If you wish to generate SHARP log data indicating SHARP statistics and confirming that SHARP is being used, add -E SHARP_COLL_LOG_LEVEL=3.  This will generate log data similar to:<br /><br />INFO job (ID: 4456568) resource request quota: ( osts:64 user_data_per_ost:256 max_groups:0 max_qps:176 max_group_channels:1, num_trees:1)</li>
</ul><p>To determine MPI collective message sizes used by an application, you can use the mpiP MPI profiler to get collective communicator and message size histogram data.  To do this using the IBM-provided mpiP library, do the following:</p>
<ul><li>Load the mpip module with "module load mpip".</li>
<li>Set the MPIP environment variable to "-y".</li>
<li>Run your application with lrun-mpip instead of lrun.</li>
<li>Your application should create an *.mpiP report file with an "Aggregate Collective Time" section with collective MPI Time %, Communicator size, and message size.</li>
<li>Do not link with "-lmpiP" as this will link with the currently broken IBM mpiP library (as of 10/11/18).</li>
</ul><p>Additional HCOLL environment variables can be found by running "/opt/mellanox/hcoll/bin/hcoll_info --all".  Additional SHARP environment variables can be found here.</p>
<h3><a name="quick18" id="quick18"></a>18. (Deprecated) Documentation for using jsrun directly (multiple threads per core examples)</h3>
<p><br />Note: As of June 2019, the rest of this jsrun section can be replaced with lrun --smt=4, etc. instead of using jsrun and these odd bsub options. </p>
<p>There is jsrun documentation at ORNL that is quite useful: <a href="https://docs.olcf.ornl.gov/systems/summit_user_guide.html#running-jobs" target="_blank">https://docs.olcf.ornl.gov/systems/summit_user_guide.html#running-jobs</a></p>
<p>THIS STILL WORKS BUT NOT RECOMMENDED:</p>
<p>Here are three commonly requested scenarios for using jsrun and bsub for testing the performance of various thread layouts on a node.    Below is the high-performance way to ask for a single node with either 4, 2, or 1 hardware thread per core (the smt4,smt2, or smt1 in the bsub line).    The cpublink option will make the allocation take up to a minute longer to start but you will get more reproducible performance.     .   Run your app where js_task_info is shown (which shows your binding).   All experiments are doing 1 MPI task per GPU (4 tasks per node).   Hopefully with the ORNL documentation (or man jsrun), this will get you started.</p>
<p>jsrun with 40 OpenMP threads per MPI task, 4 tasks per node:</p>
<pre>$ <span class="text-danger">bsub -nnodes 1 -core_isolation 2 -alloc_flags "smt4 cpublink autonumaoff" -Is -XF -W 60 -G guests -q pdebug /bin/tcsh</span>
Job &lt;16813&gt; is submitted to queue &lt;pdebug&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on rzansel62&gt;&gt;

$ <span class="text-danger">setenv OMP_NUM_THREADS 40</span>

% <span class="text-danger">jsrun -n 4 -c 10 -g 1 --bind rs --latency_priority=gpu-cpu,cpu-cpu,cpu-mem js_task_info</span>
Task 0 ( 0/4, 0/4 ) is bound to cpu[s] 0-39 on host rzansel17 with OMP_NUM_THREADS=40 and with OMP_PLACES={0:4},{4:4},{8:4},{12:4},{16:4},{20:4},{24:4},{28:4},{32:4},{36:4} and CUDA_VISIBLE_DEVICES=0
Task 1 ( 1/4, 1/4 ) is bound to cpu[s] 40-79 on host rzansel17 with OMP_NUM_THREADS=40 and with OMP_PLACES={40:4},{44:4},{48:4},{52:4},{56:4},{60:4},{64:4},{68:4},{72:4},{76:4} and CUDA_VISIBLE_DEVICES=1
Task 2 ( 2/4, 2/4 ) is bound to cpu[s] 88-127 on host rzansel17 with OMP_NUM_THREADS=40 and with OMP_PLACES={88:4},{92:4},{96:4},{100:4},{104:4},{108:4},{112:4},{116:4},{120:4},{124:4} and CUDA_VISIBLE_DEVICES=2
Task 3 ( 3/4, 3/4 ) is bound to cpu[s] 128-167 on host rzansel17 with OMP_NUM_THREADS=40 and with OMP_PLACES={128:4},{132:4},{136:4},{140:4},{144:4},{148:4},{152:4},{156:4},{160:4},{164:4} and CUDA_VISIBLE_DEVICES=3</pre><p>jsrun with 20 OpenMP threads per MPI task, 4 tasks per node:</p>
<pre>$ <span class="text-danger">bsub -nnodes 1 -core_isolation 2 -alloc_flags "smt2 cpublink autonumaoff" -Is -XF -W 60 -G guests -q pdebug /bin/tcsh</span>
Job &lt;16822&gt; is submitted to queue &lt;pdebug&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on rzansel62&gt;&gt;

$ <span class="text-danger">setenv OMP_NUM_THREADS 20</span>

$ <span class="text-danger">jsrun -n 4 -c 10 -g 1 --bind rs --latency_priority=gpu-cpu,cpu-cpu,cpu-mem js_task_info</span>
Task 0 ( 0/4, 0/4 ) is bound to cpu[s] 0-1,4-5,8-9,12-13,16-17,20-21,24-25,28-29,32-33,36-37 on host rzansel18 with OMP_NUM_THREADS=20 and with OMP_PLACES={0:2},{4:2},{8:2},{12:2},{16:2},{20:2},{24:2},{28:2},{32:2},{36:2} and CUDA_VISIBLE_DEVICES=0
Task 1 ( 1/4, 1/4 ) is bound to cpu[s] 40-41,44-45,48-49,52-53,56-57,60-61,64-65,68-69,72-73,76-77 on host rzansel18 with OMP_NUM_THREADS=20 and with OMP_PLACES={40:2},{44:2},{48:2},{52:2},{56:2},{60:2},{64:2},{68:2},{72:2},{76:2} and CUDA_VISIBLE_DEVICES=1
Task 2 ( 2/4, 2/4 ) is bound to cpu[s] 88-89,92-93,96-97,100-101,104-105,108-109,112-113,116-117,120-121,124-125 on host rzansel18 with OMP_NUM_THREADS=20 and with OMP_PLACES={88:2},{92:2},{96:2},{100:2},{104:2},{108:2},{112:2},{116:2},{120:2},{124:2} and CUDA_VISIBLE_DEVICES=2
Task 3 ( 3/4, 3/4 ) is bound to cpu[s] 128-129,132-133,136-137,140-141,144-145,148-149,152-153,156-157,160-161,164-165 on host rzansel18 with OMP_NUM_THREADS=20 and with OMP_PLACES={128:2},{132:2},{136:2},{140:2},{144:2},{148:2},{152:2},{156:2},{160:2},{164:2} and CUDA_VISIBLE_DEVICES=3</pre><p>jsrun with 10 OpenMP threads per MPI task, 4 tasks per node:</p>
<pre>$ <span class="text-danger">bsub -nnodes 1 -core_isolation 2 -alloc_flags "smt1 cpublink autonumaoff" -Is -XF -W 60 -G guests -q pdebug /bin/tcsh</span>
Job &lt;16824&gt; is submitted to queue &lt;pdebug&gt;.
&lt;&lt;ssh X11 forwarding job&gt;&gt;
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on rzansel62&gt;&gt;

$ <span class="text-danger">setenv OMP_NUM_THREADS 10</span>

$  <span class="text-danger">jsrun -n 4 -c 10 -g 1 --bind rs --latency_priority=gpu-cpu,cpu-cpu,cpu-mem js_task_info</span>
Task 0 ( 0/4, 0/4 ) is bound to cpu[s] 0,4,8,12,16,20,24,28,32,36 on host rzansel42 with OMP_NUM_THREADS=10 and with OMP_PLACES={0},{4},{8},{12},{16},{20},{24},{28},{32},{36} and CUDA_VISIBLE_DEVICES=0
Task 1 ( 1/4, 1/4 ) is bound to cpu[s] 40,44,48,52,56,60,64,68,72,76 on host rzansel42 with OMP_NUM_THREADS=10 and with OMP_PLACES={40},{44},{48},{52},{56},{60},{64},{68},{72},{76} and CUDA_VISIBLE_DEVICES=1
Task 2 ( 2/4, 2/4 ) is bound to cpu[s] 88,92,96,100,104,108,112,116,120,124 on host rzansel42 with OMP_NUM_THREADS=10 and with OMP_PLACES={88},{92},{96},{100},{104},{108},{112},{116},{120},{124} and CUDA_VISIBLE_DEVICES=2
Task 3 ( 3/4, 3/4 ) is bound to cpu[s] 128,132,136,140,144,148,152,156,160,164 on host rzansel42 with OMP_NUM_THREADS=10 and with OMP_PLACES={128},{132},{136},{140},{144},{148},{152},{156},{160},{164} and CUDA_VISIBLE_DEVICES=3</pre><h3><a name="quick19" id="quick19"></a>19. (Deprecated) Use 'check_sierra_nodes' to verify you have good nodes</h3>
<p><br />As of March, 2019 the CORAL systems now runs 'check_sierra_nodes' and many other detailed health checks after every batch allocation.   As a result, it is now incredibly unlikely that check_sierra_nodes is useful to run except as the first check after you get an account on a CORAL machine.</p>
<p>If you really want to check your nodes again, run check_sierra_nodes with no arguments before your runs, which give output in this form:</p>
<pre>$ check_sierra_nodes
STARTED: 'jsrun -r 1 -g 4 test_sierra_node -mpi -q' at Fri Apr 27 16:38:58 PDT 2018
SUCCESS: Returned 0 (all, including MPI, tests passed) at Fri Apr 27 16:39:03 PDT 2018</pre><p>The last line will start with SUCCESS if no bad nodes were found and the return code will be 0.  </p>
<p>NOTE: Some of the bad node states cause MPI_Init to hang.     The ‘check_sierra_nodes’ runs in approximately 6 seconds on a low number of nodes and in about 70 seconds on 650 nodes.   If your check_sierra_nodes invocation takes more than two minutes (on 650 or less nodes), that also indicates an error condition that needs to be reported.</p>
<p>P. S. Running ‘check_sierra_nodes -h’ provides options for more verbosity and allows for not checking MPI.   The auxiliary ‘test_sierra_node’ program (which is used in the epilogue and we have placed a copy in /usr/tcetmp/bin) can also be run directly on backend sierra nodes to test for problems.</p>
<h3><a name="quick20" id="quick20"></a>20. (Deprecated) How to manually run a detailed passphrase-less ssh key check</h3>
<p><br />In order to use a CORAL system, you must have passphrase-less ssh keys set up on SIERRA, LASSEN or RZANSEL in order to successfully run bsub, lalloc, jsrun, or lrun.   The good news is that as of 2/25/2019, passphrase-less ssh keys are automatically set up for you if you don't have them.   If you already have ssh keys partially setup for bitbucket, etc., those will be made usable if at all possible.   This ssh key setup is non-destructive and will have you contact John Gyllenhaal if it runs into problems (very old ssh keys using deprecated names like authorized_keys2 may need hand tweaking).    This requirement is expected for the lifetime of the CORAL system since the other possible solutions we were planning to switch to turned out to be incompatible with noisy dot files, which many users have.    The bsub/lalloc/jsrun/lrun commands currently uses ssh under the covers to launch daemons and you will get a variety of cryptic error messages if you do not have passphrase-less ssh keys set up (ssh-agent is NOT supported in this mode).   Many error messages are possible if you do not have passphrase-less ssh keys set up but one common one is: Error initializing RM connection. Exiting.    Another error indication is being prompted for a password or passphrase while launching a job.</p>
<p>You can remove an existing passphrase by running 'ssh-keygen -p', selecting your ssh key (i.e., .ssh/id_rsa), entering your current passphrase, and hitting enter for your new passphrase.</p>
<p>To explicitly check your ssh keys verbosely without calling bsub or lalloc, you can run: <span class="fixed">setup_ssh_keys -v</span></p>
<p>If you have ssh keys already set up, you will see something like:</p>
<pre>Testing /g/g0/gyllen permissions (currently drwx------)
Testing /g/g0/gyllen/.ssh permissions (currently 700)
Testing /g/g0/gyllen/.ssh/authorized_keys permissions (currently -rw-------)
Skipping permission check on /g/g0/gyllen/.ssh/id_rsa, doesn't exist
Ssh key test on lassen passed    ← This indicates ssh keys OK
Touching file /g/g0/gyllen/.ssh/setup_ssh_keys_verified to cache</pre><p>You don't have ssh keys already set up, you will see something like:</p>
<pre>Testing /g/g0/testbash permissions (currently drwx------)
Skipping permission check on /g/g0/testbash/.ssh, doesn't exist
Skipping permission check on /g/g0/testbash/.ssh/authorized_keys, doesn't exist
Skipping permission check on /g/g0/testbash/.ssh/id_rsa, doesn't exist
==&gt; ERROR: SSH KEYS do not work sshing to lassen
SSH KEY ~/.ssh/id_rsa.pub not found, CREATING:
Generating public/private rsa key pair.
Your identification has been saved in /g/g0/testbash/.ssh/id_rsa.
Your public key has been saved in /g/g0/testbash/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:ikXc1E+axABc+TA38vM/tQBfEJQCu96rYMWIYCu1zes testbash@lassen709
The key's randomart image is:
+---[RSA 2018]----+
|     ..o+*. .oo  | 
|     ..o= B..o   | 
|   +  o .X *. .  | 
|  o *.. o B..  . | 
| . o +..S+ oo .  | 
|  .  o..o . .o  .| 
|    ...o . . ....| 
|    . . .   . o. | 
|     E   ...   . | 
+----[SHA256]-----+
SSH ~/.ssh/authorized_keys not found, CREATING with id_rsa.pub
Testing /g/g0/testbash/.ssh/authorized_keys permissions (currently -rw-------)
Confirmed properly set up ssh keys, test on lassen passed ← This indicates ssh keys OK
Touching file /g/g0/testbash/.ssh/setup_ssh_keys_verified to cache</pre><p>If you have passphrases on your ssh keys, you will see something like:</p>
<pre>==&gt; Ah ha! ~/.ssh/id_rsa encrypted with passphrase, likely the problem!
    Highly recommend using passphrase-less keys on LC to minimize issues

Error: Passphrase-less ssh keys not set up properly for LC CORAL clusters ← This indicates ssh keys PROBLEM
       lalloc/lrun/bsub/jsrun will likely fail with mysterious errors
       Attempts to automatically fix keys failed, you need a human's help!

  --&gt;  NOTE: If your ssh keys actually work despite setup_ssh_keys test results,
       touching /g/g0/testbash/.ssh/setup_ssh_keys_skip_tests will cause
       all tests to be skipped and warnings silenced in future

  ==&gt;  Please contact John Gyllenhaal for help fixing ssh keys
       Email: gyllen@llnl.gov, LLNL lab phone: (925) 424-5485</pre><p>If you get an error message, please contact John Gyllenhaal as indicated above!</p>
<p><br />If you can ssh into machines but bsub/lalloc/jsrun/lrun still doesn't work, make sure your .ssh/config file does not default to port 622.   </p>
<p>You can remove an existing passphrase by running 'ssh-keygen -p', selecting your ssh key (i.e., .ssh/id_rsa), entering your current passphrase, and hitting enter for your new passphrase.</p>
<p>LLNL-WEB-750771</p>
<p> </p>
</div></div></div>    </div>
  </div>
</div>


<!-- Needed to activate display suite support on forms -->
  </div>
  
</div> <!-- /.block --></div>
 <!-- /.region -->
                   		</div>
                  </main>
                </div>
      		</div>
    	</div>
	</div>
  	
	

    <footer id="colophon" class="site-footer">
        <div class="container">
            <div class="row">
                <div class="col-sm-12 footer-top">

                    <a class="llnl" href="https://www.llnl.gov/" target="_blank"><img src="/sites/all/themes/tid/images/llnl.png" alt="LLNL"></a>
                    <p>
                        Lawrence Livermore National Laboratory
                        <br>7000 East Avenue • Livermore, CA 94550
                    </p>
                    <p>
                        Operated by Lawrence Livermore National Security, LLC, for the
                        <br>Department of Energy's National Nuclear Security Administration.
                    </p>
                    <div class="footer-top-logos">
                        <a class="nnsa" href="https://www.energy.gov/nnsa/national-nuclear-security-administration" target="_blank"><img src="/sites/all/themes/tid/images/nnsa2.png" alt="NNSA"></a>
                        <a class="doe" href="https://www.energy.gov/" target="_blank"><img src="/sites/all/themes/tid/images/doe_small.png" alt="U.S. DOE"></a>
                        <a class="llns" href="https://www.llnsllc.com/" target="_blank"><img src="/sites/all/themes/tid/images/llns.png" alt="LLNS"></a>
                	</div>



                </div>
                <div class="col-sm-12 footer-bottom">
                	

                    <span>UCRL-MI-131558  &nbsp;|&nbsp;&nbsp;</span><a href="https://www.llnl.gov/disclaimer" target="_blank">Privacy &amp; Legal Notice</a>	 &nbsp;|&nbsp;&nbsp; <a href="mailto:webmaster-comp@llnl.gov">Website Query</a> &nbsp;|&nbsp;&nbsp;<a href="/about-us/contact-us" >Contact Us</a>
                </div>
            </div>
        </div>
    </footer>
</div>
  </body>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/jquery_update/replace/jquery/2.1/jquery.min.js?v=2.1.4"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/jquery-extend-3.4.0.js?v=2.1.4"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/jquery-html-prefilter-3.5.0-backport.js?v=2.1.4"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/jquery.once.js?v=1.2"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/drupal.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/extlink/extlink.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/jquery.flexslider.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/slide.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/lightbox2/js/lightbox.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/matomo/matomo.js?qsohrw"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
var _paq = _paq || [];(function(){var u=(("https:" == document.location.protocol) ? "https://analytics.llnl.gov/" : "http://analytics.llnl.gov/");_paq.push(["setSiteId", "149"]);_paq.push(["setTrackerUrl", u+"piwik.php"]);_paq.push(["setDoNotTrack", 1]);_paq.push(["trackPageView"]);_paq.push(["setIgnoreClasses", ["no-tracking","colorbox"]]);_paq.push(["enableLinkTracking"]);var d=document,g=d.createElement("script"),s=d.getElementsByTagName("script")[0];g.type="text/javascript";g.defer=true;g.async=true;g.src="https://hpc.llnl.gov/sites/default/files/matomo/piwik.js?qsohrw";s.parentNode.insertBefore(g,s);})();
//--><!]]>
</script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/bootstrap.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/mobilemenu.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/custom.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/mods.js?qsohrw"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"tid","theme_token":"spavrHrTWP0nN9YXm1RIRJuPQkchLDB27QRAzkuDjXA","js":{"sites\/all\/modules\/contrib\/jquery_update\/replace\/jquery\/2.1\/jquery.min.js":1,"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/modules\/contrib\/extlink\/extlink.js":1,"sites\/all\/themes\/tid\/js\/jquery.flexslider.js":1,"sites\/all\/themes\/tid\/js\/slide.js":1,"sites\/all\/modules\/contrib\/lightbox2\/js\/lightbox.js":1,"sites\/all\/modules\/contrib\/matomo\/matomo.js":1,"0":1,"sites\/all\/themes\/tid\/js\/bootstrap.js":1,"sites\/all\/themes\/tid\/js\/mobilemenu.js":1,"sites\/all\/themes\/tid\/js\/custom.js":1,"sites\/all\/themes\/tid\/js\/mods.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/system\/system.theme.css":1,"modules\/book\/book.css":1,"sites\/all\/modules\/contrib\/date\/date_api\/date.css":1,"sites\/all\/modules\/contrib\/date\/date_popup\/themes\/datepicker.1.7.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/contrib\/extlink\/extlink.css":1,"sites\/all\/modules\/contrib\/views\/css\/views.css":1,"sites\/all\/modules\/contrib\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/contrib\/lightbox2\/css\/lightbox.css":1,"sites\/all\/modules\/contrib\/print\/print_ui\/css\/print_ui.theme.css":1,"sites\/all\/themes\/tid\/css\/bootstrap.css":1,"sites\/all\/themes\/tid\/css\/flexslider.css":1,"sites\/all\/themes\/tid\/css\/system.menus.css":1,"sites\/all\/themes\/tid\/css\/style.css":1,"sites\/all\/themes\/tid\/font-awesome\/css\/font-awesome.css":1,"sites\/all\/themes\/tid\/css\/treewalk.css":1,"sites\/all\/themes\/tid\/css\/popup.css":1,"sites\/all\/themes\/tid\/css\/mods.css":1}},"lightbox2":{"rtl":0,"file_path":"\/(\\w\\w\/)public:\/","default_image":"\/sites\/all\/modules\/contrib\/lightbox2\/images\/brokenimage.jpg","border_size":10,"font_color":"000","box_color":"fff","top_position":"","overlay_opacity":"0.8","overlay_color":"000","disable_close_click":true,"resize_sequence":0,"resize_speed":400,"fade_in_speed":400,"slide_down_speed":600,"use_alt_layout":false,"disable_resize":false,"disable_zoom":false,"force_show_nav":false,"show_caption":true,"loop_items":false,"node_link_text":"View Image Details","node_link_target":false,"image_count":"Image !current of !total","video_count":"Video !current of !total","page_count":"Page !current of !total","lite_press_x_close":"press \u003Ca href=\u0022#\u0022 onclick=\u0022hideLightbox(); return FALSE;\u0022\u003E\u003Ckbd\u003Ex\u003C\/kbd\u003E\u003C\/a\u003E to close","download_link_text":"","enable_login":false,"enable_contact":false,"keys_close":"c x 27","keys_previous":"p 37","keys_next":"n 39","keys_zoom":"z","keys_play_pause":"32","display_image_size":"original","image_node_sizes":"()","trigger_lightbox_classes":"","trigger_lightbox_group_classes":"","trigger_slideshow_classes":"","trigger_lightframe_classes":"","trigger_lightframe_group_classes":"","custom_class_handler":0,"custom_trigger_classes":"","disable_for_gallery_lists":true,"disable_for_acidfree_gallery_lists":true,"enable_acidfree_videos":true,"slideshow_interval":5000,"slideshow_automatic_start":true,"slideshow_automatic_exit":true,"show_play_pause":true,"pause_on_next_click":false,"pause_on_previous_click":true,"loop_slides":false,"iframe_width":600,"iframe_height":400,"iframe_border":1,"enable_video":false,"useragent":"Mozilla\/5.0 (Linux; Android 7.0;) AppleWebKit\/537.36 (KHTML, like Gecko) Mobile Safari\/537.36 (compatible; PetalBot;+https:\/\/webmaster.petalsearch.com\/site\/petalbot)"},"extlink":{"extTarget":0,"extClass":"ext","extLabel":"(link is external)","extImgClass":0,"extIconPlacement":0,"extSubdomains":1,"extExclude":".gov|.com|.org|.io|.be|.us|.edu","extInclude":"-int.llnl.gov|lc.llnl.gov|caas.llnl.gov|exchangetools.llnl.gov","extCssExclude":"","extCssExplicit":"","extAlert":"_blank","extAlertText":"This page is routing you to a page which requires extra authentication. You must have on-site or VPN access.\r\n\r\nPress OK to continue or cancel to return.\r\n\r\nIf this fails or times-out, you are not allowed access to the internal page or the server may be temporarily unavailable.\r\n\r\nIf you have an on-site or VPN account and are still having trouble, please send e-mail to lc-hotline@llnl.gov or call 925-422-4531 for further assistance.","mailtoClass":"mailto","mailtoLabel":"(link sends e-mail)"},"matomo":{"trackMailto":1},"urlIsAjaxTrusted":{"\/training\/tutorials\/using-lcs-sierra-system":true}});
//--><!]]>
</script>
</html>
