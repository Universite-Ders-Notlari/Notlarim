<!DOCTYPE html>
<html lang="en" dir="ltr"
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:dc="http://purl.org/dc/terms/"
  xmlns:foaf="http://xmlns.com/foaf/0.1/"
  xmlns:og="http://ogp.me/ns#"
  xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
  xmlns:sioc="http://rdfs.org/sioc/ns#"
  xmlns:sioct="http://rdfs.org/sioc/types#"
  xmlns:skos="http://www.w3.org/2004/02/skos/core#"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#">
<head>
<meta charset="utf-8" http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="Generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="/training/tutorials/livermore-computing-resources-and-environment" />
<link rel="shortlink" href="/node/594" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
<link rel="shortcut icon" href="https://hpc.llnl.gov/sites/all/themes/tid/favicon.ico" type="image/vnd.microsoft.icon" />
<title>Livermore Computing Resources and Environment | High Performance Computing</title>
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_kShW4RPmRstZ3SpIC-ZvVGNFVAi0WEMuCnI0ZkYIaFw.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_bq48Es_JAifg3RQWKsTF9oq1S79uSN2WHxC3KV06fK0.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_vAm-LJc0tkC-w_c6v7Ekq0bW26Pzl31HvPM6kbvK-pc.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://hpc.llnl.gov/sites/default/files/css/css_ca6tstDbY9-H23Ty8uKiDyFQLT1AZftZKldhbTPPnm8.css" media="all" />
<!--[if lt IE 9]><script src="/sites/all/themes/tid/js/html5.js"></script><![endif]-->
</head>
<body class="html not-front not-logged-in no-sidebars page-node page-node- page-node-594 node-type-user-portal-one-column-page">
  <div aria="contentinfo"><noscript><img src="https://analytics.llnl.gov/piwik.php?idsite=149" class="no-border" alt="" /></noscript></div>
    <div id="page">
	<div class="unclassified"></div>
	<div class="headertop">
					<div id="skip-nav" role="navigation" aria-labelledby="skip-nav" class="reveal">
  			<a href="#main-content">Skip to main content</a>
			</div>
					</div>
        <div class="headerwrapbg">
                        <div class="headerwrap-portal">
                <div id="masthead" class="site-header container" role="banner">
                    <div class="row">
                        <div class="llnl-logo col-sm-3">
                            <a href="https://www.llnl.gov" target="_blank" title="Lawrence Livermore National Laboratory">
                                <img src="/sites/all/themes/tid/images/llnl-tab-portal.png" alt="LLNL Home" />
                            </a>
                        </div>
                        <div id="logo" class="site-branding col-sm-4">
                                                            <div id="site-logo">
                                        <!--High Performance Computing<br />Livermore Computing Center-->
                                        																					<a href="/user-portal" class="text-dark" title="Livermore Computing Center High Performance Computing">
                                            <img src="/sites/all/themes/tid/images/hpc.png" alt="Portal Home" />
																					</a>
																				
                                </div>
                                                    </div>
                        <div class="col-sm-5">
                            <div id="top-search">
															<div class="input-group">
																	<form class="navbar-form navbar-search navbar-right" action="/training/tutorials/livermore-computing-resources-and-environment" method="post" id="search-block-form" accept-charset="UTF-8"><div><div class="container-inline">
      <div class="element-invisible">Search form</div>
    <div class="form-item form-type-textfield form-item-search-block-form">
  <label class="element-invisible" for="edit-search-block-form--2">Search </label>
 <input title="Enter the terms you wish to search for." type="text" id="edit-search-block-form--2" name="search_block_form" value="" size="15" maxlength="128" class="form-text" />
</div>
<div class="form-actions form-wrapper" id="edit-actions"><input type="submit" id="edit-submit" name="op" value="ï€‚" class="form-submit" /></div><input type="hidden" name="form_build_id" value="form-R68wCfq7_5CDO7_ZSGePtCdA4gVgk--RUNQyaHq6sgU" />
<input type="hidden" name="form_id" value="search_block_form" />
</div>
</div></form>                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div id="mainnav">
                    <div class="container">
                        <div class="row">
                            <nav id="Menu" aria-label="Mobile Menu" class="mobilenavi col-md-12"></nav>
                            <nav id="navigation" aria-label="Main Menu">
                                <div id="main-menu" class="main-menu-portal">
                                    <ul class="menu"><li class="first collapsed"><a href="/user-portal">Portal</a></li>
<li class="expanded"><a href="/accounts">Accounts</a><ul class="menu"><li class="first leaf"><a href="/accounts/new-account-setup">New Account Setup</a></li>
<li class="leaf"><a href="/accounts/idm-account-management">IdM Account Management</a></li>
<li class="leaf"><a href="https://hpc.llnl.gov/manuals/access-lc-systems" title="">Access to LC Systems</a></li>
<li class="leaf"><a href="/accounts/computer-coordinator-roles">Computer Coordinator Roles</a></li>
<li class="collapsed"><a href="/accounts/forms">Forms</a></li>
<li class="collapsed"><a href="/accounts/policies">Policies</a></li>
<li class="last leaf"><a href="/accounts/mailing-lists">Mailing Lists</a></li>
</ul></li>
<li class="expanded"><a href="/banks-jobs">Banks &amp; Jobs</a><ul class="menu"><li class="first leaf"><a href="/banks-jobs/allocations">Allocations</a></li>
<li class="expanded"><a href="/banks-jobs/running-jobs">Running Jobs</a><ul class="menu"><li class="first leaf"><a href="/banks-jobs/running-jobs/batch-system-primer">Batch System Primer</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/lsf-user-manual">LSF User Manual</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/lsf-quick-start-guide">LSF Quick Start Guide</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/lsf-commands">LSF Commands</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/slurm-user-manual" title="Guide to using the Slurm Workload/Resource Manager">Slurm User Manual</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/slurm-quick-start-guide">Slurm Quick Start Guide</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/slurm-commands">Slurm Commands</a></li>
<li class="leaf"><a href="/training/tutorials/slurm-and-moab">Slurm and Moab</a></li>
<li class="leaf"><a href="/banks-jobs/running-jobs/batch-system-commands">Batch System Cross-Reference</a></li>
<li class="last leaf"><a href="/banks-jobs/running-jobs/slurm-srun-versus-ibm-csm-jsrun">Slurm srun versus IBM CSM jsrun</a></li>
</ul></li>
<li class="leaf"><a href="https://hpc.llnl.gov/accounts/forms/asc-dat" title="">ASC DAT Request</a></li>
<li class="last leaf"><a href="https://hpc.llnl.gov/accounts/forms/mic-dat" title="">M&amp;IC DAT Request</a></li>
</ul></li>
<li class="expanded"><a href="/hardware">Hardware</a><ul class="menu"><li class="first collapsed"><a href="/hardware/archival-storage-hardware">Archival Storage Hardware</a></li>
<li class="collapsed"><a href="/hardware/platforms">Compute Platforms</a></li>
<li class="leaf"><a href="/hardware/compute-platforms-gpus">Compute Platforms with GPUs</a></li>
<li class="collapsed"><a href="/hardware/file-systems">File Systems</a></li>
<li class="leaf"><a href="/hardware/testbeds">Testbeds</a></li>
<li class="collapsed"><a href="/hardware/zones">Zones (aka &quot;The Enclave&quot;)</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/lorenz/mylc/mylc.cgi" title="">MyLC (Lorenz)</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi?" title="">CZ Compute Platform Status</a></li>
<li class="leaf"><a href="https://rzlc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" title="">RZ Compute System Status</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/fsstatus/fsstatus.cgi" title="">CZ File System Status</a></li>
<li class="last leaf"><a href="https://rzlc.llnl.gov/fsstatus/fsstatus.cgi" title="">RZ File System Status</a></li>
</ul></li>
<li class="expanded"><a href="/services">Services</a><ul class="menu"><li class="first collapsed"><a href="/services/green-data-oasis">Green Data Oasis (GDO)</a></li>
<li class="leaf"><a href="https://lc.llnl.gov/lorenz/mylc/mylc.cgi" title="">MyLC (Lorenz)</a></li>
<li class="last leaf"><a href="/services/visualization-services">Visualization Services</a></li>
</ul></li>
<li class="expanded"><a href="/software">Software</a><ul class="menu"><li class="first leaf"><a href="/software/archival-storage-software">Archival Storage Software</a></li>
<li class="collapsed"><a href="/software/data-management-tools-projects">Data Management Tools</a></li>
<li class="collapsed"><a href="/software/development-environment-software">Development Environment Software</a></li>
<li class="leaf"><a href="/software/mathematical-software">Mathematical Software</a></li>
<li class="leaf"><a href="/software/modules-and-software-packaging">Modules and Software Packaging</a></li>
<li class="collapsed"><a href="/software/visualization-software">Visualization Software</a></li>
<li class="last leaf"><a href="https://computing.llnl.gov/projects/radiuss" title="">RADIUSS</a></li>
</ul></li>
<li class="last expanded active-trail"><a href="/training" class="active-trail">Training</a><ul class="menu"><li class="first expanded active-trail"><a href="/training/tutorials" class="active-trail">Tutorials</a><ul class="menu"><li class="first leaf"><a href="/training/tutorials/introduction-parallel-computing-tutorial">Introduction to Parallel Computing Tutorial</a></li>
<li class="leaf"><a href="/training/tutorials/llnl-covid-19-hpc-resource-guide">LLNL Covid-19 HPC Resource Guide for New Livermore Computing Users</a></li>
<li class="leaf"><a href="/training/tutorials/using-lcs-sierra-system">Using LC&#039;s Sierra System</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-psaap3-quick-start-tutorial">Livermore Computing PSAAP3 Quick Start Tutorial</a></li>
<li class="leaf"><a href="https://hpc.llnl.gov/sites/default/files/PSAAP-alliance-quickguide.docx" title="">PSAAP Alliance Quick Guide</a></li>
<li class="leaf"><a href="/training/tutorials/linux-tutorial-exercises">Linux Tutorial Exercise One</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-linux-commodity-clusters-overview-part-one">Livermore Computing Linux Clusters Overview Part One</a></li>
<li class="leaf"><a href="/training/tutorials/livermore-computing-linux-commodity-clusters-overview-part-two">Livermore Computing Linux Clusters Overview Part Two</a></li>
<li class="leaf active-trail"><a href="/training/tutorials/livermore-computing-resources-and-environment" class="active-trail active">Livermore Computing Resources and Environment</a></li>
<li class="leaf"><a href="/training/tutorials/slurm-and-moab-exercise">Slurm and Moab Exercise</a></li>
<li class="leaf"><a href="/training/tutorials/slurm-and-moab">Slurm and Moab Tutorial</a></li>
<li class="leaf"><a href="/training/tutorials/totalview-part-2-common-functions">TotalView Part 2:  Common Functions</a></li>
<li class="leaf"><a href="/training/tutorials/totalview-part-3-debugging-parallel-programs">TotalView Part 3: Debugging Parallel Programs</a></li>
<li class="leaf"><a href="/training/tutorials/totalview-tutorial">TotalView Tutorial</a></li>
<li class="leaf"><a href="/training/tutorials/evaluation-form">Tutorial Evaluation Form</a></li>
<li class="leaf"><a href="/training/tutorials/srun-auto-affinity">srun --auto-affinity</a></li>
<li class="last leaf"><a href="/training/tutorials/srun-multi-prog">srun --multi-prog</a></li>
</ul></li>
<li class="collapsed"><a href="/training/documentation">Documentation &amp; User Manuals</a></li>
<li class="leaf"><a href="/training/technical-bulletins-catalog">Technical Bulletins Catalog</a></li>
<li class="collapsed"><a href="/training/workshop-schedule">Training Events</a></li>
<li class="last leaf"><a href="/training/user-meeting-presentations-archive">User Meeting Presentation Archive</a></li>
</ul></li>
</ul>                                                                            <div id="pagetoggle" class="btn-group btn-toggle pull-right" style="margin-right: 15px;">
                                            <a href="/" class="btn btn-default gs">General Site</a>
                                            <a href="/user-portal" class="btn btn-primary up active">User Portal</a>
                                        </div>
                                                                    </div>
                            </nav>
                        </div>
                    </div>
                </div>
            </div>
        </div>
            </div>
		<div id="main-content" class="l2content">
        <div class="container">
    		<div class="row">
        		                <div id="primary" class="content-area col-sm-12">
					                                        <section id="content" role="nav" class="clearfix col-sm-12">

                                                                                    <div id="breadcrumbs">
                                    <h2 class="element-invisible">breadcrumb menu</h2><nav class="breadcrumb" aria-label="breadcrumb-navigation"><a href="/">Home</a> Â» <a href="/training">Training</a> Â» <a href="/training/tutorials">Tutorials</a> Â» Livermore Computing Resources and Environment</nav>                                </div>
                                                    
                                            </section>
                  <main>

                                              <div id="content_top">
                                <div class="region region-content-top">
  <div id="block-print-ui-print-links" class="block block-print-ui">

    
    
  
  <div class="content">
    <span class="print_html"><a href="https://hpc.llnl.gov/print/594" title="Display a printer-friendly version of this page." class="print-page" onclick="window.open(this.href); return false" rel="nofollow">Printer-friendly</a></span>  </div>
  
</div> <!-- /.block --></div>
 <!-- /.region -->
                            </div>
                        
                        <div id="content-wrap">
                                                                                                                <div class="region region-content">
  <div id="block-system-main" class="block block-system">

    
    
  
  <div class="content">
    

<div  about="/training/tutorials/livermore-computing-resources-and-environment" typeof="sioc:Item foaf:Document" class="node node-user-portal-one-column-page node-full view-mode-full">
    <div class="row">
    <div class="col-sm-12 ">
      <div class="field field-name-title field-type-ds field-label-hidden"><div class="field-items"><div class="field-item even" property="dc:title"><h1 class="title">Livermore Computing Resources and Environment</h1></div></div></div><div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even" property="content:encoded"><h2>Table of Contents<a name="top" id="top"></a></h2>
<ol><li><a href="#abstract">Abstract</a></li>
<li><a href="#organization">Organization</a>
<ol><li><a href="#what-is-lc">What Is Livermore Computing?</a></li>
<li><a href="#history">History of Livermore Computing</a></li>
</ol></li>
<li><a href="#terminology">Terminology</a></li>
<li><a href="#hardware">LC's Hardware</a>
<ol><li><a href="#systems-summary">Systems Summary</a></li>
<li><a href="#intel-systems">Intel Xeon Systems</a></li>
<li><a href="#coral-systems">CORAL Systems</a></li>
<li><a href="#future-systems">Future Systems</a></li>
<li><a href="#linux-cluster">Typical LC Linux Cluster</a></li>
<li><a href="#interconnects">Interconnects</a></li>
<li><a href="#facilities">Facilities, Machine Room Tours, Photos</a></li>
</ol></li>
<li><a href="#accounts">LC Accounts</a></li>
<li><a href="#lc-systems">Accessing LC Systems</a>
<ol><li><a href="#passwords">Passwords, Authentication, and OTP Tokens</a></li>
<li><a href="#methods">SSH and Access Methods</a></li>
<li><a href="#ssh">A Few More Words About SSH</a></li>
<li><a href="#login">Where to Login</a></li>
<li><a href="#remote-access">VPN Remote Access Service</a></li>
<li><a href="#securenet">SecureNet</a></li>
</ol></li>
<li><a href="#file-systems">LC File Systems</a>
<ol><li><a href="#home-directories">Home Directories and Login Files</a></li>
<li><a href="#usr-workspace">/usr/workspace File Systems</a></li>
<li><a href="#temp-file-systems">Temporary File Systems</a></li>
<li><a href="#parallel-file-systems">Parallel File Systems</a></li>
<li><a href="#archival-hpss">Archival HPSS Storage</a></li>
<li><a href="#usr-gapps">/usr/gapps, /usr/gdata File Systems</a></li>
<li><a href="#quotas">Quotas</a></li>
<li><a href="#purge-policies">Purge Policies</a></li>
<li><a href="#backups">Backups</a></li>
<li><a href="#file-transfer">File Transfer and Sharing</a></li>
<li><a href="#fis">File Interchange Service (FIS)</a></li>
</ol></li>
<li><a href="#system-status">System Status and Configuration Information</a>
<ol><li><a href="#sys-config-info">System Configuration Information</a></li>
<li><a href="#sys-config-commands">System Configuration Commands</a></li>
<li><a href="#sys-status-info">System Status Information</a></li>
</ol></li>
<li><a href="#exercise-1">Exercise 1</a></li>
<li><a href="#development-environment">Software and Development Environment Overview</a>
<ol><li><a href="#deg">Development Environment Group (DEG)</a></li>
<li><a href="#toss">TOSS Operating System</a></li>
<li><a href="#software-lists">Software Lists</a></li>
<li><a href="#modules">Modules</a></li>
<li><a href="#atlassian">Atlassian Tools - Confluence, JIRA, etc.</a></li>
<li><a href="#spack">Spack Package Manager</a></li>
</ol></li>
<li><a href="#compilers">Compilers</a>
<ol><li><a href="#compilers-commands">Available Compilers and Invocation Commands</a></li>
<li><a href="#compilers-versions">Compiler Versions and Defaults</a></li>
<li><a href="#compilers-options">Compiler Options</a></li>
<li><a href="#compilers-docs">Compiler Documentation</a></li>
<li><a href="#compilers-opt">Optimizations</a></li>
<li><a href="#compilers-flop">Floating-point Exceptions</a></li>
<li><a href="#compilers-precision">Precision, Performance, and IEEE 754 Compliance</a></li>
<li><a href="#compilers-c-fortran">Mixing C and Fortran</a></li>
</ol></li>
<li><a href="#debuggers">Debuggers</a>
<ol><li><a href="#total-view">TotalView</a></li>
<li><a href="#ddt">DDT</a></li>
<li><a href="#stat">STAT - Stack Trace Analysis Tool</a></li>
<li><a href="#debugging-batch">Debugging in Batch: mxterm / sxterm</a></li>
<li><a href="#debuggers-other">Other Debuggers</a></li>
<li><a href="#debuggers-hints">A Few Additional Useful Debugging Hints</a></li>
</ol></li>
<li><a href="#performance-analysis">Performance Analysis Tools</a>
<ol><li><a href="#book">We Need a Book!</a></li>
<li><a href="#mem-correctness">Memory Correctness Tools</a></li>
<li><a href="#profiling">Profiling, Tracing, and Performance Analysis</a></li>
<li><a href="#beyond-lc">Beyond LC</a></li>
</ol></li>
<li><a href="#graphics-software-resources">Graphics Software and Resources</a>
<ol><li><a href="#consulting">Consulting</a></li>
<li><a href="#video-prod">Video Production</a></li>
<li><a href="#viz-resources">Visualization Machine Resources</a></li>
<li><a href="#power-walls">Power Walls</a></li>
</ol></li>
<li><a href="#running-jobs">Running Jobs</a>
<ol><li><a href="#where">Where to Run?</a></li>
<li><a href="#batch-vs-interactive">Batch Versus Interactive</a></li>
<li><a href="#starting-jobs">Starting Jobs - srun</a></li>
<li><a href="#interacting">Interacting with Jobs</a></li>
<li><a href="#other-topics">Other Topics of Interest</a></li>
</ol></li>
<li><a href="#batch-systems">Batch Systems</a></li>
<li><a href="#misc-topics">Miscellaneous Topics</a>
<ol><li><a href="#clusters-gpus">Clusters with GPUs</a></li>
<li><a href="#big-data">Big Data at LC</a></li>
<li><a href="#gdo">Green Data Oasis</a></li>
<li><a href="#security">Security Reminders</a></li>
</ol></li>
<li><a href="#help">Where to Get Information &amp; Help</a>
<ol><li><a href="#hotline">LC Hotline</a></li>
<li><a href="#user-home">LC Users Home Page: hpc.llnl.gov</a></li>
<li><a href="#user-dash">Lorenz User Dashboard: mylc.llnl.gov</a></li>
<li><a href="#login-banner">Login Banner</a></li>
<li><a href="#news-items">News Items</a></li>
<li><a href="#email-list">Machine Email Lists</a></li>
<li><a href="#user-meeting">LC User Meetings</a></li>
</ol></li>
<li><a href="#exercise-2">Exercise 2</a></li>
</ol><h2><a name="abstract" id="abstract"></a>Abstract</h2>
<p>This is the second tutorial in the "Livermore Computing Getting Started" workshop. It provides an overview of Livermore Computing's (LC) supercomputing resources and how to effectively use them. As such, it is definitely intended as a "getting started" document for new users or for those who want to know "in a nutshell" what supercomputing at LC is all about from a practical user's perspective. It is also intended to provide essential, practical information for attendees planning to attend the other tutorials in this workshop.</p>
<p>A wide variety of topics are covered in what is hopefully a logical progression, starting with a description of the LC organization, a summary of the available supercomputing hardware resources, how to obtain an account and how to access LC systems. Important aspects concerning the user environment are then addressed, such as the user's home directory, various files and file systems, how to transfer/share files, quotas, archival storage and getting system status/configuration information. A brief description of the software development environment (compilers, debuggers, and performance tools), a summary of video and graphics services, and the basics of how to run jobs follow. Several miscellaneous topics are discussed. Finally, this tutorial concludes with a discussion on where to obtain more information and help. Note: This tutorial only provides an overview of using LC's Slurm/Moab batch systems; these topics are covered in the EC4045 "Slurm and Moab" tutorial.</p>
<p><em>Level/Prerequisites:</em> This tutorial is geared to new users of LC systems and might actually be considered a prerequisite for using LC systems and attending other tutorials that describe parallel programming on LC systems in more detail.</p>
<h2><a name="organization" id="organization"></a>Organization</h2>
<h3><a name="what-is-lc" id="what-is-lc"></a>What Is Livermore Computing?</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-794" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/logos1jpeg">logos1.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/logos1.jpeg"><img alt="Logos for the DOE, NNSA, LLNS, and LLNL" height="74" width="400" style="font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; font-size: 14px; font-family: Oxygen, sans-serif; width: 400px; height: 74px;" class="media-element file-default" data-delta="1" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/logos1-400x74.jpeg" /></a>  </div>

  
</div>
</div></div>
<ul><li>Lawrence Livermore National Laboratory (LLNL) is managed and operated by the Lawrence Livermore National Security, LLC (LLNS), for the Department of Energy's (DOE) National Nuclear Security Administration (NNSA).<br />DOE: <a href="https://energy.gov" target="_blank">energy.gov</a><br />NNSA: <a href="https://www.energy.gov/nnsa/national-nuclear-security-administration" target="_blank">www.energy.gov/nnsa/national-nuclear-security-administration</a><br />LLNS: <a href="http://www.llnsllc.com" target="_blank">www.llnsllc.com</a><br />LLNL: <a href="http://www.llnl.gov" target="_blank">www.llnl.gov</a></li>
<li>LLNL is organized into Principal Associate Directorates (PADs).</li>
<li>Official org chart available at: <a href="https://www.llnl.gov/about/organization" target="_blank">www.llnl.gov/about/organization</a>Â andÂ <a href="https://computing-int.llnl.gov/about/organization">https://computing-int.llnl.gov/about/organization</a>Â </li>
<li><a href="https://computing.llnl.gov/livermore-computing" target="_blank">Livermore Computing</a> is a division within the <a href="https://computing.llnl.gov" target="_blank">Computing Directorate</a>.</li>
<li>Mission:
<ul><li>Provide first-class computational infrastructure that supports the computing requirements of the Laboratory's scientists.</li>
<li>Develop High Performance Computing (HPC) solutions, in collaboration with partners at Los Alamos and Sandia National Laboratories, for use by the <a href="https://www.energy.gov/nnsa/missions/maintaining-stockpile" target="_blank">Advanced Simulation and Computing Program</a>.</li>
<li>Provide leveraged, cost-effective HPC to multiple programs and independent LLNL researchers under the <a href="https://mic.llnl.gov/" target="_blank">Multiprogrammatic and Institutional Computing (M&amp;IC) program</a>.</li>
</ul></li>
<li>Approx. 120 staff members</li>
<li>Most of LC and its resources are located in buildings 453, 451, and 654. <a href="/sites/default/files/LLNLmap2.gif">Here's a map.</a></li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-2048" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/livermore-computing-png">livermore-computing.png</a></h2>
    
  
  <div class="content">
    <img alt="group shot of LC people" height="278" width="940" class="media-element file-default" data-delta="137" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/livermore-computing.png" /></div>

  
</div>
</div>
<h3><a name="history" id="history"></a>History of Livermore Computing</h3>
<ul><li>The history of Livermore Computing has its origins over 65 years ago when LLNL acquired its first computer, a Univac 1, in 1953.</li>
<li>A pictorial history of LLNL's computers is available at: <a href="https://computing.llnl.gov/history" target="_blank">computing.llnl.gov/history</a>.</li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-1822" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/lchistory1-png">LChistory1.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/LChistory1.png"><img alt="Screenshot showing the history of LLNL computing iFrame" height="508" width="500" style="width: 500px; height: 508px;" class="media-element file-default" data-delta="112" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/LChistory1-500x508.png" /></a>  </div>

  
</div>
</div>
<h2><a name="terminology" id="terminology"></a>Terminology</h2>
<p><strong>"The acronyms can be a bit overwhelming"</strong><br /><em>- Excerpt from a workshop attendee evaluation form</em></p>
<table class="table table-striped table-bordered"><tr><th scope="row">Acronym</th>
<th scope="col">Meaning</th>
</tr><tr><th scope="row">LC</th>
<td>Livermore Computing - the division / program directly responsible for LLNL's supercomputers.</td>
</tr><tr><th scope="row">HPC</th>
<td>High Performance Computing. Supercomputing. Computation using the largest scale computers available.</td>
</tr><tr><th scope="row">OCF</th>
<td>LC's Open Computing Facility - unclassified computing</td>
</tr><tr><th scope="row">SCF</th>
<td>LC's Secure Computing Facility - closed or classified computing</td>
</tr><tr><th scope="row">CZ</th>
<td>Collaboration Zone - the open "green" part of the OCF</td>
</tr><tr><th scope="row">RZ</th>
<td>Restricted Zone - the internal "yellow" part of the OCF</td>
</tr><tr><th scope="row">ASC</th>
<td>The Department of Energy's <strong>A</strong>dvanced <strong>S</strong>imulation and <strong>C</strong>omputing program. Funding supports HPC at Livermore, Los Alamos and Sandia National Laboratories. ASC website: <a href="http://nnsa.energy.gov/asc" target="_blank">nnsa.energy.gov/asc</a></td>
</tr><tr><th scope="row">M&amp;IC</th>
<td>Multiprogrammatic &amp; Institutional Computing (<a href="https://mic.llnl.gov/" target="_blank">mic.llnl.gov</a>)</td>
</tr><tr><th scope="row">Slurm<br />Moab<br />LSF</th>
<td>Batch systems employed on LC machines</td>
</tr><tr><th scope="row">Node</th>
<td>A single computer in a networked HPC system/cluster:
<ul><li>Contains multiple CPUs/cores</li>
<li>Linked to other nodes via at least one type of network</li>
</ul></td>
</tr><tr><th scope="row">CPU<br />Core</th>
<td>CPU = Central Processing Unit. The component within a node that executes programs and performs computations. Currently, CPUs are comprised of multiple, identical computational subunits called cores.</td>
</tr><tr><th scope="row">CTS, CTS-1</th>
<td>Commodity Technology Systems. Linux clusters procured under a Tri-lab proposal process for capacity computing systems. See <a href="https://asc.llnl.gov/computers/commodity" target="_blank">asc.llnl.gov/computers/commodity</a>.</td>
</tr><tr><th scope="row">TFLOPS<br />PFLOPS</th>
<td>Measure of a supercomputer's power/speed. Teraflops: Trillion Floating Point Operations per Second. Petaflops: Quadrillion Floating Point Operations per Second</td>
</tr><tr><th scope="row">Lustre</th>
<td>Linux cluster parallel file system used on most LC clusters (<a href="http://wiki.lustre.org/index.php/Main_Page" target="_blank">wiki.lustre.org/index.php/Main_Page</a>)</td>
</tr><tr><th scope="row">TOSS</th>
<td>Tri-Laboratory Operating System Stack - the operating system and software stack used on LC Linux clusters. Derived from Redhat Enterprise Linux.</td>
</tr><tr><th scope="row">Newbie</th>
<td>Who this tutorial is intended for</td>
</tr></table><p><strong><em>"Everything changes except the fact that everything changes."</em></strong></p>
<p><span class="text-danger"><strong>DISCLAIMER: All information presented today is subject to change! This information was current as of Jan 2019.</strong></span></p>
<h2><a name="hardware" id="hardware"></a>Hardware</h2>
<h3><a name="systems-summary" id="systems-summary"></a>Systems Summary</h3>
<table><tr><td><div class="media media-element-container media-default"><div id="file-1383" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/ibm-power-w-nvidia-gpus">IBM POWER w/ NVIDIA GPUs</a></h2>
    
  
  <div class="content">
    <img alt="photo of IBM POWER w/ NVIDIA GPUs" title="IBM POWER w/ NVIDIA GPUs (Sierra)" height="200" width="399" class="media-element file-default" data-delta="101" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/sierra.200pix.png" /></div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-804" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/quartz01200pixjpeg-0">quartz01.200pix.jpeg</a></h2>
    
  
  <div class="content">
    <img alt="Intel Xeon system (Quartz)" title="Intel Xeon" height="200" width="300" class="media-element file-default" data-delta="10" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/quartz01.200pix_0.jpeg" /></div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-2049" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/zin-200pix-jpg">zin.200pix.jpg</a></h2>
    
  
  <div class="content">
    <img alt="Zin machine" height="200" width="300" style="width: 300px; height: 200px;" class="media-element file-default" data-delta="138" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/zin.200pix.jpg" /></div>

  
</div>
</div></td>
</tr><tr><td>IBM POWER with NVIDIA GPUs</td>
<td>Intel Xeon</td>
<td>Intel Xeon</td>
</tr></table><h4>Mix of Resources</h4>
<ul><li><strong>IBM POWER with NVIDIA GPUs</strong>: LC's newest and largest systems. Include LLNL's CORAL Early Access and Sierra systems.</li>
<li><strong>Intel Xeon</strong>: Comprise the majority of LC's clusters; several different types</li>
<li><strong>Size</strong>: Wide range, from dozens of cores to 1.6 million cores; from less than several teraflops to 125 petaflops</li>
<li><strong>Networks</strong>: InfiniBand, Intel Omni-Path, 5D Torus interconnect, or no interconnect</li>
<li><strong>Uses</strong>: Capability Computing, Grand Challenge, routine production work, visualization work, file transfer, test-bed</li>
<li><strong>Funding</strong>: ASC; M&amp;IC, mixed</li>
</ul><h4>Primary Systems</h4>
<p>LC's primary HPC computing systems are summarized in the table below.</p>
<table class="table table-striped table-bordered"><tr><th scope="col">Cluster</th>
<th scope="col">OCF<br />SCF</th>
<th scope="col">Architecture</th>
<th scope="col">Clock Speed (CHz)</th>
<th scope="col">Nodes GPUs</th>
<th scope="col">Cores / Node / GPU</th>
<th scope="col">Cores Total</th>
<th scope="col">Memory / Node (GB)</th>
<th scope="col">Memory Total (GB)</th>
<th scope="col">TFLOPS Peak</th>
<th scope="col">Switch</th>
<th scope="col">ASC<br />M&amp;IC</th>
<th scope="col">Notes</th>
</tr><tr><td><strong>agate</strong></td>
<td>SCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>48</td>
<td>36</td>
<td>1,728</td>
<td>128</td>
<td>6,144</td>
<td>58.1</td>
<td>No</td>
<td>ASC</td>
<td>Â </td>
</tr><tr><td><strong>borax</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>48</td>
<td>36</td>
<td>1,728</td>
<td>128</td>
<td>6,144</td>
<td>58.1</td>
<td>No</td>
<td>ASC/M&amp;IC</td>
<td>Â </td>
</tr><tr><td><strong>catalyst</strong></td>
<td>OCF</td>
<td>Intel 12-core Xeon E5-2695 v2</td>
<td>2.4</td>
<td>324</td>
<td>24</td>
<td>7,776</td>
<td>128</td>
<td>41,472</td>
<td>149.3</td>
<td>IB QDR</td>
<td>ASC/M&amp;IC</td>
<td>1,4</td>
</tr><tr><td><strong>cslic</strong></td>
<td>SCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>10</td>
<td>16</td>
<td>160</td>
<td>128</td>
<td>1,280</td>
<td>3.3</td>
<td>No</td>
<td>ASC</td>
<td>2</td>
</tr><tr><td><strong>jade<br />jadeite<br />jadedev</strong></td>
<td>SCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>2,688</td>
<td>36</td>
<td>96,768</td>
<td>128</td>
<td>344,064</td>
<td>3,251.4</td>
<td>Omni-Path</td>
<td>ASC</td>
<td>5</td>
</tr><tr><td><strong>lassen</strong></td>
<td>OCF</td>
<td>IBM Power9<br />NVIDIA Tesla V100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>774<br />774*4</td>
<td>44<br />5120</td>
<td>34,056<br />15,851,520</td>
<td>256<br />16*4</td>
<td>198,144<br />49,536</td>
<td>22,508</td>
<td>IB EDR</td>
<td>ASC/M&amp;IC</td>
<td>1</td>
</tr><tr><td><strong>max</strong></td>
<td>SCF</td>
<td>Intel 8-core Xeon E5-2670<br />NVIDIA Tesla K20x</td>
<td>2.6<br />732 MHz</td>
<td>324<br />20*2</td>
<td>16<br />2688</td>
<td>5,184<br />107,520</td>
<td>256<br />6*2</td>
<td>78,336<br />240</td>
<td>108<br />52.4</td>
<td>IB QDR</td>
<td>ASC VIZ</td>
<td>3</td>
</tr><tr><td><strong>mica</strong></td>
<td>SCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>384</td>
<td>36</td>
<td>13,824</td>
<td>128</td>
<td>49,152</td>
<td>464.5</td>
<td>Omni-Path</td>
<td>ASC</td>
<td>Â </td>
</tr><tr><td><strong>oslic</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>10</td>
<td>16</td>
<td>160</td>
<td>128</td>
<td>1,280</td>
<td>3.3</td>
<td>No</td>
<td>ASC</td>
<td>2</td>
</tr><tr><td><strong>pascal</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>171<br />163*2</td>
<td>36<br />3484</td>
<td>6,156<br />1,135,784</td>
<td>256<br />16*2</td>
<td>18,176<br />5,216</td>
<td>206.8<br />1,727.8</td>
<td>Omni-Path</td>
<td>ASC/M&amp;IC</td>
<td>Â </td>
</tr><tr><td><strong>pinot</strong></td>
<td>ISNSI</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>162</td>
<td>16</td>
<td>2,592</td>
<td>64</td>
<td>10,368</td>
<td>53.9</td>
<td>IB QDR</td>
<td>M&amp;IC</td>
<td>1</td>
</tr><tr><td><strong>quartz</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>3,072</td>
<td>36</td>
<td>110,592</td>
<td>128</td>
<td>393,216</td>
<td>3,715.9</td>
<td>Omni-Path</td>
<td>ASC/M&amp;IC</td>
<td>Â </td>
</tr><tr><td><strong>ray</strong></td>
<td>OCF</td>
<td>IBM Power8<br />NVIDIA Tesla P100 (Pascal)</td>
<td>2.0-4.0<br />1481 MHz</td>
<td>62<br />54*4</td>
<td>20<br />3484</td>
<td>1,240<br />752,544</td>
<td>256<br />16*4</td>
<td>15,872<br />3,456</td>
<td>39.7<br />1,144.8</td>
<td>IB EDR</td>
<td>ASC/M&amp;IC</td>
<td>1</td>
</tr><tr><td><strong>rzalastor</strong></td>
<td>OCF</td>
<td>Intel 10-core Xeon E5-2670 v2</td>
<td>2.8</td>
<td>36</td>
<td>20</td>
<td>720</td>
<td>64</td>
<td>2,304</td>
<td>16.1</td>
<td>IB QDR</td>
<td>ASC</td>
<td>1</td>
</tr><tr><td><strong>rzansel</strong></td>
<td>OCF</td>
<td>IBM Power9<br />NVIDIA Tesla V100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>54<br />54*4</td>
<td>44<br />5120</td>
<td>2376<br />1,105,920</td>
<td>256<br />16*4</td>
<td>13,824<br />3,456</td>
<td>1,570</td>
<td>IB EDR</td>
<td>ASC</td>
<td>1</td>
</tr><tr><td><strong>rzgenie</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>48</td>
<td>36</td>
<td>1,728</td>
<td>128</td>
<td>6,144</td>
<td>58.1</td>
<td>Omni-Path</td>
<td>ASC</td>
<td>Â </td>
</tr><tr><td><strong>rzhasgpu</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2667 v3<br />NVIDIA Tesla K80 GPU</td>
<td>3.2<br />824 MHz</td>
<td>20<br />16*4</td>
<td>16<br />2496</td>
<td>320<br />159,744</td>
<td>128<br />24*2</td>
<td>2,560<br />768</td>
<td>8.2<br />59.8</td>
<td>IB QDR</td>
<td>ASC/M&amp;IC</td>
<td>1</td>
</tr><tr><td><strong>rzmanta</strong></td>
<td>OCF</td>
<td>IBM Power8<br />NVIDIA Tesla P100 (Pascal)</td>
<td>2.0-4.0<br />1481 MHz</td>
<td>44<br />36*4</td>
<td>20<br />3484</td>
<td>880<br />501,696</td>
<td>256<br />16*4</td>
<td>11,264<br />2,304</td>
<td>28.2<br />763.2</td>
<td>IB EDR</td>
<td>ASC</td>
<td>1</td>
</tr><tr><td><strong>rzslic</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>10</td>
<td>16</td>
<td>160</td>
<td>128</td>
<td>1,280</td>
<td>3.3</td>
<td>No</td>
<td>ASC</td>
<td>2</td>
</tr><tr><td><strong>rztopaz</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>768</td>
<td>36</td>
<td>27,648</td>
<td>128</td>
<td>98,304</td>
<td>929</td>
<td>Omni-Path</td>
<td>ASC</td>
<td>Â </td>
</tr><tr><td><strong>rztrona</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>20</td>
<td>36</td>
<td>720</td>
<td>128</td>
<td>2,560</td>
<td>24.2</td>
<td>No</td>
<td>ASC</td>
<td>Â </td>
</tr><tr><td><strong>shark</strong></td>
<td>SCF</td>
<td>IBM Power8<br />NVIDIA Tesla P100 (Pascal)</td>
<td>2.0-4.0<br />1481 MHz</td>
<td>44<br />36*4</td>
<td>20<br />3484</td>
<td>880<br />501,696</td>
<td>256<br />16*4</td>
<td>11,264<br />2,304</td>
<td>28.2<br />763.2</td>
<td>IB EDR</td>
<td>ASC</td>
<td>1</td>
</tr><tr><td><strong>sierra</strong></td>
<td>SCF</td>
<td>IBM Power9<br />NVIDIA Tesla V100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>4320<br />4320*4</td>
<td>44<br />5120</td>
<td>190,080<br />88,473,600</td>
<td>256<br />16*4</td>
<td>1,101,920<br />276,480</td>
<td>125,000</td>
<td>IB EDR</td>
<td>ASC</td>
<td>1</td>
</tr><tr><td><strong>surface</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2670<br />NVIDIA Tesla K40 GPU</td>
<td>2.6<br />745 MHz</td>
<td>162<br />158*2</td>
<td>16<br />2880</td>
<td>2,592<br />910,080</td>
<td>256<br />12*2</td>
<td>41,472<br />3,792</td>
<td>53.9<br />451.9</td>
<td>IB QDR</td>
<td>ASC/M&amp;IC<br />VIZ</td>
<td>Â </td>
</tr><tr><td><strong>syrah</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>324</td>
<td>16</td>
<td>5,184</td>
<td>64</td>
<td>20,736</td>
<td>107.8</td>
<td>IB QDR</td>
<td>ASC/M&amp;IC</td>
<td>1</td>
</tr><tr><td><strong>zin</strong></td>
<td>SCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>2,916</td>
<td>16</td>
<td>46,656</td>
<td>32</td>
<td>93,312</td>
<td>970.4</td>
<td>IB QDR</td>
<td>ASC</td>
<td>Â </td>
</tr></table><p><strong>Notes:</strong></p>
<ol><li>Limited access, no Generally Available or not a production system.</li>
<li>Primary use is for the transfer to storage.</li>
<li>Two Tesla K20x GPUs on 20 nodes; 6 GB memory per GPU</li>
<li>Login nodes have 48 cores; compute nodes have additional 800 GB NVRAM.</li>
<li>Jade is split into 3 subsytems - compute nodes are: jade (1302), jadeita (1270), jadedev (32)</li>
</ol><h4>Peak Comparisons</h4>
<p></p><div class="media media-element-container media-default"><div id="file-2051" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/lcmachinestflopslinear-gif-0">LCmachinesTFLOPSlinear.gif</a></h2>
    
  
  <div class="content">
    <img alt="bar graph showing teraflops linearly" height="377" width="969" class="media-element file-default" data-delta="140" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/LCmachinesTFLOPSlinear_2.gif" /></div>

  
</div>
</div>
<p></p><div class="media media-element-container media-default"><div id="file-2052" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/lcmachinestflopslog-gif-0">LCmachinesTFLOPSlog.gif</a></h2>
    
  
  <div class="content">
    <img alt="bar graph showing teraflops logarithmically" height="369" width="969" class="media-element file-default" data-delta="141" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/LCmachinesTFLOPSlog_2.gif" /></div>

  
</div>
</div>
<h3><a name="intel-systems" id="intel-systems"></a>Intel Xeon Systems</h3>
<ul><li>The majority of LC's systems are Intel Xeon based Linux clusters, and include the following processor architectures:
<ul><li>Intel Xeon 18-core E5-2695 v4 (Broadwell)</li>
<li>Intel Xeon 8-core E5-2670 (Sandy Bridge - TLCC2) w/without NVIDIA GPUs</li>
<li>Intel Xeon 12-core E5-2695 v2 (Ivy Bridge)</li>
</ul></li>
<li>Mix of resources:
<ul><li>8, 12, and 18 core processors</li>
<li>OCF and SCF</li>
<li>ASC, M&amp;IC, VIZ</li>
<li>Capacity, Grand Challenge, visualization, testbed</li>
<li>Several GPU enabled clusters</li>
</ul></li>
<li>64-bit architecture</li>
<li>TOSS operating system stack</li>
<li>InfiniBand and Intel Omni-Path interconnects</li>
<li>Hyper-threading enabled (2 threads/core)</li>
<li>Vector/SIMD operations</li>
<li>For detailed hardware information, please see the "Additional Information" references below.</li>
</ul><h4>Additional Information</h4>
<ul><li>Linux Clusters Tutorial: <a href="http://computing.llnl.gov/tutorials/linux_clusters" target="_blank">computing.llnl.gov/tutorials/linux_clusters</a></li>
<li>Usage Information for LC's new CTS-1 systems: <a href="https://lc.llnl.gov/confluence/display/TCE/TCE+Home" target="_blank">lc.llnl.gov/confluence/display/TCE/TCE+Home</a></li>
<li>Reference list of Intel Xeon Processors: <a href="http://en.wikipedia.org/wiki/List_of_Intel_Xeon_microprocessors" target="_blank">en.wikipedia.org/wiki/List_of_Intel_Xeon_microprocessors</a></li>
</ul><table><tr><td>
<p></p><div class="media media-element-container media-default"><div id="file-825" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/quartz011000pixjpeg-0">quartz01.1000pix.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/quartz01.1000pix_0.jpeg" title="Frames - CTS-1"><img alt="Frames - CTS-1" title="Frames - CTS-1" height="267" width="400" style="height: 267px; width: 400px;" class="media-element file-default" data-delta="128" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/quartz01.1000pix_0-400x267.jpeg" /></a>  </div>

  
</div>
</div><br />Quartz Intel Cluster
</td>
<td>Â </td>
<td>
<p></p><div class="media media-element-container media-default"><div id="file-815" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/zin1000pixjpeg">zin.1000pix.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/zin.1000pix.jpeg" title="Zin Intel Cluster"><img alt="Zin Intel Cluster" title="Zin Intel Cluster" height="267" width="400" style="height: 267px; width: 400px;" class="media-element file-default" data-delta="129" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/zin.1000pix-400x267.jpeg" /></a>  </div>

  
</div>
</div><br />Zin Intel Cluster
</td>
</tr></table><h4>System Details</h4>
<table class="table table-striped table-bordered"><tr><th scope="col">Cluster</th>
<th scope="col">OCF<br />SCF</th>
<th scope="col">Architecture</th>
<th scope="col">Clock Speed (CHz)</th>
<th scope="col">Nodes GPUs</th>
<th scope="col">Cores / Node / GPU</th>
<th scope="col">Cores Total</th>
<th scope="col">Memory / Node (GB)</th>
<th scope="col">Memory Total (GB)</th>
<th scope="col">TFLOPS Peak</th>
<th scope="col">Switch</th>
<th scope="col">ASC<br />M&amp;IC</th>
</tr><tr><td><strong>agate</strong></td>
<td>SCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>48</td>
<td>36</td>
<td>1,728</td>
<td>128</td>
<td>6,144</td>
<td>58.1</td>
<td>No</td>
<td>ASC</td>
</tr><tr><td><strong>borax</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>48</td>
<td>36</td>
<td>1,728</td>
<td>128</td>
<td>6,144</td>
<td>58.1</td>
<td>No</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>catalyst</strong></td>
<td>OCF</td>
<td>Intel 12-core Xeon E5-2695 v2</td>
<td>2.4</td>
<td>324</td>
<td>24</td>
<td>7,776</td>
<td>128</td>
<td>41,472</td>
<td>149.3</td>
<td>IB QDR</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>cslic</strong></td>
<td>SCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>10</td>
<td>16</td>
<td>160</td>
<td>128</td>
<td>1,280</td>
<td>3.3</td>
<td>No</td>
<td>ASC</td>
</tr><tr><td><strong>jade<br />jadeite<br />jadedev</strong></td>
<td>SCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>2,688</td>
<td>36</td>
<td>96,768</td>
<td>128</td>
<td>344,064</td>
<td>3,251.4</td>
<td>Omni-Path</td>
<td>ASC</td>
</tr><tr><td><strong>max</strong></td>
<td>SCF</td>
<td>Intel 8-core Xeon E5-2670<br />NVIDIA Tesla K20x</td>
<td>2.6<br />732 MHz</td>
<td>324<br />20*2</td>
<td>16<br />2688</td>
<td>5,184<br />107,520</td>
<td>256<br />6*2</td>
<td>78,336<br />240</td>
<td>108<br />52.4</td>
<td>IB QDR</td>
<td>ASC VIZ</td>
</tr><tr><td><strong>mica</strong></td>
<td>SCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>384</td>
<td>36</td>
<td>13,824</td>
<td>128</td>
<td>49,152</td>
<td>464.5</td>
<td>Omni-Path</td>
<td>ASC</td>
</tr><tr><td><strong>oslic</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>10</td>
<td>16</td>
<td>160</td>
<td>128</td>
<td>1,280</td>
<td>3.3</td>
<td>No</td>
<td>ASC</td>
</tr><tr><td><strong>pascal</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>171<br />163*2</td>
<td>36<br />3484</td>
<td>6,156<br />1,135,784</td>
<td>256<br />16*2</td>
<td>18,176<br />5,216</td>
<td>206.8<br />1,727.8</td>
<td>Omni-Path</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>pinot</strong></td>
<td>ISNSI</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>162</td>
<td>16</td>
<td>2,592</td>
<td>64</td>
<td>10,368</td>
<td>53.9</td>
<td>IB QDR</td>
<td>M&amp;IC</td>
</tr><tr><td><strong>quartz</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>3,072</td>
<td>36</td>
<td>110,592</td>
<td>128</td>
<td>393,216</td>
<td>3,715.9</td>
<td>Omni-Path</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>rzalastor</strong></td>
<td>OCF</td>
<td>Intel 10-core Xeon E5-2670 v2</td>
<td>2.8</td>
<td>36</td>
<td>20</td>
<td>720</td>
<td>64</td>
<td>2,304</td>
<td>16.1</td>
<td>IB QDR</td>
<td>ASC</td>
</tr><tr><td><strong>rzgenie</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>48</td>
<td>36</td>
<td>1,728</td>
<td>128</td>
<td>6,144</td>
<td>58.1</td>
<td>Omni-Path</td>
<td>ASC</td>
</tr><tr><td><strong>rzhasgpu</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2667 v3<br />NVIDIA Tesla K80 GPU</td>
<td>3.2<br />824 MHz</td>
<td>20<br />16*4</td>
<td>16<br />2496</td>
<td>320<br />159,744</td>
<td>128<br />24*2</td>
<td>2,560<br />768</td>
<td>8.2<br />59.8</td>
<td>IB QDR</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>rzslic</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>10</td>
<td>16</td>
<td>160</td>
<td>128</td>
<td>1,280</td>
<td>3.3</td>
<td>No</td>
<td>ASC</td>
</tr><tr><td><strong>rztopaz</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>768</td>
<td>36</td>
<td>27,648</td>
<td>128</td>
<td>98,304</td>
<td>929</td>
<td>Omni-Path</td>
<td>ASC</td>
</tr><tr><td><strong>rztrona</strong></td>
<td>OCF</td>
<td>Intel 18-core Xeon E5-2695 v4</td>
<td>2.1</td>
<td>20</td>
<td>36</td>
<td>720</td>
<td>128</td>
<td>2,560</td>
<td>24.2</td>
<td>No</td>
<td>ASC</td>
</tr><tr><td><strong>surface</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2670<br />NVIDIA Tesla K40 GPU</td>
<td>2.6<br />745 MHz</td>
<td>162<br />158*2</td>
<td>16<br />2880</td>
<td>2,592<br />910,080</td>
<td>256<br />12*2</td>
<td>41,472<br />3,792</td>
<td>53.9<br />451.9</td>
<td>IB QDR</td>
<td>ASC/M&amp;IC<br />VIZ</td>
</tr><tr><td><strong>syrah</strong></td>
<td>OCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>324</td>
<td>16</td>
<td>5,184</td>
<td>64</td>
<td>20,736</td>
<td>107.8</td>
<td>IB QDR</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>zin</strong></td>
<td>SCF</td>
<td>Intel 8-core Xeon E5-2670</td>
<td>2.6</td>
<td>2,916</td>
<td>16</td>
<td>46,656</td>
<td>32</td>
<td>93,312</td>
<td>970.4</td>
<td>IB QDR</td>
<td>ASC</td>
</tr></table><p><span class="note-red">Note:</span> The pinot cluster is dedicated to <a href="https://wiki.llnl.gov/index.php/ISNSI_FAQs" target="_blank">ISNSI</a> use.</p>
<h3><a name="coral-systems" id="coral-systems"></a>CORAL Systems</h3>
<h4>CORAL</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-967" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/corallogo-jpg">coralLogo.jpg</a></h2>
    
  
  <div class="content">
    <img alt="CORAL collaboration logo" height="190" width="392" style="width: 392px; height: 190px;" class="media-element file-default" data-delta="95" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/coralLogo.jpg" /></div>

  
</div>
</div></div>
<ul><li>CORAL = <strong>C</strong>ollaboration <strong>O</strong>ak <strong>R</strong>idge, <strong>A</strong>rgonne, <strong>L</strong>ivermore</li>
<li>A first-of-its-kind U.S. DOEÂ collaboration between the NNSA's <a href="https://nnsa.energy.gov/asc" target="_blank">ASC Program</a> and the <a href="https://www.energy.gov/science/office-science" target="_blank">Office of Science's</a> <a href="https://www.energy.gov/science/ascr/advanced-scientific-computing-research" target="_blank">Advanced Scientific Computing Research program (ASCR)</a>.</li>
<li>CORAL is the next major phase in the DOE's scientific computing roadmap and path to exascale computing.</li>
<li>Will culminate in three ultra-high performance supercomputers at Lawrence Livermore, Oak Ridge, and Argonne national laboratories.</li>
<li>Will be used for the most demanding scientific and national security simulation and modeling applications, and will enable continued U.S. leadership in computing.</li>
<li>The three CORAL systems are:
<ul><li>Argonne: <a href="http://aurora.alcf.anl.gov/" target="_blank">Aurora</a></li>
<li>LLNL: <a href="https://asc.llnl.gov/coral-info" target="_blank">Sierra</a></li>
<li>ORNL: <a href="https://www.olcf.ornl.gov/summit/" target="_blank">Summit</a></li>
</ul></li>
<li>LLNL and ORNL systems were delivered in the 2017-18 timeframe. The Argonne system's planned delivery (revised) is in 2021.</li>
</ul><h4>CORAL Early Access (EA) Systems</h4>
<table><tr><td>
<ul><li>In preparation for the final delivery Sierra systems, LLNL has implemented three "early access" systems, one on each network:
<ul><li>ray - OCF-CZ</li>
<li>rzmanta - OCF-RZ</li>
<li>shark - SCF</li>
</ul></li>
<li>Primary purpose was to provide platforms where Tri-lab users can begin porting and preparing for the hardware and software that will be delivered with the final Sierra systems. Still available for development and testing purposes.</li>
<li>Similar to the final delivery Sierra systems but use the previous generation IBM Power processors and NVIDIA GPUs.</li>
<li>IBM Power Systems S822LC Server: Hybrid architecture using IBM POWER8+ processors and NVIDIA Pascal GPUs.</li>
<li>IBM POWER8+ processors:
<ul><li>2 per node (dual-socket)</li>
<li>10 cores/socket; 20 cores per node</li>
<li>8 SMT threads per core; 160 SMT threads per node</li>
<li>Clock: due to adaptive power management options, the clock speed can vary depending upon the system load. At LC speeds can vary from approximately 2 GHz - 4 GHz.</li>
</ul></li>
<li>NVIDIA GPUs:
<ul><li>4 NVIDIA Tesla P100 (Pascal) GPUs per compute node (not on login/service nodes)</li>
<li>3584 CUDA cores per GPU; 14,336 per node</li>
</ul></li>
<li>Memory:
<ul><li>256 GB DDR4 per node</li>
<li>16 GB HBM2 (High Bandwidth Memory 2) per GPU; 732 GB/s peak bandwidth</li>
</ul></li>
<li>NVLINK 1.0:
<ul><li>Interconnect for GPU-GPU and CPU-GPU shared memory</li>
<li>4 links per GPU with 160 GB/s total bandwidth</li>
</ul></li>
<li>NVRAM: 1.6 TB NVMe PCIe SSD per compute node (CZ ray system only)</li>
<li>Network:
<ul><li>Mellanox 100 Gb/s Enhanced Data Rate (EDR) InfiniBand</li>
<li>One dual-port 100 Gb/s EDR Mellanox adapter per node</li>
</ul></li>
<li>Parallel File System: IBM Spectrum Scale (GPFS)
<ul><li>ray: 1.3 PB</li>
<li>rzmanta: 431 TB</li>
<li>shark: 431 TB</li>
</ul></li>
<li>Batch System: IBM Spectrum LSF</li>
<li>Additional information:
<ul><li>User Guide: <a href="https://lc.llnl.gov/confluence/display/CORALEA/CORAL+EA+Systems" target="_blank">lc.llnl.gov/confluence/display/CORALEA/CORAL+EA+Systems</a> (LC internal wiki)</li>
<li>ray configuration: <a href="/hardware/platforms/Ray">hpc.llnl.gov/hardware/platforms/Ray</a></li>
<li>rzmanta configuration: <a href="/hardware/platforms/RZManta">hpc.llnl.gov/hardware/platforms/RZManta</a></li>
<li>shark configuration: <a href="/hardware/platforms/Shark">hpc.llnl.gov/hardware/platforms/Shark</a></li>
</ul></li>
</ul></td>
<td>
<p></p><div class="media media-element-container media-default"><div id="file-984" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/rayphoto1-jpg-0">rayPhoto1.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/rayPhoto1_0.jpg"><img alt="CORAL EA Ray Cluster" height="266" width="400" style="height: 266px; width: 400px;" class="media-element file-default" data-delta="130" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/rayPhoto1_0-400x266.jpg" /></a>  </div>

  
</div>
</div>
<p>CORAL EA Ray Cluster</p>
</td>
</tr></table><h5>System Details</h5>
<table class="table table-striped table-bordered"><tr><th scope="col">Cluster</th>
<th scope="col">OCF<br />SCF</th>
<th scope="col">Architecture</th>
<th scope="col">Clock Speed (CHz)</th>
<th scope="col">Nodes GPUs</th>
<th scope="col">Cores / Node / GPU</th>
<th scope="col">Cores Total</th>
<th scope="col">Memory / Node (GB)</th>
<th scope="col">Memory Total (GB)</th>
<th scope="col">TFLOPS Peak</th>
<th scope="col">Switch</th>
<th scope="col">ASC<br />M&amp;IC</th>
</tr><tr><td><strong>ray</strong></td>
<td>OCF</td>
<td>IBM Power8<br />NVIDIA Tesla P100 (Pascal)</td>
<td>2.0-4.0<br />1481 MHz</td>
<td>62<br />54*4</td>
<td>20<br />3484</td>
<td>1,240<br />752,544</td>
<td>256<br />16*4</td>
<td>15,872<br />3,456</td>
<td>39.7<br />1,144.8</td>
<td>IB EDR</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>rzmanta</strong></td>
<td>OCF</td>
<td>IBM Power8<br />NVIDIA Tesla P100 (Pascal)</td>
<td>2.0-4.0<br />1481 MHz</td>
<td>44<br />36*4</td>
<td>20<br />3484</td>
<td>880<br />501,696</td>
<td>256<br />16*4</td>
<td>11,264<br />2,304</td>
<td>28.2<br />763.2</td>
<td>IB EDR</td>
<td>ASC</td>
</tr><tr><td><strong>shark</strong></td>
<td>SCF</td>
<td>IBM Power8<br />NVIDIA Tesla P100 (Pascal)</td>
<td>2.0-4.0<br />1481 MHz</td>
<td>44<br />36*4</td>
<td>20<br />3484</td>
<td>880<br />501,696</td>
<td>256<br />16*4</td>
<td>11,264<br />2,304</td>
<td>28.2<br />763.2</td>
<td>IB EDR</td>
<td>ASC</td>
</tr></table><h4>Sierra Systems</h4>
<table><tr><td>
<ul><li>Sierra is a classified, 125-petaflop, IBM Power Systems AC922 hybrid architecture system comprised of IBM POWER9 nodes with NVIDIA Volta GPUs. Sierra is a Tri-lab resource sited at LLNL.</li>
<li>Unclassified Sierra systems are similar, but smaller, and include:
<ul><li><strong>Lassen</strong> - a 20-petaflop system located on LC's CZ zone</li>
<li><strong>rzansel</strong> - a 1.5-petaflop system is located on LC's RZ zone</li>
</ul></li>
<li>IBM Power Systems AC922 Server: Hybrid architecture using IBM POWER9 processors and NVIDIA Volta GPUs.</li>
<li>IBM POWER9 processors (compute nodes):
<ul><li>2 per node (dual-socket)</li>
<li>22 cores/socket; 44 cores per node</li>
<li>4 SMT threads per core; 176 SMT threads per node</li>
<li>Clock: due to adaptive power management options, the clock speed can vary depending upon the system load. At LC speeds can vary from approximately 2.3 - 3.8 GHz. LC can also set the clock to a specific speed regardless of workload.</li>
</ul></li>
<li>NVIDIA GPUs:
<ul><li>4 NVIDIA Tesla V100 (Volta) GPUs per compute, login, launch node</li>
<li>5120 CUDA cores per GPU; 20,480 per node</li>
</ul></li>
<li>Memory:
<ul><li>256 GB DDR4 per compute node</li>
<li>16 GB HBM2 (High Bandwidth Memory 2) per GPU; 900 GB/s peak bandwidth</li>
</ul></li>
<li>NVLINK 2.0:
<ul><li>Interconnect for GPU-GPU and CPU-GPU shared memory</li>
<li>6 links per GPU with 300 GB/s total bandwidth</li>
</ul></li>
<li>NVRAM: 1.6 TB NVMe PCIe SSD per compute node</li>
<li>Network:
<ul><li>Mellanox 100 Gb/s Enhanced Data Rate (EDR) InfiniBand</li>
<li>One dual-port 100 Gb/s EDR Mellanox adapter per node</li>
</ul></li>
<li>Parallel File System: IBM Spectrum Scale (GPFS)</li>
<li>Batch System: IBM Spectrum LSF</li>
<li>Water (warm) cooled compute nodes</li>
<li>Additional information:
<ul><li>Tutorial: <a href="/training/tutorials/using-lcs-sierra-system">hpc.llnl.gov/training/tutorials/using-lcs-sierra-system</a></li>
<li>User Guide: <a href="https://lc.llnl.gov/confluence/display/SIERRA/Sierra+Systems" target="_blank">lc.llnl.gov/confluence/display/SIERRA/Sierra+Systems</a> (LC internal wiki)</li>
</ul></li>
</ul></td>
<td>
<p></p><div class="media media-element-container media-default"><div id="file-1827" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/sierra-img-3714-png-2">sierra.IMG_3714.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierra.IMG_3714_2.png"><img alt="Sierra" height="200" width="400" style="height: 200px; width: 400px;" class="media-element file-default" data-delta="131" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierra.IMG_3714_2-400x200.png" /></a>  </div>

  
</div>
</div>
<p>Sierra</p>
</td>
</tr></table><h5>System Details</h5>
<table class="table table-striped table-bordered"><tr><th scope="col">Cluster</th>
<th scope="col">OCF<br />SCF</th>
<th scope="col">Architecture</th>
<th scope="col">Clock Speed (CHz)</th>
<th scope="col">Nodes GPUs</th>
<th scope="col">Cores / Node / GPU</th>
<th scope="col">Cores Total</th>
<th scope="col">Memory / Node (GB)</th>
<th scope="col">Memory Total (GB)</th>
<th scope="col">TFLOPS Peak</th>
<th scope="col">Switch</th>
<th scope="col">ASC<br />M&amp;IC</th>
</tr><tr><td><strong>sierra</strong></td>
<td>SCF</td>
<td>IBM Power9<br />NVIDIA Tesla V100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>4320<br />4320*4</td>
<td>44<br />5120</td>
<td>190,080<br />88,473,600</td>
<td>256<br />16*4</td>
<td>1,101,920<br />276,480</td>
<td>125,000</td>
<td>IB EDR</td>
<td>ASC</td>
</tr><tr><td><strong>lassen</strong></td>
<td>OCF</td>
<td>IBM Power9<br />NVIDIA Tesla V100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>774<br />774*4</td>
<td>44<br />5120</td>
<td>34,056<br />15,851,520</td>
<td>256<br />16*4</td>
<td>198,144<br />49,536</td>
<td>22,508</td>
<td>IB EDR</td>
<td>ASC/M&amp;IC</td>
</tr><tr><td><strong>rzansel</strong></td>
<td>OCF</td>
<td>IBM Power9<br />NVIDIA Tesla V100 (Volta)</td>
<td>2.3-3.8<br />1530 MHz</td>
<td>54<br />54*4</td>
<td>44<br />5120</td>
<td>2376<br />1,105,920</td>
<td>256<br />16*4</td>
<td>13,824<br />3,456</td>
<td>1,570</td>
<td>IB EDR</td>
<td>ASC</td>
</tr></table><h3><a name="future-systems" id="future-systems"></a>Future Systems</h3>
<h4>Advanced Technology Systems (ATS)</h4>
<ul><li>Supercomputers dedicated to the largest and most complex calculations critical to stockpile stewardship; "capability computing"</li>
<li>Typically include leading-edge/novel architecture components, custom engineering</li>
<li>Shared across the Tri-labs; accounts granted to projects via a formal proposal process</li>
<li>ATS-3 "Crossroads": Will be sited at LANL</li>
<li>ATS-4 "El Capitan": Will be sited at LLNL</li>
</ul><h4>Commodity Technology Systems (CTS)</h4>
<ul><li>Robust, cost-effective systems to meet the day-to-day simulation workload needs of the ASC program; "work-horse, capacity computing"</li>
<li>Common Tri-Lab procurement with platforms delivered to all three labs; accounts handled independently by each lab.</li>
<li>CTS-1 systems are currently in production at all three labs.</li>
<li>CTS-2: TBA</li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-1829" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/ascplatformtimeline-png">ASCplatformTimeline.png</a></h2>
    
  
  <div class="content">
    <img alt="timeline of CTS and ATS procurements at national labs" height="880" width="1478" class="media-element file-default" data-delta="119" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/ASCplatformTimeline.png" /></div>

  
</div>
</div>
<h3><a name="linux-cluster" id="linux-cluster">Typical LC Linux Cluster</a></h3>
<h4>Basic Components</h4>
<ul><li>Currently, LC has several types of production Linux clusters based on the following processor architectures:
<ul><li>Intel Xeon 18-core E5-2695 v4 (Broadwell)</li>
<li>Intel Xeon 8-core E5-2670 (Sandy Bridge - TLCC2) w/without NVIDIA GPUs</li>
<li>Intel Xeon 12-core E5-2695 (Ivy Bridge)</li>
</ul></li>
<li>All of LC's Linux clusters differ in their configuration details, however they do share the same basic hardware building blocks:
<ul><li>Nodes</li>
<li>Frames / racks</li>
<li>High speed interconnect (most clusters)</li>
<li>Other hardware (file systems, management hardware, etc.)</li>
</ul></li>
</ul><h4>Nodes</h4>
<ul><li>The basic building block of a Linux cluster is the node. A node is essentially an independent computer. Key features:
<ul><li>Self-contained, diskless, multi-core computer.</li>
<li>Low form-factor - Clusters nodes are very thin to save space.</li>
<li>Rack Mounted - Nodes are mounted compactly in a drawer fashion to facilitate maintenance, reduced footprint, etc.</li>
<li>Remote Management - There is no keyboard, mouse, monitor or other device typically used to interact with a computer. All node management occurs over the network from a "management" node.</li>
</ul></li>
<li>ExampleÂ (click for larger image):</li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-822" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/cts-1nodejpeg">CTS-1node.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/CTS-1node.jpeg" title="Single compute node - CTS-1"><img alt="Single compute node - CTS-1" title="Single compute node - CTS-1" height="160" width="600" style="width: 600px; height: 160px;" class="media-element file-default" data-delta="28" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/CTS-1node-600x160.jpeg" /></a>  </div>

  
</div>
</div>
<p>Single compute node - CTS-1</p>
<ul><li>In general, an LC production cluster has four types of nodes, based upon function, which can differ in configuration details:
<ul><li>Login</li>
<li>Interactive/debug</li>
<li>Batch</li>
<li>I/O and service nodes (unavailable to users)</li>
</ul></li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-823" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/nodetypesgif">nodeTypes.gif</a></h2>
    
  
  <div class="content">
    <img alt="Typical LC system diagram" height="253" width="500" class="media-element file-default" data-delta="29" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/nodeTypes.gif" /></div>

  
</div>
</div>
<p><strong>Login nodes:</strong></p>
<ul><li>Every system has a designated number of login nodes - depends upon the size of the system. Some examples:<br />agate = 2<br />sierra = 5<br />quartz = 14<br />zin = 20</li>
<li>Login nodes are shared by multiple users</li>
<li>Primarily used for interactive work such as editing files, submitting batch jobs, compiling, running GUIs, etc.</li>
<li>Interactive use exclusively - login only nodes do not permit any batch jobs.</li>
<li>DO NOT run production jobs on login nodes! Remember, you are sharing login nodes with other users.</li>
</ul><p><strong>Interactive/debug (pdebug) nodes:</strong></p>
<ul><li>Most LC systems have nodes that are designated for interactive work.</li>
<li>Meant for testing, prototyping, debugging, and small, short jobs</li>
<li>Cannot be logged into (rsh) unless you already have a job running on them</li>
<li>Nodes run one job at a time - not shared like login nodes</li>
<li>Can also be used through the batch system</li>
</ul><p><strong>Batch (pbatch) nodes:</strong></p>
<ul><li>Comprise the majority of nodes on each system</li>
<li>Meant for production work</li>
<li>Work is submitted via a batch scheduler (Slurm, Moab</li>
<li>Cannot be logged into (rsh) unless you already have a job running on them</li>
<li>Nodes run one job at a time - not shared like login nodes</li>
</ul><h4>Frames / Racks</h4>
<ul><li>Frames are the physical cabinets that hold most of a cluster's components:
<ul><li>Nodes of various types</li>
<li>Switch components</li>
<li>Other network and cluster management components</li>
<li>Parallel file system disk resources (usually in separate racks)</li>
</ul></li>
<li>Vary in size/appearance between the different Linux clusters at LC.</li>
<li>Power and console management - frames include hardware and software that allow system administrators to perform most tasks remotely.</li>
<li>Example images below (click for larger image):</li>
</ul><table><tr><td>
<p></p><div class="media media-element-container media-default"><div id="file-2056" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/sierra-img-3708-png">sierra.IMG_3708.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierra.IMG_3708.png"><img alt="Sierra supercomputer" height="250" width="499" style="height: 250px; width: 499px;" class="media-element file-default" data-delta="145" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierra.IMG_3708-499x250.png" /></a>  </div>

  
</div>
</div><br />Frames - Sierra
</td>
<td>Â </td>
<td>
<p></p><div class="media media-element-container media-default"><div id="file-825--2" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/quartz011000pixjpeg-0">quartz01.1000pix.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/quartz01.1000pix_0.jpeg" title="Frames - CTS-1"><img alt="Frames - CTS-1" title="Frames - CTS-1" height="250" width="400" style="width: 400px; height: 250px;" class="media-element file-default" data-delta="31" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/quartz01.1000pix_0-400x250.jpeg" /></a>  </div>

  
</div>
</div><br />Frames - Quartz
</td>
</tr></table><h4>Scalable Unit</h4>
<ul><li>The basic building block of LC's production Linux clusters is called a "Scalable Unit" (SU). An SU consists of:
<ul><li>Nodes (compute, login, management, gateway)</li>
<li>First stage switches that connect to each node directly</li>
<li>Miscellaneous management hardware</li>
<li>Frames sufficient to house all required hardware</li>
<li>Additionally, second stage switch hardware is needed to connect multi-SU clusters (not shown).</li>
</ul></li>
<li>The number of nodes in an SU depends upon the type of switch hardware being used. For example:
<ul><li>QLogic = 162 nodes</li>
<li>Intel Omni-Path = 192 nodes</li>
</ul></li>
<li>Multiple SUs are combined to create a cluster. For example:
<ul><li>2 SU = 324 / 384 nodes</li>
<li>4 SU = 648 / 768 nodes</li>
<li>8 SU = 1296 / 1536 nodes</li>
</ul></li>
<li>The SU design is meant to:
<ul><li>Standardize configuration details across the enterprise</li>
<li>Easily "grow" clusters in incremental units</li>
<li>Leverage procurements and reduce costs across the Tri-labs</li>
</ul></li>
<li>An example of a 2 SU cluster is shown below for illustrative purposes. Note that a frame holding the second level switch hardware is not shown.</li>
</ul><h3><a name="interconnects" id="interconnects"></a>Interconnects</h3>
<ul><li>Types of interconnects:
<ul><li>Varies by cluster; a few clusters do not have interconnects.</li>
<li>Intel Xeon CTS-1 clusters use Intel Omni-Path.</li>
<li>Most other Intel Xeon clusters use 4x QDR (Quad Data Rate) QLogic InfiniBand</li>
<li>CORAL/Sierra clusters use Mellanox EDR (Enhanced Data Rate) InfiniBand</li>
</ul></li>
<li>Bandwidths:
<ul><li>QLogic 4x QDR = 40 Gbits/sec</li>
<li>Intel Omni-Path = 100 Gbits/sec</li>
<li>Mellanox EDR = 100 Gbits/sec</li>
</ul></li>
</ul><h4>Primary Components</h4>
<p><span class="note-red">Note:</span> For additional details on Sierra systems see: <a href="/training/tutorials/using-lcs-sierra-system">hpc.llnl.gov/training/tutorials/using-lcs-sierra-system</a></p>
<p><strong>Adapter Card</strong></p>
<ul><li>Communications processor packaged on network PCI Express adapter card.</li>
<li>Remote Direct Memory Access (RDMA) improves communication bandwidth by off-loading communications from the CPU.</li>
<li>Provides the interface between a node and a two-stage network.</li>
<li>Connected to a first stage switch by copper cable (most cases) .</li>
<li>Types: Intel Omni-Path, QLogic 4x QDR IB, Mellanox EDR InfiniBand</li>
</ul><table><tr><td><div class="media media-element-container media-default"><div id="file-856" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/omnipathadapterjpg-0">omnipathAdapter.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/omnipathAdapter_0.jpg"><img alt="Omni-Path Fabric Adapter" height="250" width="257" style="width: 257px; height: 250px;" class="media-element file-default" data-delta="35" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/omnipathAdapter_0-257x250.jpg" /></a>  </div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-855" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/qlogicadaptergif-0">qlogicAdapter.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/qlogicAdapter_0.gif"><img alt="QLogic IB Adapter" height="250" width="222" style="width: 222px; height: 250px;" class="media-element file-default" data-delta="34" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/qlogicAdapter_0-222x250.gif" /></a>  </div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-2057" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mellanoxconnectx-5adapter-jpg-1">mellanoxConnectX-5adapter.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/mellanoxConnectX-5adapter_1.jpg"><img alt="Mellanox EDR InfiniBand Adapter" height="250" width="317" style="height: 250px; width: 317px;" class="media-element file-default" data-delta="146" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/mellanoxConnectX-5adapter_1-317x250.jpg" /></a>  </div>

  
</div>
</div></td>
</tr><tr><td>
<p><a href="/sites/default/files/omnipathAdapter.pdf">Omni-Path Fabric Adapter PDF</a><br /><em>(Image source: Intel)</em></p>
</td>
<td>QLogic IB Adapter<br /><em>(Image source: QLogic)</em></td>
<td>Mellanox EDR InfiniBand Adapter<br />(Image source: Mellanox)</td>
</tr></table><p><strong>1st Stage Switch:</strong></p>
<ul><li><a href="/sites/default/files/omnipath1stStage.pdf">Intel Omni-Path 48-port</a>: 32 ports connect to adapters in nodes and 16 ports connect to second stage switches.</li>
<li>QLogic QDR 36-port: 18 ports connect to adapters in nodes and 18 ports connect to second stage switches.</li>
<li>Mellanox Switch-IB 36-port: 18 ports connect to adapters in nodes and 12 ports connect to second stage switches.</li>
</ul><p><strong>2nd Stage Switch:</strong></p>
<ul><li><a href="/sites/default/files/omnipath2ndStage.pdf">Intel Omni-Path 768-port</a>: all used ports connect to a first stage switch via optic fiber cabling.</li>
<li>QLogic QDR 18-864 port: all used ports connect to a first stage switch via optic fiber cabling.</li>
<li><a href="/sites/default/files/Mellanox.CS7500switch_0.pdf">Mellanox CS7500 648-port</a>:Â all used ports connect to a first stage switch via optic fiber cabling.</li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-857" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/ibswitchesqlogicjpg-0">IBswitchesQLogic.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/IBswitchesQLogic_0.jpg"><img alt="QLogic 1st and 2nd Stage Switches (back)" height="333" width="500" style="height: 333px; width: 500px;" class="media-element file-default" data-delta="36" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/IBswitchesQLogic_0-500x333.jpg" /></a>  </div>

  
</div>
</div>
<p>QLogic 1st and 2nd Stage Switches (back)</p>
<h4>Topology</h4>
<ul><li>Two-stage, federated, bidirectional, fat-tree.</li>
<li>Examples:</li>
</ul><table><tr><td><div class="media media-element-container media-default"><div id="file-858" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/2688wayomnipathgif">2688wayOmniPath.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/2688wayOmniPath.gif"><img alt="2688 Way Omni-Path" height="220" width="385" style="width: 385px; height: 220px;" class="media-element file-default" data-delta="37" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/2688wayOmniPath-385x220.gif" /></a>  </div>

  
</div>
</div><br />2688-way Interconnect<br />Jade - 14 SU</td>
<td><div class="media media-element-container media-default"><div id="file-2059" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/sierranetwork-jpg-1">sierraNetwork.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/sierraNetwork_1.jpg"><img alt="diagram of Sierra&amp;#039;s node and rack topology" height="220" width="242" style="width: 242px; height: 220px;" class="media-element file-default" data-delta="147" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/sierraNetwork_1-242x220.jpg" /></a>  </div>

  
</div>
</div><br />Sierra cluster</td>
</tr></table><h4>Performance</h4>
<ul><li>The inter-node bandwidth measurements below were taken on live, <strong><em>heavily loaded</em></strong> LC machines using a simple MPI non-blocking test code. One task on each of two nodes. Message size = 1 MB. Not all systems are represented. Your mileage may vary.</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">System Type</th>
<th scope="col">Latency</th>
<th scope="col">Bandwidth</th>
</tr><tr><td>Intel Xeon Clusters with QDR QLogic</td>
<td>~1-2 us</td>
<td>~4.1 GB/sec</td>
</tr><tr><td>Intel Xeon Clusters with QDR QLogic (TLCC2)</td>
<td>~1 us</td>
<td>~5.0 GB/sec</td>
</tr><tr><td>Intel Xeon Clusters with Intel Omni-Path (CTS-1)</td>
<td>~1 us</td>
<td>~21 GB/sec</td>
</tr><tr><td>Sierra Clusters with Mellanox EDR Infiniband</td>
<td>~1 us</td>
<td>~21 GB/sec</td>
</tr></table><h3><a name="facilities" id="facilities"></a>Facilities, Machine Room Tours, Photos</h3>
<h4>Facilities</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-860" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/tsf2jpeg">TSF2.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/TSF2.jpeg"><img alt="Building 453" height="333" width="400" style="height: 333px; width: 400px;" class="media-element file-default" data-delta="38" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/TSF2-400x333.jpeg" /></a>  </div>

  
</div>
</div><br /><div class="media media-element-container media-default"><div id="file-861" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/tsf1jpeg-0">TSF1.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/TSF1_0.jpeg"><img alt="LCC from across the lake" height="333" width="400" style="height: 333px; width: 400px;" class="media-element file-default" data-delta="39" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/TSF1_0-400x333.jpeg" /></a>  </div>

  
</div>
</div></div>
<ul><li>Most of LC's computing resources are located in the Livermore Computing Complex (LCC) building 453, and buildings 451 and 654. The LCC was formerly known as the Terascale Simulation Facility (TSF).</li>
<li>Map available <a href="/files/llnlmap2-gif">here</a></li>
</ul><ul><li>LCC highlights:
<ul><li>Four-story office tower with 121,600 square feet for 285 offices, a visualization theater, a 150-seat auditorium, and several conference rooms on each floor.</li>
<li>Machine room with 48,000 square feet of unobstructed computer room floor</li>
<li>30 megawatts machine power capacity</li>
<li>Mechanical cooling system with cooling towers boasting total capacity of 12,600 gallons per minute, a chiller plant with total capacity of 7,200 tons, and air handlers with a total capacity of 2,720,000 cubic feet per minute</li>
<li>3,600-gallon-per-minute, closed-loop, liquid-cooling system for Sequoia that can cool up to 9.6 megawatts.</li>
</ul></li>
<li>LC's building 654 comprises 6,000 square feet of computer floor space and is scalable up to 7.5 MW. <a href="/files/b654schematic-jpg">B654 schematic drawing</a></li>
<li>Additional reading/viewing:
<ul><li><a href="https://asc.llnl.gov/facilities" target="_blank">asc.llnl.gov/facilities</a></li>
</ul></li>
</ul><h4>Machine Room Tours</h4>
<ul><li>LLNL hosts can request tours of the B453 machine room for visitors and groups. Hosts are responsible for providing Administrative Escorts (AE) and ensuring AE policies/rules are followed.</li>
<li>Tour participants must be U.S. citizens.</li>
<li>For LCC building 453 tour information, contact <a href="mailto:hpc-tours@llnl.gov">hpc-tours@llnl.gov</a>.</li>
<li><em><strong>Summer students</strong></em>: "Virtual" tours are offered for summer students). See the Lab Events Calendar for details and registration: <a href="https://ebb.llnl.gov/" target="_blank">ebb.llnl.gov</a>.</li>
</ul><h4>Machine Photos</h4>
<ul><li>Photo collections of some LC systems, present and past, are available at this internal URL that requires authentication:Â lc.llnl.gov/confluence/display/gallery/Photo+Gallery+of+LC+Systems</li>
</ul><h2><a name="accounts" id="accounts"></a>Accounts</h2>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-969" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/lc-idmscreen-gif">lc-idmScreen.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/lc-idmScreen_0.gif"><img alt="LC Identity Management Screen" height="752" width="500" style="height: 752px; width: 500px;" class="media-element file-default" data-delta="97" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/lc-idmScreen_0-500x752.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>The process for obtaining an LC account varies, depending upon factors such as:
<ul><li>Lab employee?</li>
<li>Collaborator (non-employee)?</li>
<li>Foreign national?</li>
<li>Classified or unclassified?</li>
</ul></li>
<li>It also involves more than one account processing system:
<ul><li><a href="https://ezid.llnl.gov/" target="_blank">EZid Identity Management System</a> for setting up and managing LLNL-wide identities and collaborations.</li>
<li><a href="https://lc-idm.llnl.gov/" target="_blank">Livermore Computing Identity Management System</a> for managing LC machine accounts.</li>
<li><a href="https://vts.llnl.gov/" target="_blank">Visitor Tracking System</a> for managing LLNL-wide foreign national processes.</li>
<li><a href="https://sarape.sandia.gov/" target="_blank">SARAPE</a> system for managing Tri-lab and ASC Alliances accounts.</li>
</ul></li>
<li>Because things can get a little complex, you should consult the LC accounts documentation at: <a href="/accounts">hpc.llnl.gov/accounts</a>.</li>
<li>One Time Password (OTP) Tokens:
<ul><li>For OCF accounts, you will receive via US mail, an RSA One-time Password (OTP) token. Instructions on how to activate and use this token are included with your account notification email.</li>
<li>For OCF RZ accounts, you will also receive an RZ RSA OTP token.</li>
<li>For SCF accounts, you will be asked to visit the LC Hotline to obtain your OTP token and setup your PIN.</li>
</ul></li>
<li>Required training: All account requests require completion of online training before they are activated.</li>
<li>Annual renewal: Accounts are subject to annual revalidations and completion of online training.</li>
<li>Foreign national accounts require additional processing and take longer to set up.</li>
<li>Virtual Private Network (VPN) Account: for remote access may also be required. Discussed later under "VPN Remote Access Service."</li>
<li>Questions? Contact the LC Hotline: (925) 422-4533 <a href="mailto:lc-support@llnl.gov">lc-support@llnl.gov</a></li>
</ul><h2><a name="lc-systems" id="lc-systems">Accessing LC Systems</a></h2>
<h3><a name="passwords" id="passwords"></a>Passwords, Authentication, and OTP Tokens</h3>
<h4>One-time Passwords (OTP)</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-862" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/otp3gif-0">otp3.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/otp3_0.gif"><img alt="OTP Token" height="104" width="220" style="width: 220px; height: 104px;" class="media-element file-default" data-delta="40" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/otp3_0-220x104.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>Single-use passwords are mandatory on all LC machines: classified and unclassified.</li>
<li>Based upon a "two factor" authentication:
<ul><li>static, 4-8 character alphanumeric PIN for every user</li>
<li>6-digit random number generated by an RSA SecureID token device (similar to a cryptocard).</li>
</ul></li>
<li>OTP authentication is also used for other services:
<ul><li>Access to internal web pages</li>
<li>Remote Access Services such as VPN (discussed later)</li>
</ul></li>
</ul><h4>OCF Collaboration Zone (CZ) or Restricted Zone (RZ)</h4>
<ul><li>LC's unclassified HPC systems are configured into two separate zones:</li>
<li>Collaboration Zone (CZ):
<ul><li>Most unclassified HPC clusters are in CZ, which will permit Foreign Nationals.</li>
<li>CZ machines can be accessed directly from anywhere on the Internet.</li>
<li>Authenticate with your LC username and PIN + OTP RSA token.</li>
<li>LLNL's VPN service is not required.</li>
<li>LANL/Sandia users: see the LANL/Sandia Access Methods section for differences.</li>
</ul></li>
</ul><div class="float-right"><div class="media media-element-container media-default"><div id="file-865" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/rzotpgif-0">rzotp.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/rzotp_0.gif"><img alt="RZ OTP Token" height="103" width="220" style="height: 103px; width: 220px;" class="media-element file-default" data-delta="148" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/rzotp_0-220x103.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>Restricted Zone (RZ):
<ul><li>For programmatic and export control reasons, selected machines are in the RZ.</li>
<li>RZ machines have names that begin with "rz", such as rzansel, rztopaz, rzslic, etc.</li>
<li>Foreign National access is very limited.</li>
<li>Authentication requires your LC username and a separate RZ PIN and RZ OTP token.</li>
<li>Access to the RZ from outside LLNL also requires using LLNL's VPN service (discussed later).</li>
<li>LANL/Sandia users: see the LANL/Sandia Access Methods section for differences.</li>
</ul></li>
</ul><h4>SCF Authentication</h4>
<ul><li>LC's classified systems use the same RSA OTP token as the CZ.</li>
<li>Authentication requires your LC username, SCF PIN and RSA OTP token.</li>
<li>LANL/Sandia users: see the LANL/Sandia Access Methods section for differences.</li>
</ul><h4>Problems?</h4>
<ul><li>Under certain circumstances, an OTP server and your token may get out of sync. In such cases it is necessary to enter two consecutive token codes so the server can resynchronize itself.</li>
<li>You may also need/want to change your PIN.</li>
<li>Both of these actions can be performed via the OTP web pages listed below:
<ul><li>CZ OTP home page:Â <a href="https://otp.llnl.gov" target="_blank">otp.llnl.gov</a></li>
<li>RZ OTP home page:Â <a href="https://rzotp.llnl.gov" target="_blank">rzotp.llnl.gov</a></li>
<li>SCF OTP home page: <a href="https://otp.llnl.gov" target="_blank">otp.llnl.gov</a></li>
</ul></li>
<li>Contact theÂ LC HotlineÂ if problems persist, or for other token related issues/questions:Â (925) 422-4533 <a href="mailto:lc-support@llnl.gov">lc-support@llnl.gov</a></li>
</ul><h3><a name="methods" id="methods"></a>SSH and Access Methods</h3>
<h4>SSH Required</h4>
<ul><li>Secure Shell (SSH) is required for access to all LC systems, whether you are internal to LC or external, whether you are on the OCF or the SCF.</li>
<li>The main advantages of SSH are:
<ul><li>No clear text password goes over network</li>
<li>The data stream is encrypted</li>
<li>Use of RSA/DSA authentication between LC clusters</li>
</ul></li>
<li>Mac and Linux users:
<ul><li>SSH is included on Mac and Linux platforms</li>
<li>Can simply be used from a terminal window command line. Examples:</li>
</ul></li>
</ul><pre>ssh joeuser@quartz.llnl.gov
ssh -l joeuser sierra.llnl.gov</pre><ul><li>Windows PC users:</li>
</ul><pre>ssh -m hmac-sha2-256 joeuser@quartz.llnl.gov
ssh -m hmac-sha2-512 -l joeuser sierra.llnl.gov
</pre><p>To avoid the need to enter a MAC type each time, simply create a <span class="fixed">C:\Users\joeuser\.ssh\config file</span> and add the following line to it:</p>
<pre>MACs hmac-sha2-256,hmac-sha2-512</pre><ul><li>Typically need to install an SSH app such as X-Win32 (provided by LLNL via LANDESK Portal Manager) or PuTTY. Searching the web will reveal other options.</li>
<li>X-Win32 instructions are available at:Â <a href="/manuals/access-lc-systems/x-win32-configuration">hpc.llnl.gov/manuals/access-lc-systems/x-win32-configuration</a>.</li>
<li>Windows 10 provides an OpenSSH SSH client, which can be used from a Command Prompt window or PowerShell window. Note that you will probably need to specify the MAC (authentication) type. Examples:</li>
</ul><h4>Collaboration Zone (CZ) Access Methods</h4>
<ul><li>CZ machines can be accessed directly from anywhere on the Internet.</li>
<li>Simply use SSH (or for Windows, use your favorite SSH app) and connect to a cluster where you have an account.</li>
<li>Authenticate with your LC username and CZ PIN + RSA OTP token.</li>
</ul><h4>Restricted Zone (RZ) Access Methods</h4>
<ul><li>Requires the use of an LC RZ OTP token, not be confused with the RSA OTP token.</li>
<li>From inside LLNL:
<ul><li>You must be inside the RZ network or inside the LLNL institutional network. Access from the CZ is not permitted.</li>
<li>Use SSH (or for Windows, use your favorite SSH app) and connect to a CZ cluster where you have an account.</li>
<li>Authenticate with your LC username and RZ PIN + RZ OTP token</li>
</ul></li>
<li>From outside LLNL:
<ul><li>Must first have a <a href="#remote-access">VPN Remote Access Service</a> account and software setup (discussed later)</li>
<li>Then, start up and authenticate to VPN using your LLNL OUN (Official User Name) and your CZ PIN + RSA OTP token</li>
<li>Use SSH (or for Windows, use your favorite SSH app) to connect to an RZ cluster where you have an account.</li>
<li>Authenticate with your LC username and RZ PIN + RZ OTP token</li>
</ul></li>
</ul><h4>SCF Access Methods</h4>
<ul><li>From inside the LLNL classified (iSRD) network:
<ul><li>Simply use SSH (or for Windows, use your favorite SSH app) and connect to a cluster where you have an account.</li>
<li>Authenticate with your LC username and SCF PIN + RSA OTP token</li>
</ul></li>
<li>From outside the LLNL classified (iSRD) network:
<ul><li>Must be part of the DOE <a href="#securenet">SecureNet</a> network.</li>
<li>Then, simply use SSH (or for Windows, use your favorite SSH app) and connect to a cluster where you have an account.</li>
<li>Authenticate with your LC username and SCF PIN + RSA OTP token</li>
</ul></li>
</ul><h4>Storage and FIS Access Methods</h4>
<ul><li>CZ-only users may access Storage and FIS from CZ machines and desktops</li>
<li>RZ-only users: may access Storage and FIS from RZ machines and desktops</li>
<li>CZ+RZ users may access Storage and FIS from RZ machines and desktops; not from CZ machines.</li>
<li>SCF users may access Storage and FIS may from SCF machines and iSRD desktops.</li>
<li>For details see the following:
<ul><li><a href="/hardware/zone-access">hpc.llnl.gov/hardware/zone-access</a></li>
<li><a href="#archival-hpss">Archival Storage</a></li>
<li><a href="#fis">File Interchange Service (FIS)</a></li>
</ul></li>
</ul><h4>Web Page Access</h4>
<ul><li>The majority of LC's web pages atÂ <a href="https://hpc.llnl.gov">hpc.llnl.gov</a>Â are publicly available over the Internet without the need for authentication.</li>
<li>Web pages located on LC's Confluence Wikis (CZ/RZ/SCF) require authentication with an LC username and the relevant domain PIN + OTP token.</li>
<li>Likewise, web pages located on the MyLC portals require the appropriate LC authentication method.</li>
</ul><table><tr><td><div class="media media-element-container media-default"><div id="file-867" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/internalwebaccess1gif-0">internalWebAccess1.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/internalWebAccess1_0.gif"><img alt="CZ Confluence" height="180" width="265" style="width: 265px; height: 180px;" class="media-element file-default" data-delta="149" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/internalWebAccess1_0-265x180.gif" /></a>  </div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-868" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/internalwebaccess2gif-0">internalWebAccess2.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/internalWebAccess2_0.gif"><img alt="LC Collaboration Zone" height="180" width="290" style="width: 290px; height: 180px;" class="media-element file-default" data-delta="150" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/internalWebAccess2_0-290x180.gif" /></a>  </div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-869" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/internalwebaccess3gif-0">internalWebAccess3.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/internalWebAccess3_0.gif"><img alt="LC Restricted Zone" height="180" width="284" style="width: 284px; height: 180px;" class="media-element file-default" data-delta="151" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/internalWebAccess3_0-284x180.gif" /></a>  </div>

  
</div>
</div></td>
</tr></table><ul><li>Notes:
<ul><li>LANL/Sandia users: see LANL/Sandia Access Methods below.</li>
<li>LLNL's institutional web pages (e.g.,Â LITE, LAPIS, LTRAIN) are unrelated to LC web pages and usually require LLNL OUN/PAC or Active Directory authentication (not covered here).</li>
</ul></li>
</ul><h4>LANL/Sandia Access Methods</h4>
<ul><li>Tri-lab access methods differ from those used by other users, as described below.</li>
<li>CZ Access:
<ul><li>Begin on a LANL/Sandia iHPC login node. For example, at LANL start fromÂ <strong>ihpc-gate1.lanl.gov</strong>; at Sandia start fromÂ <strong>ihpc.sandia.gov</strong>.</li>
<li>Then connect to an LC cluster in the CZ using your LC username</li>
<li>Authentication: No password required</li>
</ul></li>
</ul><pre>ssh -l lc-username loginmachine.llnl.gov</pre><ul><li>RZ Access:
<ul><li>Begin on a LANL/Sandia iHPC login node. For example, at LANL start fromÂ <strong>ihpc-gate1.lanl.gov</strong>; at Sandia start fromÂ <strong>ihpc.sandia.gov</strong>.</li>
<li>Then connect to an LC cluster in the RZ using your LC username</li>
<li>Authentication: RZ PIN + RZ OTP token</li>
</ul></li>
</ul><pre>ssh -l lc-username loginmachine.llnl.gov</pre><ul><li>SCF Access:
<ul><li>Login to a local, classified HPC system</li>
<li>Then connect to an LC SCF cluster using your LC username</li>
<li>Authentication: No password required</li>
</ul></li>
</ul><pre>ssh -l lc-username loginmachine.llnl.gov</pre><ul><li>Web Page Access:
<ul><li>For LC web pages requiring authentication, LANL and Sandia users can use their local credentials to authenticate.</li>
<li>LANL:
<ul><li>Username:Â <span><a href="mailto:lanl-username@lanl.gov">lanl-username@lanl.gov</a></span></li>
<li>Password: LANL Kerberos password</li>
</ul></li>
<li>Sandia:
<ul><li>Username:Â <span><a href="mailto:sandia-username@dce.sandia.gov">sandia-username@dce.sandia.gov</a></span></li>
<li>Password: Sandia Kerberos password</li>
</ul></li>
</ul></li>
<li>Logging into LANL/Sandia machines from LLNL:
<ul><li>LANL:Â <a href="/connecting-lanl-hpc-platforms">hpc.llnl.gov/connecting-lanl-hpc-platforms</a></li>
<li>Sandia:Â <a href="/sandia-access-instructions">hpc.llnl.gov/sandia-access-instructions</a></li>
</ul></li>
</ul><h3><a name="ssh" id="ssh"></a>A Few More Words About SSH</h3>
<h4>OpenSSH</h4>
<ul><li>All OCF and SCF production machines use OpenSSH.</li>
<li>OpenSSH supports both RSA and DSA authentication.</li>
<li>OpenSSH home page: <a href="http://www.openssh.com" target="_blank">www.openssh.com</a></li>
</ul><h4>RSA/DSA Authentication (SSH Keys)</h4>
<ul><li>By default, SSH will authenticate in secure password mode. That is, when host1 does an ssh to host2, and is prompted for a userid and password, the information will be sent in encrypted form to host2. That way, passwords cannot be "sniffed" or sent "clear text" over the network.</li>
<li>One of the features of SSH is that it allows you bypass this usual login method (userid/password) by setting up RSA/DSA authentication keys. Both are supported by OpenSSH.</li>
<li>The RSA/DSA key authentication methods allow you to optionally:
<ul><li>Improve security even more by requiring a login passphrase, which can be much longer than the typical UNIX password</li>
<li>Relax the need to enter a userid/password at all. There are known security risks with this convenience.</li>
</ul></li>
<li>In a nutshell, creating RSA/DSA keys with OpenSSH is a one-time deal that can be done as follows:
<ul><li>Execute <span class="fixed">ssh-keygen -t type</span> where <em>type </em>is either "rsa" or "dsa". Take your pick.</li>
<li>When prompted, enter a passphrase if you want improved security. If you want the convenience of being able to ssh into other LC OpenSSH machines without entering a userid/password, don't enter anything.</li>
<li>After the command completes <span class="fixed">cd</span> to your <span class="fixed">.ssh</span> file and copy the file which ends in <span class="fixed">.pub</span> to a file named <span class="fixed">authorized_keys</span>. This is your public key. For example:</li>
</ul></li>
</ul><pre>cp id_dsa.pub authorized_keys</pre><ul><li>Because all OCF/SCF machines share the same home directory, you don't need to copy your public key file to each host. One copy does the trick.</li>
<li>Make sure that your .ssh files are readable only by you!!!</li>
<li>Use of ssh keys is permitted only between LC machines - not from outside the LC network or from desktop office machines.</li>
</ul><h4>SSH Timeouts</h4>
<ul><li>If you find that your sessions are being disconnected too quickly due to lack of keyboard interaction try either of the following:</li>
<li>Use the two options below with your ssh command:</li>
</ul><pre>-o ServerAlive Interval=60 -o ServerAliveCountMax=30</pre><ul><li>Create a .ssh/config file and include the two lines below in it:</li>
</ul><pre>ServerAliveInterval=60
ServerAliveCountMax=30</pre><h4>SSH and X11</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-871" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/x11gif">X11.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/X11.gif"><img alt="X11" height="102" width="100" style="width: 100px; height: 102px;" class="media-element file-default" data-delta="49" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/X11-100x102.gif" /></a>  </div>

  
</div>
</div><br /><div class="media media-element-container media-default"><div id="file-872" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/xquartzjpeg">XQuartz.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/XQuartz.jpeg"><img alt="XQuartz" height="55" width="150" style="width: 150px; height: 55px;" class="media-element file-default" data-delta="50" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/XQuartz-150x55.jpeg" /></a>  </div>

  
</div>
</div><br /><div class="media media-element-container media-default"><div id="file-873" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/xminggif">Xming.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/Xming.gif"><img alt="X Ming" height="56" width="150" style="width: 150px; height: 56px;" class="media-element file-default" data-delta="51" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/Xming-150x56.gif" /></a>  </div>

  
</div>
</div><br /><div class="media media-element-container media-default"><div id="file-874" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/x-win32jpeg">X-Win32.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/X-Win32.jpeg"><img alt="X-Win32" height="124" width="150" style="width: 150px; height: 124px;" class="media-element file-default" data-delta="52" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/X-Win32-150x124.jpeg" /></a>  </div>

  
</div>
</div></div>
<ul><li>If you are logged into an LC cluster from your desktop, and are running applications that generate graphical displays, you will need to have X11 setup on your desktop.</li>
<li><strong>Linux:</strong> automatic - nothing special needs to be done in most cases</li>
<li><strong>Macs:</strong> you'll need X server software installed. XQuartz is commonly used (<a href="http://www.xquartz.org/" target="_blank">www.xquartz.org/</a>).</li>
<li><strong>Windows:</strong> you'll need X server software installed. LLNL provides X-Win32, which can be downloaded/installed from your desktop's LANDesk Management software. Xming is a popular, free X server available for non-LLNL systems.</li>
<li>Helpful Hints:
<ul><li>X-Win32 setup instructions for LLNL: <a href="/manuals/access-lc-systems/x-win32-configuration">hpc.llnl.gov/manuals/access-lc-systems/x-win32-configuration</a></li>
<li>It's usually not necessary to define your DISPLAY variable in an SSH session between LC hosts. It should be picked up automatically.</li>
<li>Make sure your X server is setup to allow tunneling/forwarding of X11 connections BEFORE you connect to the LC host.</li>
<li>Often, you need to supply the <span class="fixed">-X</span> or <span class="fixed">-Y</span> flag to your <span class="fixed">ssh</span> command to enable X11 forwarding.</li>
<li>May also try setting the two parameters below in your .ssh/config file:</li>
</ul></li>
</ul><pre>ForwardX11=yes
ForwardX11Trusted=yes</pre><ul><li>Use the verbose option to troubleshoot problems:</li>
</ul><pre>ssh -v [other options] [host]</pre><h4>Need SSH?</h4>
<ul><li>Linux, Macs: included as part of the operating system</li>
<li>Windows: LLNL provides X-Win32 for lab machines. Can be downloaded from the LLNL Software Portal via your LANDesk Management software:</li>
</ul><pre>All Programs --&gt; LANDesk Management --&gt; Desktop Manager</pre><ul><li>Free versions, such as PuTTY, are available for most platforms - search the web</li>
</ul><h4>More Information</h4>
<ul><li>SSH Guide for Livermore Computing (internal wiki): <a href="https://lc.llnl.gov/confluence/x/QgAN" target="_blank">lc.llnl.gov/confluence/display/czconfdocs/SSH+Guide+for+Livermore+Computing</a></li>
<li>ssh man page</li>
</ul><h3><a name="login" id="login"></a>Where to Login</h3>
<h4>Login Nodes</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1834" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/clusterlogin-gif-0">clusterLogin.gif</a></h2>
    
  
  <div class="content">
    <img height="259" width="294" class="media-element file-default" data-delta="121" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/clusterLogin_2.gif" alt="" /></div>

  
</div>
</div></div>
<ul><li class="clear-floats">LC clusters have specific nodes dedicated to user login sessions.</li>
<li>Login nodes are shared by multiple users.</li>
<li>LC provides a "generic" login alias (cluster login) for each cluster. The cluster login automatically rotates between available login nodes for load balancing purposes.</li>
<li>For example: <strong>sierra.llnl.gov</strong> is the cluster login alias - which could be any of the physical login nodes.</li>
<li>Users don't need to know (in most cases) the actual login node they are rotated onto - unless there are problems. Using the <span class="fixed">hostname </span>command will indicate the actual login node name for support purposes.</li>
<li>If the login node you are on is having problems, you can <span class="fixed">ssh</span> directly to another one. To find the list of available login nodes, use the command: <span class="fixed">nodeattr -c login</span></li>
</ul><h4>Logging into Compute Nodes</h4>
<ul><li>LC permits users to login to compute nodes on Linux clusters <strong><em>while they have a job running there</em></strong>.</li>
<li>Accessing BG/Q compute nodes is not permitted.</li>
<li>Very useful for debugging running jobs</li>
<li>Several commonly used commands can be used to determine which nodes your job is using, such as: <span class="fixed">squeue, checkjob, sview</span></li>
<li>Nodes are named as: <span class="fixed">[system][#]</span>. For example:</li>
</ul><pre>borax8
zin223
sierra309
jade122
quartz1022</pre><ul><li>Note: You can use either <span class="fixed">rsh</span> or <span class="fixed">ssh</span> to access compute nodes.</li>
<li>You can also use LC's <span class="fixed">mxterm/sxterm</span> utilities to acquire compute nodes for "interactive" work.</li>
<li>How to use <span class="fixed">mxterm/sxterm</span>:</li>
</ul><ol><li>Starting from your desktop machine, make sure you have your X11 environment setup correctly</li>
<li>ssh to an LC cluster login node</li>
<li>Issue the command as follows:</li>
</ol><pre>mxterm #nodes #tasks #minutes
sxterm #nodes #tasks #minutes</pre><p>Where:<br />#nodes = number of nodes your job requires<br />#tasks = number of tasks your job requires<br />#minutes = how much time your job needs</p>
<p>4. This will submit a batch job for you that will open an xterm on your desktop when it starts to run.<br />5. After the xterm appears, you will be on a compute node and can do your work interactively.<br />6. This utility does not have a man page, however you can view the usage information by simple typing the name of the command.</p>
<h3><a name="remote-access" id="remote-access"></a>VPN Remote Access Service</h3>
<ul><li>Use of a Remote Access Service (usually VPN) is required if you are outside of the LLNL internal network, and wish to access:
<ul><li>Institutional network services (LITE, LTRAIN, email, etc.)</li>
<li>Livermore Computing Restricted Zone (RZ) compute resources</li>
</ul></li>
<li>Provided by the Cyber Security Program</li>
<li>Does not apply to the SCF</li>
<li>Not required for access to LC OCF Collaboration Zone (CZ) machines</li>
<li>To request LLNL VPN access, download software and see setup instructions, go to:Â <a href="https://access.llnl.gov/vpn/" target="_blank">access.llnl.gov/vpn/</a>.</li>
<li>LLNL also offers a browser-based SSL VPN Web Portal:
<ul><li>The web portal should be used for Internet kiosks, such as at an airport or a conference, to access LLNL systems from off-site.</li>
<li>This service can be used for submitting your timecard or sending unencrypted email.</li>
<li>For details, see the link provided above.</li>
</ul></li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-2066" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/vpnportal-png-2">VPNportal.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/VPNportal_2.png"><img alt="screen shot of portal UI" height="362" width="350" style="height: 362px; width: 350px;" class="media-element file-default" data-delta="152" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/VPNportal_2-350x362.png" /></a>  </div>

  
</div>
</div>
<h3><a name="securenet" id="securenet"></a>SecureNet</h3>
<p></p><div class="media media-element-container media-default"><div id="file-2067" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/securenetmap2-png">secureNetMap2.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/secureNetMap2.png"><img alt="Map of the US" height="233" width="350" style="height: 233px; width: 350px;" class="media-element file-default" data-delta="153" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/secureNetMap2-350x233.png" /></a>  </div>

  
</div>
</div>
<ul><li>SecureNet is the network that provides access between classified systems at DOE national laboratories and facilities.
<ul><li>LLNL</li>
<li>LANL</li>
<li>Sandia (New Mexico)</li>
<li>Sandia (California)</li>
<li>Honeywell Kansas City Plant</li>
<li>Pantex Plant</li>
<li>Westinghouse Savannah River Site</li>
<li>Y-12 National Security Complex</li>
</ul></li>
<li>All LC classified systems must be accessed over SecureNet from non-LLNL systems.</li>
<li>Non-Tri-Lab users who wish to access LLNL classified resources require a SecureNet account in addition to an SCF account.</li>
<li>For a SecureNet account application form see: <a href="/accounts/forms">hpc.llnl.gov/accounts/forms</a>.</li>
</ul><h2><a name="file-systems" id="file-systems"></a>File Systems</h2>
<h3><a name="home-directories" id="home-directories"></a>Home Directories and Login Files</h3>
<h4>Home Directories</h4>
<ul><li>LC user home directories are <strong>global</strong> to their network partition: 1 home directory system for the SCF, 1 for the OCF-CZ and 1 for the OCF-RZ.</li>
<li>Naming scheme: <span class="fixed">/g/g#/user_name</span>. Examples:</li>
</ul><p><span class="fixed">/g/g15/joeuser</span><br /><span>/g/g0/joestaff</span></p>
<ul><li>Backups:
<ul><li>Online: <span class="fixed">.snapshot</span> directories - twice daily</li>
<li>Daily incremental</li>
<li>Monthly</li>
<li>Bi-annual offsite disaster recovery</li>
<li>See the <a href="#backups">Backups</a> section for details</li>
</ul></li>
<li>NFS mounted:
<ul><li>Access is slower than local or parallel file systems</li>
<li>Not recommended for parallel I/O - may cause NFS server problems</li>
</ul></li>
<li>Quota in effect - see the <a href="#quotas">Quotas</a> section for details.</li>
</ul><h4>LC's Login Files</h4>
<ul><li>Your login shell is established when your LC account is initially setup. The usual login shells are supported:<br />/bin/bash<br />/bin/csh<br />/bin/ksh<br />/bin/sh<br />/bin/tcsh<br />/bin/zsh</li>
<li>All LC users automatically receive a set of login files. These include:</li>
</ul><pre>.cshrcÂ Â Â Â Â Â Â  .kshenvÂ Â Â Â Â Â  .loginÂ Â Â Â Â Â Â  .profile
Â Â Â Â Â Â Â Â Â Â Â Â Â  .kshrcÂ Â Â Â Â Â Â  .logout
.cshrc.linuxÂ  .kshrc.linuxÂ  .login.linuxÂ  .profile.linux</pre><ul><li>The files which are "sourced" when you login depends upon your shell.</li>
<li>Note for bash and zsh users: LC does not provide .bashrc, .bash_profile, .zprofile or .zshrc files at this time.</li>
</ul><h4>Operating System Specific Dot Files</h4>
<ul><li>LC also provides the ability for users to create dot files that are specific to a particular operating system.</li>
<li>These are not automatically provided - you need to create them yourself if you desire this feature.</li>
<li>Naming of these files is based upon the SYS_TYPE environment variable. For example, the file <span class="fixed">.cshrc.chaos_5_x86_64_ib</span> will only be sourced on systems running the chaos_5_x86_64_ib operating system.</li>
<li>How to find SYS_TYPE?
<ul><li><span class="fixed">echo $SYS_TYPE</span></li>
<li>Look in the <span class="fixed">/etc/home.config</span> file</li>
</ul></li>
<li>Note: operating systems change, so it is up to the user to keep such files current.</li>
</ul><h4>A Few Hints</h4>
<ul><li>Login files contain some important settings that should not be modified. Read the comments inside the file for guidance.</li>
<li>Place your modifications carefully, especially interactive commands. Again, read the comments inside the file for guidance.</li>
<li>Some of the more insidious and "odd" behaviors users encounter occur due to modifications to dot files.</li>
</ul><h4>Need a New Copy?</h4>
<ul><li>If you accidentally delete or clobber a dot file, a fresh copy can be obtained:
<ul><li>Architecture specific: <span class="fixed">/gadmin/etc/arch/skel/</span> directory, where <em>arch</em> matches one of the supported architectures.</li>
<li>Master dot files can be copied from <span class="fixed">/gadmin/etc/skel/</span></li>
</ul></li>
</ul><h3><a name="usr-workspace" id="usr-workspace"></a>/usr/workspace File Systems</h3>
<ul><li>LC provides 2 terabytes of NFS mounted file space for each user and group.</li>
<li>Located under <span class="fixed">/usr/workspace/<em>username</em></span><em> </em>and <span class="fixed">/usr/workspace/<em>groupname</em></span></li>
<li><span class="fixed">/usr/workspace/<em>username</em></span>Â is accessible by the user only.Â <span class="fixed">/usr/workspace/<em>groupname</em></span>Â may be accessed by the group members.</li>
<li>Similar to home directory:
<ul><li>Cross mounted from appropriate clusters</li>
<li>Not purged</li>
<li>Includes <span class="fixed">.snapshot</span> directory for twice-daily online backups</li>
<li>Not intended for parallel I/O</li>
</ul></li>
<li>Different from home directory:
<ul><li>Not backed up</li>
<li>7 days of .snapshot backups</li>
</ul></li>
</ul><h3><a name="temp-file-systems" id="temp-file-systems"></a>Temporary File Systems</h3>
<ul><li><strong>/tmp<br />/usr/tmp<br />/var/tmp</strong>
<ul><li>Different names for the same <span class="fixed">/tmp</span> file system</li>
<li>Local to each individual node; very small compared to other temporary file systems</li>
<li><strong>Note:</strong> Uses the node's local memory, which may impact the amount of memory left for the job running on the node.</li>
<li>Faster than NFS</li>
<li>No quota, no backups</li>
<li>Purged between batch jobs</li>
</ul></li>
</ul><ul><li><strong>/p/lustre#</strong>
<ul><li>Lustre parallel file systems</li>
<li>Global temporary file systems - shared by all users</li>
<li>Very large, multi-petabyte in size - varies by file system</li>
<li>Available on most OCF and SCF systems</li>
<li>Quotas are in place for the /p/lustre# file systems</li>
<li>Not subject to purging</li>
<li>No backups</li>
</ul></li>
</ul><ul><li><strong>/p/gpfs#<br />/p/gscratch#</strong>
<ul><li>Large, temporary parallel file systems found on Sierra and CORAL EA systems</li>
<li>IBM Spectrum Scale product (formerly known as GPFS)</li>
<li>No quotas, subject to purging, no backups</li>
</ul></li>
</ul><h4>Useful Commands</h4>
<ul><li>The following commands are useful for determining which file systems are mounted, how full a file system is, and how much space your files are consuming.</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">Command</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">bdf</span></td>
<td>Easy-to-read listing of mounted file systems</td>
</tr><tr><td><span class="fixed">df</span></td>
<td>Same as <span class="fixed">bdf </span>but not as easy to read</td>
</tr><tr><td><span class="fixed">df -h</span></td>
<td>Easier to read version of <span class="fixed">df</span></td>
</tr><tr><td><span class="fixed">df <em>filesystem</em></span></td>
<td>Displays info for a specified file system. Useful if the file system is not a mount point and doesn't show up on usual <span class="fixed">df</span> list</td>
</tr><tr><td><span class="fixed">du</span></td>
<td>Listing of space used by all files current directory</td>
</tr><tr><td><span class="fixed">du -k</span></td>
<td>Same as <span class="fixed">du</span> with size in Kbyte blocks. Kbytes is the default on Linux systems.</td>
</tr><tr><td><span class="fixed">du -s</span></td>
<td>Summary of space used for all files in current directory</td>
</tr><tr><td><span class="fixed">du -ks</span></td>
<td>Combination</td>
</tr></table><h3><a name="parallel-file-systems" id="parallel-file-systems"></a>Parallel File Systems</h3>
<ul><li>In a typical cluster, most nodes are <strong><em>compute nodes</em></strong> where programs actually run. A subset of the system's nodes are dedicated to serve as<strong><em> I/O nodes</em></strong>. I/O nodes are also referred to as <em><strong>gateway nodes</strong></em>.</li>
<li>I/O nodes are the interface to disk resources. All I/O performed on compute nodes is routed to the I/O nodes over the internal switch network (such as InfiniBand).</li>
<li>The I/O nodes then send the I/O requests to storage servers over the SAN (Storage Area Network) which can be 10Gbit Ethernet or InfiniBand. The storage servers then perform the actual I/O to attached physical disk resources.</li>
</ul><table class="table table-bordered"><tr><td><div class="media media-element-container media-default"><div id="file-876" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/clustertypicalgif">clusterTypical.gif</a></h2>
    
  
  <div class="content">
    <img alt="Typical cluster" height="309" width="600" class="media-element file-default" data-delta="54" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/clusterTypical.gif" /></div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-877" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/sangif">SAN.gif</a></h2>
    
  
  <div class="content">
    <img alt="Storage Area Network" height="348" width="392" class="media-element file-default" data-delta="55" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/SAN.gif" /></div>

  
</div>
</div></td>
</tr></table><ul><li>Individual files are stored as a series of "blocks" that are striped across the disks of different storage servers. This permits concurrent access by a multi-task application when tasks read/write to different segments of a common file.</li>
</ul><table class="table table-bordered"><tr><td><div class="media media-element-container media-default"><div id="file-1835" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/filestriping-gif">fileStriping.gif</a></h2>
    
  
  <div class="content">
    <img height="361" width="577" class="media-element file-default" data-delta="122" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/fileStriping_0.gif" alt="" /></div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-1836" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/lustrefilesystem1-jpg">lustreFileSystem1.jpg</a></h2>
    
  
  <div class="content">
    <img height="423" width="646" class="media-element file-default" data-delta="123" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/lustreFileSystem1.jpg" alt="" /></div>

  
</div>
</div></td>
</tr></table><ul><li>Internally, file striping is set to a specific block size that is configurable. At LC, the most efficient use of parallel file systems is with large files. The use of many small files is not advised if performance is important.</li>
<li>Parallelism:
<ul><li>Simultaneous reads/writes to non-overlapping regions of the same file by multiple tasks</li>
<li>Concurrent reads and writes to different files by multiple tasks</li>
<li>I/O will be serial if tasks attempt to use the same stripe of a file simultaneously.</li>
</ul></li>
</ul><h4>Parallel File Systems - Lustre</h4>
<ul><li>Most of LC's Linux clusters use Lustre parallel file systems.</li>
<li>To the user, it simply appears as another mounted file system</li>
<li>Naming scheme: <strong>/p/lustre#</strong> for Linux. For example:</li>
</ul><pre>% <span class="text-danger">bdf | grep lustre</span>
172.19.1.165@o2ib100:172.19.1.Â Â Â  4.9PÂ Â  1.1PÂ Â  3.9PÂ  22%Â  /p/lustre3
172.19.3.1@o2ib600:172.19.3.2@Â Â Â Â  15PÂ Â  3.8PÂ Â Â  12PÂ  25%Â  /p/lustre2
172.19.3.98@o2ib600:172.19.3.9Â Â Â Â  15PÂ Â  622TÂ Â Â  15PÂ Â  5%Â  /p/lustre1
</pre><ul><li>LC's Lustre parallel file systems are usually mounted by more than one Linux cluster.</li>
<li>No backups</li>
<li><span class="fixed">/p/lustre#</span> enforces quotas and is NOT subject to purging</li>
<li>For additional information also see: <a href="http://wiki.lustre.org/" target="_blank">wiki.lustre.org</a></li>
</ul><h4>Parallel File Systems - IBM Spectrum Scale</h4>
<ul><li>LC's Sierra and CORAL EA systems use IBM's Spectrum Scale parallel file systems (formerly known as GPFS).</li>
<li>From a user perspective, they look and feel like Lustre parallel file systems on other LC clusters.</li>
<li>Naming scheme: <span class="fixed">/p/gpfs#</span> for Sierra clusters, and <span class="fixed">/p/gscratch#</span> for CORAL EA clusters. For example:</li>
</ul><pre>% <span class="text-danger">bdf | grep gpfs</span>
gpfs1Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  140PÂ Â Â  13PÂ Â  127PÂ Â  9%Â  /p/gpfs1
% <span class="text-danger">bdf | grep gscratch</span>
gpfs0Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  1.3PÂ Â  379TÂ Â  915TÂ  30%Â  /p/gscratchr</pre><ul><li>No backups, subject to purging, no quotas</li>
</ul><h4>LC Parallel File Systems Summary</h4>
<ul><li>Shows which clusters mount which parallel file systems. File system capacities are also shown.</li>
<li>As of Jan 2020. Subject to change.</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">OCF-CZ</th>
<th scope="col">lustre1<br />15 PB</th>
<th scope="col">lustre2<br />15 PB</th>
<th scope="col">lustre3<br />4.9 PB</th>
<th scope="col">gscratchr<br />1.3 PB</th>
<th scope="col">gpfs1<br />24 PB</th>
</tr><tr><td><strong>borax</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>boraxo</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>catalyst</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>corona</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>lassen</strong></td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>X</td>
</tr><tr><td><strong>oslic</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>X - lassengpfs1</td>
</tr><tr><td><strong>pascal</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>quartz</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>ray</strong></td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>X</td>
<td>Â </td>
</tr><tr><td><strong>surface</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>syrah</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr></table><p>Â </p>
<table class="table table-striped table-bordered"><tr><th scope="col">OCF-RZ</th>
<th scope="col">lustre1<br />7.5 PB</th>
<th scope="col">czlustre1<br />15 PB</th>
<th scope="col">czlustre2<br />15 PB</th>
<th scope="col">czlustre3<br />4.9 PB</th>
<th scope="col">gscratchrzm<br />431 TB</th>
<th scope="col">gpfs1<br />1.5 PB</th>
</tr><tr><td><strong>rzalastor</strong></td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>rzansel</strong></td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>X</td>
</tr><tr><td><strong>rzgenie</strong></td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>rzhasgpu</strong></td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>rzmanta</strong></td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>X</td>
<td>Â </td>
</tr><tr><td><strong>rzslic*</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>X -rzanselgpfs1<br />lassengpfs1</td>
</tr><tr><td><strong>rztopaz</strong></td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>rztrona</strong></td>
<td>X</td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
</tr></table><p>* For convenience, rzslic mounts CZ lustre and gpfs file systems to facilitate RZ-CA data transfer.</p>
<table class="table table-striped table-bordered"><tr><th scope="col">SCF</th>
<th scope="col">lustre1<br />15 PB</th>
<th scope="col">lustre2<br />15 PB</th>
<th scope="col">gscratch9<br />431 TB</th>
<th scope="col">gpfs1<br />140 PB</th>
</tr><tr><td><strong>agate</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>cslic</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>X - sierragpfs1</td>
</tr><tr><td><strong>jade</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>jadeita</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>magma</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>max</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>mica</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr><tr><td><strong>shark</strong></td>
<td>Â </td>
<td>Â </td>
<td>X</td>
<td>Â </td>
</tr><tr><td><strong>sierra</strong></td>
<td>Â </td>
<td>Â </td>
<td>Â </td>
<td>X</td>
</tr><tr><td><strong>zin</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>Â </td>
</tr></table><h3><a name="archival-hpss" id="archival-hpss"></a>Archival HPSS Storage</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-880" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/hpssdiagramgif">hpssDiagram.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/hpssDiagram.gif"><img alt="HPSS Diagram" height="323" width="350" style="width: 350px; height: 323px;" class="media-element file-default" data-delta="58" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/hpssDiagram-350x323.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>High Performance Storage System (HPSS) archival storage is available on both the OCF and SCF.
<ul><li>Provides "virtually unlimited" tape archive storage in the petabyte range. Both capacity and performance are continually increasing to keep up with the ever increasing user demand.</li>
<li>GigE connectivity to all production clusters</li>
</ul></li>
<li>Primary components:
<ul><li>Server machines</li>
<li>RAID disk cache</li>
<li>Magnetic tape libraries</li>
<li>Jumbo frame GigE network</li>
</ul></li>
<li>FTP client on LC production machines defaults to an enhanced parallel HPSS FTP client</li>
<li>No back up, no purge</li>
</ul><p><strong>Access Methods and Usage:</strong></p>
<ul><li>The HPSS system is named <strong>storage.llnl.gov</strong> on both the OCF and SCF</li>
<li>All LC users automatically receive an HPSS storage account with their regular production machine account.</li>
<li>Data Transfer Tools: The more commonly used ones are simply listed here and described in more detail in the <a href="#file-transfer">File Transfer and Sharing</a> section that follows later.
<ul><li>Hopper: <a href="/software/hopper">hpc.llnl.gov/software/hopper</a></li>
<li>FTP/PFTP: <a href="/manuals/ezstorage/ftp/">hpc.llnl.gov/manuals/ezstorage/ftp/</a></li>
<li>NFT: <a href="/manuals/ezstorage/nft">hpc.llnl.gov/manuals/ezstorage/nft</a></li>
<li>HSI:Â <a href="http://hpss-collaboration.org/documents/HSI_8.3_Reference_Manual.pdf">http://hpss-collaboration.org/documents/HSI_8.3_Reference_Manual.pdf</a></li>
<li>HTAR: <a href="/manuals/ezstorage/htar">hpc.llnl.gov/manuals/ezstorage/htar</a></li>
<li>Tri-lab high bandwidth file transfers over SecureNet (see the File Transfer and Sharing section)</li>
<li>Note: <span class="fixed">ssh/scp/sftp</span> to storage are not supported.</li>
</ul></li>
<li><strong>Recommendation:</strong> Initiate your file transfers from one of LC's special purpose clusters, which have been optimized for high-speed data movement to storage:<br /><span class="fixed">oslic</span> on the OCF-CZ<br /><span class="fixed">rzslic</span> on the OCF-RZ<br /><span class="fixed">cslic</span> on the SCF</li>
<li>These clusters exist solely for the purpose of offloading data to storage:
<ul><li>Multiple GigE connections to the network.</li>
<li>Users can start file transfers on multiple nodes, and have them running concurrently.</li>
<li>Observed transfer rates to storage are around 120 MB/sec per HTAR session.</li>
<li>A variety of file transfer tools in addition to HTAR are supported.</li>
</ul></li>
<li>OCF RZ / CZ Restrictions:
<ul><li>Unlike most other resources, LC decided not to duplicate the HPSS system on the RZ. There is a single HPSS system on the OCF which serves both the CZ and RZ.</li>
<li>OCF-CZ only users can access storage from:
<ul><li>CZ machines</li>
<li>Desktop machines</li>
</ul></li>
<li>OCF-RZ only and OCF-RZ+CZ users can access storage from:
<ul><li>RZ machines</li>
<li>Desktop machines via <span class="fixed">ftp rzarchive</span> or <span class="fixed">ftp rzstorage</span>. Authentication is with your RZ PIN + CRYPTOCard passcode.</li>
</ul></li>
<li>For convenience, the <span class="fixed">rzslic</span> cluster mounts both CZ and RZ parallel (lscratch) file systems. <span class="fixed">oslic</span> mounts only the CZ parallel file systems.</li>
</ul></li>
<li>Also able to be accessed from Tri-lab and other remote sites. Note that for remote access to OCF storage, VPN is required.</li>
<li>Storing dual-copy files in HPSS archival storage: For mission critical files, it is possible to store two copies at once using FTP, HSI, HTAR or NFT. <strong>Technical Bulletin 435</strong> discusses how to accomplish this, located at: <a href="https://lc.llnl.gov/computing/techbulletins/bulletin435.pdf" target="_blank">lc.llnl.gov/computing/techbulletins/bulletin435.pdf</a> (requires authentication).</li>
<li>Quotas:
<ul><li>Based on a user's annual growth in HPSS file space</li>
<li>OCF yearly growth quota (FY20): 300Â TB</li>
<li>SCF yearly growth quota (FY20): 300Â TB</li>
<li>For details see Technical Bulletin 534Â (<a href="/technical-bulletin-534-fy20-hpss-yearly-growth-quotas">/technical-bulletin-534-fy20-hpss-yearly-growth-quotas</a>)</li>
</ul></li>
<li>How much storage am I using? The <span class="fixed">aquota</span> command provides this information. For example:</li>
</ul><pre>oslic5% <span class="text-danger">kinit</span>
<em>[authenticate here]</em>

oslic5% <span class="text-danger">aquota</span>
Welcome to HPSS Quota Server oslici.llnl.gov

aq&gt; <span class="text-danger">show allowance</span>

Pool Name                 Pool Manager  Allowance
------------------------  ------------  ---------------
lcreserve                 lc-hotline             0.0 B
default                   lc-hotline             1.5 TB
   Total                                         1.5 TB

From 10/01/2019 through 02/06/2020:
     5 files created.
     46.8 GB of data used. 3.12% of total.
     Avg. Per Month:  11.1 GB

Total Data:      85.1 TB
Total Files:     2127240

aq&gt;</pre><p>Usage notes:</p>
<ul><li>Currently, you must be logged into <span class="fixed">oslic / rzslic / cslic</span> to use this command.</li>
<li>Use the aquota <span class="fixed">help </span>subcommand for additional options.</li>
<li>You may need to authenticate with theÂ <span class="fixed">kinitÂ </span>command first.</li>
</ul><h4>Additional Information</h4>
<ul><li>HPSS Home Page: <a href="http://www.hpss-collaboration.org/" target="_blank">www.hpss-collaboration.org</a></li>
</ul><h3><a name="usr-gapps" id="usr-gapps"></a>/usr/gapps, /usr/gdata File Systems</h3>
<ul><li>LC provides shared, collaborative, NFS file space for user developed and supported applications and data on LC systems:</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">File System</th>
<th scope="col">CZ</th>
<th scope="col">RZ</th>
<th scope="col">SCF</th>
<th scope="col">Notes</th>
</tr><tr><td><strong>/usr/gapps</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>User applications. Distinct between CZ, RZ and SCF.</td>
</tr><tr><td><strong>/collab/usr/gapps/</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>User applications. Shared between RZ and CZ.</td>
</tr><tr><td><strong>/usr/gapps2</strong></td>
<td>Â </td>
<td>Â </td>
<td>X</td>
<td>User applications.</td>
</tr><tr><td><strong>/usr/gdata</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>User data. Distinct between CZ, RZ and SCF.</td>
</tr><tr><td><strong>/collab/usr/gdata</strong></td>
<td>X</td>
<td>X</td>
<td>Â </td>
<td>User data. Shared between RZ and CZ.</td>
</tr></table><ul><li>Unlike your home directory, these file systems can be used (with approval) to share file space within a group or even the world.</li>
<li>For convenience, OCF-RZ users can use <span class="fixed">/collab/usr/gapps</span> and <span class="fixed">/collab/usr/gdata</span> to share files with OCF-CZ users.</li>
<li>Backups:
<ul><li>Online: <span class="fixed">.snapshot</span> directories.</li>
<li>Daily incremental</li>
<li>Monthly</li>
<li>Bi-annual offsite disaster recovery</li>
<li>See the <span>Backups</span> section for details.</li>
</ul></li>
<li>Never purged</li>
<li>Multiple architectures are handled through the $SYS_TYPE variable:
<ul><li>Every LC machine sets this environment variable to a specific string that matches its architecture. For example:</li>
</ul></li>
</ul><pre>toss_3_x86_64_ib
blueos_3_ppc64le_ib</pre><ul><li>Versions of code built for specific architectures are placed in subdirectories named to match $SYS_TYPE strings</li>
<li>User scripts can select the appropriate code versions based upon the $SYS_TYPE setting. For example: <span class="fixed">cd /usr/gapps/myApp/$SYS_TYPE/bin</span></li>
</ul><ul><li>Requesting a directory within <span class="fixed">/usr/gapps</span>: submit the <a href="/sites/default/files/usr_gapps.pdf">LC USR_GAPPS</a> form to create/change/delete a directory.</li>
<li>Sharing files and directories in your <span class="fixed">/usr/gapps</span> directory with a group:
<ul><li>Create and manage UNIX groups: <a href="https://lc-idm.llnl.gov/" target="_blank">lc-idm.llnl.gov</a></li>
<li>Then use UNIX permissions to permit group sharing</li>
</ul></li>
<li>For additional information see the <a href="/hardware/file-systems/usr-gapps-file-system">/usr/gapps web page</a></li>
</ul><h3><a name="quotas" id="quotas"></a>Quotas</h3>
<h4>Home Directories</h4>
<ul><li>Checkout our <a href="/hardware/archival-storage-hardware#quotas">Quotas overview</a> for up-to-date information</li>
</ul><ul><li>To check usage and limits: <span class="fixed">quota -v</span></li>
<li>Example:</li>
</ul><pre>% <span class="text-danger">quota -v</span>

Disk quotas for joeuser:
FilesystemÂ Â Â Â  usedÂ Â  quotaÂ  limitÂ Â Â  timeleftÂ  filesÂ  quotaÂ  limitÂ Â Â  timeleft
<span class="text-danger">/g/g0Â Â Â Â Â Â Â Â Â  1.1GÂ Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  3.9KÂ Â  n/aÂ Â Â  n/a</span>Â Â Â Â Â 
/g/g10Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g11Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g12Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g13Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g14Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g15Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g16Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g17Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g18Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g19Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g20Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g21Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g22Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g23Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g24Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g90Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g91Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g92Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/g/g99Â Â Â Â Â Â Â Â  -0-Â Â Â  24.0GÂ  24.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/usr/gappsÂ Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/collab/usr/gapps
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/usr/giveÂ Â Â Â Â  -0-Â Â Â  25.0GÂ  25.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/usr/globalÂ Â Â  -0-Â Â Â  32.0GÂ  32.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/collab/usr/global
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  32.0GÂ  32.0GÂ Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/usr/workspace/wsa
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â 
/usr/workspace/wsb
Â Â Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â  -0-Â Â Â  n/aÂ Â Â  n/aÂ Â Â Â Â Â Â Â Â 
/p/lustre1Â Â Â Â  24.5KÂ  18.0TÂ  20.0TÂ Â Â Â Â Â Â Â Â Â Â Â Â  0.0KÂ Â  900.0K 1.0MÂ Â Â Â 
/p/lustre2Â Â Â Â  24.5KÂ  18.0TÂ  20.0TÂ Â Â Â Â Â Â Â Â Â Â Â Â  0.0KÂ Â  900.0K 1.0M</pre><ul><li>Requests for additional disk space should be directed to the LC Hotline through your computer coordinator or PI.</li>
<li>To view usage by user per file system, see the log files located in <span class="fixed">/usr/global/docs/filerUsageInfo</span></li>
<li><strong>Exceeding quota:</strong>
<ul><li>Warning appears in login messages if usage over 90% quota</li>
<li>Heed quota warnings - risk of data loss if quota is exceeded!</li>
</ul></li>
</ul><h4>Other File Systems</h4>
<ul><li>HPSS archival storage: Checkout our <a href="/hardware/archival-storage-hardware#quotas">Quotas overview</a> for up-to-date information</li>
</ul><h3><a name="purge-policies" id="purge-policies"></a>Purge Policies</h3>
<ul><li>When file systems become full, performance can be significantly degraded. Because of this, LC maintains policies for purging temporary file systems.</li>
<li>The following <strong><em>temporary</em></strong> file systems are subject to purging:</li>
</ul><pre>/tmp
/usr/tmp
/var/tmp
/nfs/tmp#
/p/lscratch#</pre><ul><li>The <strong>/p/lustre#</strong> temporary file systems are NOT subject to purging since they enforce quotas. Likewise for theÂ <span class="fixed">/p/gpfs#</span>Â file systems once quotas are implemented for them.</li>
<li>When are files purged?
<ul><li><span class="fixed">/tmp, /var/tmp, /usr/tmp:</span> node-local temporary file space is purged daily and/or in between batch jobs.</li>
<li><span class="fixed">/p/lscratch#:</span> as needed</li>
</ul></li>
<li>Files in temporary file systems are <strong>not</strong> backed up</li>
<li><strong>Don't forget:</strong> <span class="fixed">tmp</span> or <span class="fixed">scratch</span> in the name means <strong>temporary!</strong></li>
</ul><h3><a name="backups" id="backups"></a>Backups</h3>
<h4>Online .snapshot Directories</h4>
<ul><li>User home directories, <span class="fixed">/usr/workspace</span>,Â <span class="fixed">/usr/gapps</span> and <span class="fixed">/usr/gdata</span> have a special, online directory for regular, automatic backups.</li>
<li>Hidden <span class="fixed">.snapshot</span> subdirectory
<ul><li>It is not listed by the <span class="fixed">ls</span> command but you can <span class="fixed">cd .snapshot</span></li>
<li>Contains multiple subdirectories, each containing a full backup and a timestamp when the backup was created.</li>
<li><span class="fixed">.snapshot</span> is read-only directory</li>
</ul></li>
<li>If you delete or mangle a file it may save you:
<ul><li>If the file existed before the last <span class="fixed">.snapshot</span> backup was done</li>
<li>Just use the <span class="fixed">cp</span> command to copy replacement</li>
</ul></li>
<li>Feature with Network Appliance NFS servers. See their documentation at <a href="https://netapp.com" target="_blank">netapp.com</a>.</li>
<li>Example:</li>
</ul><pre>% <span class="text-danger">ls -l .snapshot</span>
total 80
drwx------ 98 joeuser joeuser 20480 May 8 13:27 2_per_day.2019-05-08_1900
drwx------ 98 joeuser joeuser 20480 May 8 13:27 2_per_day.2019-05-09_1200
drwx------ 98 joeuser joeuser 20480 May 8 13:27 2_per_day.2019-05-09_1900
drwx------ 98 joeuser joeuser 20480 May 8 13:27 2_per_day.2019-05-10_1200
drwx------ 97 joeuser joeuser 20480 May 3 15:39 weekly.2019-05-05_0015
% <span class="text-danger">ls -l .snapshot/2_per_day.2018-05-28_1900</span>
total 24712
-rw-------  1 joeuser joeuser   31575 Aug 30 12:19 Batch_Limits.doc
-rw-------  1 joeuser joeuser 2120192 Sep 01 12:04 FY01Blueprint.doc
drwx------  2 joeuser joeuser    4096 May 07 15:44 Mail
drwx------  2 joeuser joeuser    4096 Nov 07 2000  Misc
drwx------ 16 joeuser joeuser    4096 Oct 24 1998  NPB2.3
-rw-------  1 joeuser joeuser 3039744 Aug 30 10:22 WhitePIX.ppt
drwx------  2 joeuser joeuser    4096 Mar 29 13:09 bin
-rw-------  1 joeuser joeuser      39 May 09 09:20 blank.html
-r--------  1 joeuser joeuser 2433035 Aug 24 14:01 cforaix.pdf
....</pre><h4>Livermore Computing System Backups</h4>
<ul><li>LC performs regular backups of the following file systems:
<ul><li><span class="fixed">/g/g##</span>: User home directories</li>
<li><span class="fixed">/usr/gapps, /usr/gdata</span>: User application and data directories</li>
<li><span class="fixed">/usr/local, /usr/global</span>: LC developed or maintained application directories</li>
<li>Atlassian Tools: Jira, Confluence, Bitbucket, etc.</li>
</ul></li>
<li>Daily backups of new or changed files</li>
<li>Full monthly backup of all files. Retained onsite for 6 months.</li>
<li>Disaster recovery backups:
<ul><li>Performed every 6 months</li>
<li>Data (both OCF and SCF) is stored offsite at the Nevada Test Facility</li>
<li>Retained for 2 years</li>
</ul></li>
<li>For detailed information on LC backups, see the internal wiki document located at: <a href="https://lc.llnl.gov/confluence/display/LCBackups/LC+Backups+Home" target="_blank">lc.llnl.gov/confluence/display/LCBackups/LC+Backups+Home</a> (requires authentication)</li>
</ul><p><span class="note-red">Note:</span>Temporary file systems are not backed up:</p>
<ul><li><span class="fixed">/tmp, /var/tmp, /usr/tmp</span></li>
<li><span class="fixed">/p/lustre#</span></li>
<li><span class="fixed">/p/gscratch#</span></li>
<li><span class="fixed">/p/gpfs#</span></li>
</ul><h4>Archival HPSS Storage</h4>
<ul><li>Users are responsible for backing up all other data they wish to preserve, particularly any files residing in temporary file systems.</li>
<li>The preferred location for these backups is the archival HPSS storage system available on both the OCF and SCF.</li>
<li>See t<span class="text-info">heÂ <a href="#archival-hpss">Archival HPSS Storage</a> s</span>ection for details.</li>
</ul><h3><a name="file-transfer" id="file-transfer"></a>File Transfer and Sharing</h3>
<h4>File Transfer Tools</h4>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-894" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/hopper400pixjpeg">hopper400pix.jpeg</a></h2>
    
  
  <div class="content">
    <img alt="Hopper" height="256" width="400" class="media-element file-default" data-delta="72" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/hopper400pix.jpeg" /></div>

  
</div>
</div><br /><div class="media media-element-container media-default"><div id="file-895" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mylc400pixjpeg">mylc400pix.jpeg</a></h2>
    
  
  <div class="content">
    <img alt="MyLC" height="324" width="400" class="media-element file-default" data-delta="73" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/mylc400pix.jpeg" /></div>

  
</div>
</div><br /><div class="media media-element-container media-default"><div id="file-2069" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/trilabtransfers-gif">TrilabTransfers.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/TrilabTransfers_0.gif"><img alt="diagrams of LANL, LLNL, and SNL data transfers" height="290" width="400" style="height: 290px; width: 400px;" class="media-element file-default" data-delta="155" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/TrilabTransfers_0-400x290.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>There are a number of ways to transfer files - depending upon what you want to do.</li>
<li><strong>hopper</strong> - A powerful, interactive, cross-platform tool that allows users to transfer and manipulate files and directories by means of a graphical user interface. Users can connect to and manage resources using most of the major file transfer protocols, including FTP, SFTP, SSH, NFT, and HTAR. See the hopper web pages ( <a href="/software/hopper">hpc.llnl.gov/software/hopper</a>), hopper man page or use the <span class="fixed">hopper -readme</span> command for more information.</li>
<li><strong>ftp</strong> - Is available for file transfer between LC machines. The ftp client at LC is an optimized parallel ftp implementation. It can be used to transfer files with machines outside LLNL if the command originates from an LLNL machine and the foreign host will permit it. FTP to LC machines from outside LLNL is not permitted unless the user is connected via an appropriate Remote Access service such as OTS or VPN. Documentation is available via the ftp man page or the FTP Usage Guide (<a href="/manuals/ezstorage/ftp">hpc.llnl.gov/manuals/ezstorage/ftp</a>)</li>
<li><strong>scp</strong> - (secure copy) is available on all LC machines. Example:<br /><span class="fixed">scp thisfile user@host2:thatfile</span></li>
<li><strong>sftp</strong> - Performs ftp-like operations over encrypted ssh.</li>
<li><strong>MyLC</strong> - Livermore Computing's user portal provides a mechanism for transferring files to/from your desktop machine and your home directory on an LC machine. See the "utilities" tab. Available at <a href="https://mylc.llnl.gov/" target="_blank">mylc.llnl.gov</a></li>
<li><strong>nft</strong> - (Network File Transfer) is LC's utility for persistent file transfer with job tracking. This is a command line utility that assumes transfers with storage and has a specific syntax. Documentation is available via its man page or the NFT Reference Manual (<a href="/manuals/ezstorage/nft">hpc.llnl.gov/manuals/ezstorage/nft</a>).</li>
<li><strong>htar</strong> - Is highly optimized for creation of archive files directly into HPSS, without having to go through the intermediate step of first creating the archive file on local disk storage, and then copying the archive file to HPSS via some other process such as ftp. The program uses multiple threads and a sophisticated buffering scheme in order to package member files into in-memory buffers, while making use of the high-speed network striping capabilities of HPSS. Syntax resembles that of the UNIX tar command. Documentation is available via its man page or the HTAR Reference Manual (<a href="/manuals/ezstorage/htar">hpc.llnl.gov/manuals/ezstorage/htar</a>).</li>
<li><strong>hsi</strong> - Hierarchical Storage Interface. HSI is a utility that communicates with HPSS via a user- friendly interface that makes it easy to transfer files and manipulate files and directories using familiar UNIX-style commands. HSI supports recursion for most commands as well as CSH-style support for wildcard patterns and interactive command line and history mechanisms. Documentation is available via its man page.</li>
<li><strong><a id="tri-lab-file-transfer" name="tri-lab-file-transfer"></a>Tri-lab high bandwidth file transfers over SecureNet:</strong>
<ul><li>All three Labs support wrapper scripts for enhanced data transfer between sites - classified side only.</li>
<li>Three different protocols can be used: <span class="fixed">hsi</span>, <span class="fixed">htar </span>and <span class="fixed">pftp</span>.</li>
<li>Transfers can be from host to storage or host to host</li>
<li>Commands are given names that are self-explanatory - see the accompanying image at right.</li>
<li>At LLNL, these scripts should already be in your path</li>
<li>For additional information please see <a href="https://aces.sandia.gov/tri_lab_home.html#file_xfer" target="_blank">aces.sandia.gov/tri_lab_home.html#file_xfer</a> (requires authentication)</li>
</ul></li>
</ul><h4>File Sharing Rules</h4>
<ul><li>User home directories are required to be accessible to the user only. No group or world sharing is permitted.</li>
<li>Likewise, <span class="fixed">/usr/workshare</span> directories are accessible by the user only.</li>
<li>Group sharing is permitted in <span class="fixed">/usr/workspace/groupname</span> directories.</li>
<li>Group sharing is permitted in lustre directories.</li>
<li>The collaborative <span class="fixed">/usr/gapps</span> file systems permit group sharing. World sharing is permitted with Associate Director approval.</li>
</ul><h4>Give and Take Utilities</h4>
<ul><li>LC provides the <span class="fixed">give</span> and <span class="fixed">take</span> utilities for sharing files between users.</li>
<li>Syntax:</li>
</ul><pre>give <em>user file</em>
take <em>user file</em></pre><ul><li>Examples:
<ul><li><span>Give one file:</span><span class="fixed"> give jsmith input1Â Â Â Â Â  </span></li>
<li><span>Give multiple files: </span><span class="fixed">give jsmith input1 input2Â Â Â Â Â  </span></li>
<li>Give multiple files via wildcard: <span class="fixed">give jsmith in*Â Â Â Â Â  </span></li>
<li>"ungive" (remove) a file given to jsmith: <span class="fixed">give -u jsmith input2Â Â Â Â Â  </span></li>
<li>Take one file: <span class="fixed">take ljones dataÂ Â Â Â Â  </span></li>
<li>Take multiple files: <span class="fixed">take ljones data2 data3Â Â Â  </span></li>
<li>Takes all ljones files - do not use asterisk: <span class="fixed">take ljonesÂ Â Â Â Â  </span></li>
<li>Lists files to be taken: <span class="fixed">takeÂ Â Â Â Â  </span></li>
<li>Lists files you have given: <span class="fixed">giveÂ Â Â </span></li>
</ul></li>
</ul><ul><li>Files are spooled to the <span class="fixed">/usr/give</span> directory
<ul><li>Mounted and visible on all production clusters</li>
<li>Separate spool directories for OCF-CZ and OCF-RZ machines</li>
<li>Limited in size - if you plan on giving large files, check how full the directory is first using the <span class="fixed">df</span> or <span class="fixed">bdf</span> commands.</li>
<li>Currently, a 25GB quota per user</li>
</ul></li>
<li>Files which have been given, but not taken, will be purged from the spool directory after a week or so</li>
<li>Cannot give a directory structure; tar it up and then give</li>
<li>Files must be taken on a machine where both users (giver and taker) have accounts.</li>
<li>For options and additional information, see the give and take man pages.</li>
</ul><h4>Anonymous FTP Server</h4>
<ul><li>LC does not support an anonymous FTP server</li>
<li>LLNL does offer <span class="fixed">ftp.llnl.gov</span> for this purpose</li>
<li>For usage information, do a <strong>search</strong> on the Lab's ServiceNow Knowledgebase. This will give the most up-to-date information.
<ul><li>As of January 2020, there is an article here:Â <span><span><a href="https://llnl.servicenowservices.com/ess/?id=kb_article_view&amp;sys_kb_id=dafc4872dbc1f74c0716348c7c9619a7">https://llnl.servicenowservices.com/ess/?id=kb_article_view&amp;sys_kb_id=dafc4872dbc1f74c0716348c7c9619a7</a></span></span></li>
</ul></li>
</ul><h3><a name="fis" id="fis"></a>File Interchange Service (FIS)</h3>
<ul><li>Use of LC's File Interchange Service (FIS) is required to move files between the OCF and the SCF</li>
<li>Requires that an FIS account be setup first
<ul><li>Must already have valid OCF and SCF accounts</li>
<li><a href="/sites/default/files/fis-2019.pdf">Complete the form</a></li>
</ul></li>
<li>Two different types of FIS:
<ul><li><strong>fastfis:</strong> Uses an automated, unidirectional One Way Link (OWL) from OCF to SCF. Transfers initiate quickly.</li>
<li><strong>tapefis:</strong> Uses a manual transfer of tape by operator from OCF to SCF. Transfers initiate more slowly because of manual involvement.</li>
</ul></li>
<li>Fastfis has one channel for small files and one channel for large (over 1 GB) files. Helps reduce blocking of small file transfer by large files.</li>
<li>Recommendation: use a tar file if many small files are to be transferred.</li>
<li>File size limits and transfer speeds:
<ul><li>No file size limits per se, on either fastfis or tapefis</li>
<li>System space is currently 1 TB for fastfis and 3 TB for tapefis (8/18)</li>
<li>Transfer speed of 100 GB/hr for fastfis and 200 GB/hr for tapefis</li>
</ul></li>
<li>Purging: be sure to complete your transfer in a timely manner on the FROM side, as files are periodically purged from the TO and FROM directories.</li>
<li>Transferring files from the SCF to the OCF requires an Associate Director's approval and occurs via tapefis.</li>
<li>Documentation is available at:
<ul><li><a href="/manuals/fis/">hpc.llnl.gov/manuals/fis</a>.</li>
<li><a href="https://lc.llnl.gov/computing/techbulletins/bulletin489.pdf" target="_blank">Technical Bulletin #489</a> (requires authentication)</li>
<li><a href="https://lc.llnl.gov/computing/techbulletins/bulletin496.pdf" target="_blank">Technical Bulletin #496</a> (requires authentication)</li>
</ul></li>
</ul><h4>Usage</h4>
<table class="table table-striped table-bordered"><tr><th scope="col">Sending Files</th>
<th scope="col">Hostname</th>
<th scope="col">Alias</th>
<th scope="col">Valid Protocols</th>
<th scope="col">Transfer Method</th>
<th scope="col">Authentication Method</th>
<th scope="col">Notes</th>
</tr><tr><td colspan="1" rowspan="3"><strong>CZ only user</strong><br />From OCF-CZ or desktop machine to SCF</td>
<td>fis.llnl.gov</td>
<td>fis</td>
<td colspan="1" rowspan="3">ftp, sftp</td>
<td colspan="1" rowspan="2">OWL</td>
<td colspan="1" rowspan="3">RSA OTP</td>
<td colspan="1" rowspan="3"><span class="fixed">cd</span> to the <span class="fixed">TO</span> directory and then <span class="fixed">put</span> your files there. You will be notified by email when your files have been moved to the SCF.</td>
</tr><tr><td>fastfis.llnl.gov</td>
<td>fastfis</td>
</tr><tr><td>tapefis.llnl.gov</td>
<td>tapefis</td>
<td>tape</td>
</tr><tr><td colspan="1" rowspan="3"><strong>RZ or RZ/CZ user</strong><br />From OCF-RZ or desktop machine to SCF</td>
<td>rzfis.llnl.gov</td>
<td>rzfis</td>
<td colspan="1" rowspan="3">ftp, sftp</td>
<td colspan="1" rowspan="2">OWL</td>
<td colspan="1" rowspan="3">CRYPTOcard OTP</td>
</tr><tr><td>rzfastfis.llnl.gov</td>
<td>rzfastfis</td>
<td><span class="fixed">cd</span> to the <span class="fixed">TO</span> directory and then <span class="fixed">put</span> your files there. You will be notified by email when your files have been moved to the SCF.</td>
</tr><tr><td>rztapefis.llnl.gov</td>
<td>rztapefis</td>
<td>tape</td>
</tr><tr><td><strong>SCF user</strong><br />From SCF to OCF</td>
<td>tapefis.llnl.gov</td>
<td>tapefis</td>
<td>ftp</td>
<td>tape</td>
<td>RSA OTP</td>
<td><span class="fixed">cd</span> to the <span class="fixed">TO</span> directory and then put your files there. Requires review/approval by an Authorized Derivative Classifier (ADC) from your department/program See <a href="https://ocec-r.llnl.gov/" target="_blank">ocec-r.llnl.gov/</a> to find yours.</td>
</tr><tr><td colspan="1" rowspan="2"><strong>iSNSI CZ only user</strong><br />From OCF-CZ or desktop machine to iSNSI (pinot)</td>
<td>snsifis.llnl.gov</td>
<td>snsifis</td>
<td>ftp, sftp</td>
<td>OWL</td>
<td>LC username + RSA OTP</td>
<td colspan="1" rowspan="2"><span class="fixed">cd</span> to the <span class="fixed">TO</span> directory and then <span class="fixed">put</span> your files there.</td>
</tr><tr><td>snsitapefis.llnl.gov</td>
<td>snsitapefis</td>
<td>Â </td>
<td>tape</td>
<td>Â </td>
</tr><tr><td colspan="1" rowspan="2"><strong>iSNSI RZ or RZ/CZ user</strong><br />From OCF-RZ or desktop machine to iSNSI (pinot)</td>
<td>rzsnsifis.llnl.gov</td>
<td>rzsnsifis</td>
<td>ftp, sftp</td>
<td>OWL</td>
<td>LC username + CRYPTOcard OTP</td>
<td colspan="1" rowspan="2"><span class="fixed">cd</span> to the <span class="fixed">TO</span> directory and then <span class="fixed">put</span> your files there.</td>
</tr><tr><td>rzsnsitapefis.llnl.gov</td>
<td>rzsnsitapefis</td>
<td>Â </td>
<td>tape</td>
<td>Â </td>
</tr><tr><td><strong>iSNSI user</strong><br />From iSNSI (pinot) to OCF</td>
<td>tapefis.llnl.doe.sgov.gov</td>
<td>tapefis</td>
<td>ftp</td>
<td>tape</td>
<td>OUN + SNSI PIN with RSA OTP</td>
<td><span class="fixed">cd</span> to the <span class="fixed">TO</span> directory and then <span class="fixed">put</span> your files there. Requires review/approval by an Authorized Derivative Classifier (ADC) from your department/program See <a href="https://ocec-r.llnl.gov/" target="_blank">ocec-r.llnl.gov</a> to find yours.</td>
</tr></table><div>
<p>Â </p>
<table class="table table-striped table-bordered"><tr><th scope="row">Retrieving Files</th>
<th scope="col">Hostname</th>
<th scope="col">Alias</th>
<th scope="col">Valid Protocols</th>
<th scope="col">Transfer Method</th>
<th scope="col">Authentication Method</th>
<th scope="col">Notes</th>
</tr><tr><td colspan="1" rowspan="3"><strong>SCF user</strong><br />After transfer from either OCF-CZ or OCF-RZ machine</td>
<td>fis.llnl.gov</td>
<td>fis</td>
<td colspan="1" rowspan="2">ftp, sftp</td>
<td colspan="1" rowspan="2">OWL</td>
<td colspan="1" rowspan="3">RSA OTP</td>
</tr><tr><td>fastfis.llnl.gov</td>
<td>fastfis</td>
<td><span class="fixed">cd</span> to the <span class="fixed">FROM</span> directory and then <span class="fixed">get</span> your files from there.</td>
</tr><tr><td>tapefis.llnl.gov</td>
<td>tapefis</td>
<td>ftp</td>
<td>tape</td>
</tr><tr><td colspan="1" rowspan="2"><strong>iSNSI user</strong><br />After transfer from either OCF-CZ or OCF-RZ machine</td>
<td>fastfis.llnl.doe.sgov.gov</td>
<td>fastfis</td>
<td>ftp, sftp</td>
<td>OWL</td>
<td colspan="1" rowspan="2">OUN + SNSI PIN with RSA OTP</td>
<td colspan="1" rowspan="2"><span class="fixed">cd</span> to the <span class="fixed">FROM</span> directory and then <span class="fixed">get</span> your files from there.</td>
</tr><tr><td>fis.llnl.doe.sgov.gov<br />(tape)</td>
<td>fis</td>
<td>ftp</td>
<td>tape</td>
</tr><tr><td><strong>OCF CZ user</strong><br />After transfer from SCF</td>
<td>tapefis.llnl.gov</td>
<td>tapefis</td>
<td>ftp</td>
<td>tape</td>
<td>RSA OTP</td>
<td colspan="1" rowspan="2"><span class="fixed">cd</span> to the <span class="fixed">FROM</span> directory and then <span class="fixed">get</span> your files from there. Requires previous review/approval by an Authorized Derivative Classifier (ADC) from your department/program See <a href="https://ocec-r.llnl.gov/" target="_blank">ocec-r.llnl.gov/</a> to find yours.</td>
</tr><tr><td><strong>OCF RZ or RZ/CZ user</strong><br />After transfer from SCF</td>
<td>rztapefis.llnl.gov</td>
<td>rztapefis</td>
<td>ftp</td>
<td>tape</td>
<td>CRYPTOcard OTP</td>
</tr><tr><td><strong>OCF CZ user</strong><br />After transfer from iSNSI</td>
<td>snsitapefis.llnl.gov</td>
<td>snsitapefis</td>
<td>ftp</td>
<td>tape</td>
<td>LC username + RSA OTP</td>
<td colspan="1" rowspan="2"><span class="fixed">cd</span> to the <span class="fixed">FROM</span> directory and then <span class="fixed">get</span> your files from there. Requires previous review/approval by an Authorized Derivative Classifier (ADC) from your department/program See <a href="https://ocec-r.llnl.gov/" target="_blank">ocec-r.llnl.gov/</a> to find yours.</td>
</tr><tr><td><strong>OCF RZ or RZ/CZ user</strong><br />After transfer from iSNSI</td>
<td>snsirztapefis.llnl.gov</td>
<td>snsirztapefis</td>
<td>ftp</td>
<td>tape</td>
<td>LC username + CRYPTOcard OTP</td>
</tr></table><h2><a name="system-status" id="system-status"></a>System Status and Configuration Information</h2>
<ul><li>Before you attempt to run your parallel application, it is important to know a few details about the way the system is configured. This is especially true at LC where every system is configured differently and where things change frequently.</li>
<li>It is also useful to know the status of the machines you intend on using. Are they available or down for maintenance?</li>
<li>System configuration and status information for all LC systems is readily available from the <a href="https://hpc.llnl.gov">LC Homepage</a><strong> </strong>and the <a href="https://mylc.llnl.gov" target="_blank">MyLC Portal</a>.</li>
</ul><table><tr><td><div class="media media-element-container media-default"><div id="file-2070" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/lchomepage-jpg-0">LChomepage.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/LChomepage_0.jpg"><img alt="screen shot of LC home page" height="225" width="351" style="height: 225px; width: 351px;" class="media-element file-default" data-delta="156" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/LChomepage_0-351x225.jpg" /></a>  </div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-2071" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/mylchomepage-jpg-0">MYLChomepage.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/MYLChomepage_0.jpg"><img alt="screen shot of My LC home page" height="225" width="349" style="height: 225px; width: 349px;" class="media-element file-default" data-delta="157" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/MYLChomepage_0-349x225.jpg" /></a>  </div>

  
</div>
</div></td>
</tr></table><h3><a name="sys-config-info" id="sys-config-info"></a>System Configuration Information</h3>
<ul><li>LC Homepage:
<ul><li><a href="https://hpc.llnl.gov">hpc.llnl.gov</a> (User Portal toggle) ==&gt; Hardware ==&gt; Compute Platforms</li>
<li>Direct link: <a href="/hardware/platforms">hpc.llnl.gov/hardware/platforms</a></li>
<li>All production systems appear in a summary table showing basic hardware information.</li>
<li>Diving on a machine's name will take you to a page of detailed hardware and configuration information for that machine.</li>
</ul></li>
<li>MyLC Portal:
<ul><li><a href="https://mylc.llnl.gov" target="_blank">mylc.llnl.gov</a></li>
<li>Click on a machine name in the "machine status" portlet, or the "my accounts" portlet.</li>
<li>Then select the "details", "topology" and/or "job limits" tabs for detailed hardware and configuration information.</li>
</ul></li>
<li>LC Tutorials:
<ul><li>Located on the LC Homepage under the "Training" menu.</li>
<li>Direct link: <a href="/training/tutorials">hpc.llnl.gov/training/tutorials</a></li>
</ul></li>
<li>Systems Summary Tables:
<ul><li>Systems Summary Table: <a href="/hardware/platforms">hpc.llnl.gov/hardware/platforms</a>. Concise summary of basic hardware information for LC systems.</li>
<li>LC Systems Summary: <a href="/sites/default/files/LC-systems-summary.pdf">hpc.llnl.gov/sites/default/files/LC-systems-summary.pdf</a>. Even more concise 1-page summary of LC production systems.</li>
</ul></li>
</ul><h3><a name="sys-config-commands" id="sys-config-commands"></a>System Configuration Commands</h3>
<ul><li>After logging into a machine, there are a number of commands that can be used for determining detailed, real-time machine hardware and configuration information.</li>
<li>A table of some useful commands with example output is provided below. Hyperlinked commands display their man page</li>
</ul><table class="table table-bordered table-striped"><tr><th scope="col">Command</th>
<th scope="col">Descriptionf</th>
<th scope="col">Example Output</th>
</tr><tr><td><span class="fixed">news job.lim.machinename</span></td>
<td>LC command for displaying system configuration, job limits and usage policies, where machinename is the actual name of the machine.</td>
<td><button>Example Output</button></td>
</tr><tr><td><span class="fixed"><strong><a href="/sites/default/files/lscpu_1.txt">lscpu</a></strong></span></td>
<td>Basic information about the CPU(s), including model, cores, sockets, threads, clock and cache.</td>
<td><button>Example Output</button></td>
</tr><tr><td><span class="fixed"><a href="/sites/default/files/lscpu_1.txt">lscpu</a> -eÂ </span></td>
<td>One line of basic information about the CPU(s), cores, sockets, threads and clock.</td>
<td><button>Example Output</button></td>
</tr><tr><td><span class="fixed">cat /proc/cpuinfo</span></td>
<td>Model and clock information for each thread of each core.</td>
<td><button>Example Output</button></td>
</tr><tr><td>
<p><span class="fixed"><a href="/sites/default/files/lstopo.txt">lstopo</a></span></p>
</td>
<td>Display a graphical topological map of node hardware.</td>
<td><button>Example Output</button></td>
</tr><tr><td><span class="fixed"><a href="/sites/default/files/lstopo.txt">lstopo</a> --only cores</span></td>
<td>List the physical cores only.</td>
<td><button>Example Output</button></td>
</tr><tr><td><span class="fixed"><a href="/sites/default/files/lstopo.txt">lstopo</a> -v</span></td>
<td>Detailed (verbose) information about a node's hardware components.</td>
<td><button>Example Output</button></td>
</tr><tr><td><span class="fixed"><a href="/sites/default/files/vmstat.txt">vmstat</a> -s</span></td>
<td>Memory configuration and usage details.</td>
<td><button>Example Output</button></td>
</tr><tr><td><span class="fixed">cat /proc/meminfo</span></td>
<td>Memory configuration and usage details.</td>
<td><button>Example Output</button></td>
</tr><tr><td><span class="fixed"><a href="https://hpc.llnl.gov/sites/default/files/uname.txt" target="_blank">uname</a> -a<br />distro_version<br />cat /etc/redhat-release<br />cat /etc/toss-release</span></td>
<td>Display operating system details, version.</td>
<td><button>Example Output</button></td>
</tr><tr><td><span class="fixed">bdf<br /><a href="https://hpc.llnl.gov/sites/default/files/df.txt" target="_blank">df</a> -h</span></td>
<td>Show mounted file systems.</td>
<td><button>Example Output</button></td>
</tr></table><h3><a name="sys-status-info" id="sys-status-info"></a>System Status Information</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1840" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/systemstatusmenu-png-0">systemStatusMenu.png</a></h2>
    
  
  <div class="content">
    <img height="323" width="500" class="media-element file-default" data-delta="132" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/systemStatusMenu_0.png" alt="" /></div>

  
</div>
</div></div>
<ul><li>LC Homepage:
<ul><li><a href="https://hpc.llnl.gov">hpc.llnl.gov</a> (User Portal toggle) - just look on the man page for the System Status links (shown at right).</li>
<li>The same links appear under the <a href="/hardware">Hardware</a> menu.</li>
<li>Unclassified systems only</li>
</ul></li>
<li>MyLC Portal:
<ul><li><a href="https://mylc.llnl.gov" target="_blank">mylc.llnl.gov</a></li>
<li>Several portlets provide system status information:
<ul><li>machine status</li>
<li>login node status</li>
<li>scratch file system status</li>
<li>enclave status</li>
</ul></li>
<li>Classified MyLC is at: lc.llnl.gov/lorenz/</li>
</ul></li>
<li>Machine status email lists:
<ul><li>Provide the most timely status information for system maintenance, problems, and system changes/updates</li>
<li>ocf-status and scf-status cover all machines on the OCF / SCF</li>
<li>Additionally, each machine has its own status list - for example: <strong><a href="mailto:sierra-status@llnl.gov">sierra-status@llnl.gov</a></strong></li>
</ul></li>
<li>Login banner &amp; news items - always displayed immediately after logging in
<ul><li>Login banner includes basic configuration information, announcements and news items. Example login banner <a href="/sites/default/files/loginBanner.txt">HERE</a>.</li>
<li>News items (unread) appear at the bottom of the login banner. For usage, type <span class="fixed">news -h</span>.</li>
</ul></li>
<li>Direct links for systems and file systems status pages:</li>
</ul><table class="table table-bordered table-striped"><tr><th scope="col">Description</th>
<th scope="col">Network</th>
<th scope="col">Links</th>
</tr><tr><td colspan="1" rowspan="3">System status web pages</td>
<td>OCF CZ</td>
<td><a href="https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" target="_blank">lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi</a></td>
</tr><tr><td>OCF RZ</td>
<td><a href="https://rzlc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" target="_blank">rzlc.llnl.gov/cgi-bin/lccgi/customstatus.cgi</a></td>
</tr><tr><td>SCF</td>
<td><a href="https://lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi" target="_blank">lc.llnl.gov/cgi-bin/lccgi/customstatus.cgi</a></td>
</tr><tr><td colspan="1" rowspan="4">File systems status web pages</td>
<td>OCF CZ</td>
<td><a href="https://lc.llnl.gov/fsstatus/fsstatus.cgi" target="_blank">lc.llnl.gov/fsstatus/fsstatus.cgi</a></td>
</tr><tr><td>OCF RZ</td>
<td><a href="https://rzlc.llnl.gov/fsstatus/fsstatus.cgi" target="_blank">rzlc.llnl.gov/fsstatus/fsstatus.cgi</a></td>
</tr><tr><td>OCF CZ+RZ</td>
<td><a href="https://rzlc.llnl.gov/fsstatus/allfsstatus.cg" target="_blank">rzlc.llnl.gov/fsstatus/allfsstatus.cg</a><a href="https://rzlc.llnl.gov/fsstatus/allfsstatus.cgi" target="_blank">i</a></td>
</tr><tr><td>SCF</td>
<td><a href="https://lc.llnl.gov/fsstatus/fsstatus.cgi" target="_blank">lc.llnl.gov/fsstatus/fsstatus.cgi</a></td>
</tr></table><h4>Examples</h4>
<table class="table table-bordered"><tr><td><a href="/sites/default/files/machineStatus1_1.gif"><div class="media media-element-container media-default"><div id="file-899" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/machinestatus1gif-1">machineStatus1.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/machineStatus1_1.gif"><img alt="CZ Machine Status" height="354" width="400" style="width: 400px; height: 354px;" class="media-element file-default" data-delta="77" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/machineStatus1_1-400x354.gif" /></a>  </div>

  
</div>
</div></a><br />CZ Machine Status</td>
<td><a href="/sites/default/files/fileSystemStatus.gif"><div class="media media-element-container media-default"><div id="file-900" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/filesystemstatusgif">fileSystemStatus.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/fileSystemStatus.gif" title="CZ + RZ File System Status"><img alt="CZ + RZ File System Status" title="CZ + RZ File System Status" height="359" width="400" style="width: 400px; height: 359px;" class="media-element file-default" data-delta="78" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/fileSystemStatus-400x359.gif" /></a>  </div>

  
</div>
</div></a><br />CZ + RZ File System Status</td>
<td><a href="/sites/default/files/lorenz3.gif"><div class="media media-element-container media-default"><div id="file-1841" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/lorenz3-gif">lorenz3.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/lorenz3.gif"><img alt="My LC user interface" height="357" width="400" style="height: 357px; width: 400px;" class="media-element file-default" data-delta="133" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/lorenz3-400x357.gif" /></a>  </div>

  
</div>
</div></a><br />mylc.llnl.gov</td>
</tr></table><h2><a name="exercise-1" id="exercise-1"></a>Exercise 1</h2>
<p>Logging in, basic configuration, and file systems information:</p>
<ul><li>Login to an LC cluster with X11 forwarding enabled</li>
<li>Test X11</li>
<li>Identify and SSH to other login nodes</li>
<li>Familiarize yourself with the cluster's configuration</li>
<li>Try the mxterm utility to access compute nodes</li>
<li>Learn where/how to obtain hardware, OS and other configuration information for LC clusters</li>
<li>Review basic file system info</li>
<li>Try moving files to the HPSS storage system</li>
<li>View file system status information</li>
</ul><h2><a name="development-environment" id="development-environment"></a>Software and Development Environment Overview</h2>
<h3><a name="deg" id="deg"></a>Development Environment Group (DEG)</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1843" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/degstaff-jpg">DEGstaff.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/DEGstaff.jpg"><img alt="Development Environment Group group photo" height="303" width="500" style="height: 303px; width: 500px;" class="media-element file-default" data-delta="135" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/DEGstaff-500x303.jpg" /></a>  </div>

  
</div>
</div></div>
<ul><li>LC's Development Environment Group (DEG) provides a stable, usable, leading-edge parallel application development environment that enables users to improve the reliability and scalable performance of LLNL applications.</li>
<li>DEG installs and supports the following LC software:
<ul><li>Compilers and Preprocessors</li>
<li>Debuggers</li>
<li>Memory Tools</li>
<li>Profiling Tools</li>
<li>Tracing Tools</li>
<li>Performance Analysis</li>
<li>Correctness Tools</li>
<li>Utilities</li>
</ul></li>
<li>Additionally, DEG's mission includes:
<ul><li>Working to make computing tools reliable, scalable and to help users make effective use of these tools.</li>
<li>Partnering with its application development user community to identify user requirements and evaluate tool effectiveness.</li>
<li>Collaborating with vendors and other third party software developers to ensure a complete environment in the most cost effective way possible and meet the needs of today's code developers utilizing emerging technologies.</li>
</ul></li>
<li>DEG Home Page: <a href="http://computing.llnl.gov/livermore-computing/development-environment-group" target="_blank">computing.llnl.gov/livermore-computing/development-environment-group</a></li>
</ul><h3><a name="toss" id="toss"></a>TOSS Operating System</h3>
<ul><li>TOSS = <strong>T</strong>ri-Laboratory <strong>O</strong>perating <strong>S</strong>ystem <strong>S</strong>tack</li>
<li>Based on Red Hat Enterprise Linux (RHEL) with modifications to support targeted HPC hardware and cluster computing</li>
<li>Used by most LC (and Tri-lab) production Linux clusters:
<ul><li>For Blue Gene systems, the login nodes use TOSS, but the compute nodes run a special Linux-like Compute Node Kernel (CNK).</li>
<li>CORAL EA and Sierra clusters use a "TOSS-like" OS/software stack, called <em><strong>blueos </strong></em>by LC.</li>
</ul></li>
<li>The primary components of TOSS include:
<ul><li>RHEL kernel optimized for large scale cluster computing</li>
<li>OpenFabrics Enterprise Distribution InfiniBand software stack including MVAPICH and OpenMPI libraries</li>
<li>Slurm workload manager</li>
<li>Integrated Lustre and Panasas parallel file system software</li>
<li>Scalable cluster administration tools</li>
<li>Cluster monitoring tools</li>
<li>GNU, C, C++ and Fortran90 compilers (GNU, Intel, PGI)</li>
<li>Testing software framework for hardware and operating system validation</li>
</ul></li>
<li>See <a href="http://www.redhat.com/" target="_blank">Redhat's documentation</a> for details on the RHEL kernel.</li>
<li>Version information for LC's clusters:
<ul><li>TOSS: <span class="fixed">distro_version</span> or cat <span class="fixed">/etc/toss-release</span></li>
<li>Redhat: <span class="fixed">cat /etc/redhat-release</span></li>
</ul></li>
</ul><h3><a name="software-lists" id="software-lists"></a>Software Lists, Documentation, and Downloads</h3>
<p>The table below lists and provides links to the majority of software available through LC and related organizations.</p>
<table class="table table-striped table-bordered"><tr><th scope="col">Software Category</th>
<th scope="col">Description and More Information</th>
</tr><tr><td>Compilers</td>
<td>Lists which compilers are available for each LC system: <a href="/software/development-environment-software/compilers">hpc.llnl.gov/software/development-environment-software/compilers</a></td>
</tr><tr><td>Supported Software and Computing Tools</td>
<td>Development Environment Group supported software includes compilers, libraries, debugging, profiling, trace generation/visualization, performance analysis tools, correctness tools, and several utilities: <a href="/software/development-environment-software">hpc.llnl.gov/software/development-environment-software.</a></td>
</tr><tr><td>Graphics Software</td>
<td>Graphics Group supported software includes visualization tools, graphics libraries, and utilities for the plotting and conversion of data: <a href="/data-vis/vis-software">hpc.llnl.gov/data-vis/vis-software</a></td>
</tr><tr><td>Mathematical Software Overview</td>
<td>Lists and describes the primary mathematical libraries and interactive mathematical tools available on LC machines: <a href="/software/mathematical-software">hpc.llnl.gov/software/mathematical-software</a></td>
</tr><tr><td>LINMath</td>
<td>The Livermore Interactive Numerical Mathematical Software Access Utility, is a Web-based access utility for math library software. The LINMath Web site also has pointers to packages available from external sources: <a href="https://www-lc.llnl.gov/linmath/" target="_blank">www-lc.llnl.gov/linmath/</a></td>
</tr><tr><td>Center for Applied Scientific Computing (CASC) Software</td>
<td>A wide range of software available for download from LLNL's CASC. Includes mathematical software, language tools, PDE software frameworks, visualization, data analysis, program analysis, debugging, and benchmarks: <a href="https://computing.llnl.gov/hpc/software" target="_blank">computing.llnl.gov/hpc/software</a>, <a href="https://computing.llnl.gov/projects" target="_blank">computing.llnl.gov/projects</a></td>
</tr><tr><td>LLNL Software Portal</td>
<td>Lab-wide portal of software repositories: <a href="https://software.llnl.gov/" target="_blank">software.llnl.gov/</a></td>
</tr></table><h3><a name="modules" id="modules"></a>Modules</h3>
<ul><li>Most LC clusters support Lmod modules for software packaging:
<ul><li>Provide a convenient, uniform way to select among multiple versions of software installed on LC systems.</li>
<li>Many LC software applications require that you load a particular "package" in order to use the software.</li>
</ul></li>
<li>Using Modules:</li>
</ul><pre>List available modules:     module avail
Load a module:              module add|load <em>modulefile</em>
Unload a module:            module rm|unload <em>modulefile</em>
List loaded modules:        module list
Read module help info:      module
Display module contents:    module display|show <em>modulefile</em></pre><ul><li>For more information see:
<ul><li><a href="/software/modules-and-software-packaging">LC modules documentation</a></li>
<li><a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod" target="_blank">TACC documentation: LMOD: ENVIRONMENTAL MODULES SYSTEM</a></li>
<li>The module man page</li>
</ul></li>
</ul><h3><a name="atlassian" id="atlassian"></a>Atlassian Tools - Confluence, JIRA, etc.</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-902" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/atlassianjpeg">atlassian.jpeg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/atlassian.jpeg"><img alt="Atlassian tools" height="149" width="225" style="width: 225px; height: 149px;" class="media-element file-default" data-delta="80" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/atlassian-225x149.jpeg" /></a>  </div>

  
</div>
</div></div>
<ul><li>LC supports a suite of web-based collaboration tools from Atlassian:
<ul><li><strong>Confluence Wiki:</strong> used for documentation, collaboration, knowledge sharing, file sharing, mockups, diagrams... anything you can put on a webpage.</li>
<li><strong>JIRA:</strong> issue tracking and project management system</li>
<li><strong>Bitbucket:</strong> for git repository hosting. Similar to popular sites like GitHub and Bitbucket, but it is intended for internal use on intranets.</li>
<li><strong>Bamboo:</strong> a continuous integration and delivery tool that combines automated builds, tests, and releases in a single workflow.</li>
</ul></li>
<li>All three collaboration tools:
<ul><li>Are based on LC usernames / groups and are intended to foster collaboration between LC users working on HPC projects.</li>
<li>Are installed on the CZ, RZ and SCF networks</li>
<li>Require authentication with your LC username and RSA PIN + token</li>
<li>Have a User Guide for usage information</li>
</ul></li>
<li>Locations:</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">Network</th>
<th scope="col">Confluence Wiki</th>
<th scope="col">JIRA</th>
<th scope="col">STASH</th>
</tr><tr><td>CZ</td>
<td><a href="https://lc.llnl.gov/confluence/" target="_blank">lc.llnl.gov/confluence/</a></td>
<td><a href="https://lc.llnl.gov/jira/" target="_blank">lc.llnl.gov/jira/</a></td>
<td><a href="https://lc.llnl.gov/stash/" target="_blank">lc.llnl.gov/stash/</a></td>
</tr><tr><td>RZ</td>
<td><a href="https://rzlc.llnl.gov/confluence/" target="_blank">rzlc.llnl.gov/confluence/</a></td>
<td><a href="https://rzlc.llnl.gov/jira/" target="_blank">rzlc.llnl.gov/jira/</a></td>
<td><a href="https://rzlc.llnl.gov/stash/" target="_blank">rzlc.llnl.gov/stash/</a></td>
</tr><tr><td>SCF</td>
<td>lc.llnl.gov/confluence/</td>
<td>lc.llnl.gov/jira/</td>
<td>lc.llnl.gov/stash/</td>
</tr></table><h3><a name="spack" id="spack"></a>Spack Package Manager</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-2072" class="file file-image file-image-png">

        <h2 class="element-invisible"><a href="/files/spack-png">spack.png</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/spack.png"><img alt="Spack logo" height="78" width="225" style="height: 78px; width: 225px;" class="media-element file-default" data-delta="158" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/spack-225x78.png" /></a>  </div>

  
</div>
</div></div>
<ul><li>Spack is a flexible package manager for HPC</li>
<li>Easy to download and install. For example:</li>
</ul><pre>% git clone https://github.com/spack/spack
% . spack/share/spack/setup-env.csh (or setup-env.sh)</pre><ul><li>There is an increasing number of software packages (over 4,200 as of May 2020) available for installation with Spack. Many open source contributions from the international community.</li>
<li>To view available packages:Â <span class="fixed">spack list</span></li>
<li>Then, to install a desired package:Â <span class="fixed">spack installÂ <em>packagename</em></span></li>
<li>Additional Spack features:
<ul><li>Allows installations to be customized. Users can specify the version, build compiler, compile-time options, and cross-compile platform, all on the command line.</li>
<li>Allows dependencies of a particular installation to be customized extensively.</li>
<li>Non-destructive installs - Spack installs every unique package/dependency configuration into its own prefix, so new installs will not break existing ones.</li>
<li>Creation of packages is made easy.</li>
</ul></li>
<li>Extensive documentation is available at:Â <a href="https://spack.readthedocs.io" target="_blank">spack.readthedocs.io</a></li>
</ul><h2><a name="compilers" id="compilers"></a>Compilers</h2>
<h3><a name="compilers-commands" id="compilers-commands"></a>Available Compilers and Invocation Commands</h3>
<ul><li>The table below summarizes compiler availability and invocation commands on LC <strong>Linux clusters</strong>.</li>
<li>For <strong>Sierra</strong> and <strong>CORAL EA</strong> compiler information, please see: <a href="/training/tutorials/using-lcs-sierra-system#compilers">hpc.llnl.gov/training/tutorials/using-lcs-sierra-system#compilers</a></li>
<li>Note that parallel compiler commands are actually LC scripts that ultimately invoke the corresponding serial compiler.</li>
<li>For details on the MPI parallel compiler commands, seeÂ <a href="https://computing.llnl.gov/tutorials/mpi/#LLNLÂ ">https://computing.llnl.gov/tutorials/mpi/#LLNLÂ </a></li>
</ul><table class="table table-striped table-bordered"><tr><th colspan="4" scope="col">Linux Cluster Compilers</th>
</tr><tr><td colspan="2" rowspan="1"><strong>Compiler</strong></td>
<td><strong>Serial Command</strong></td>
<td><strong>Parallel Commands</strong></td>
</tr><tr><td colspan="1" rowspan="3">Intel</td>
<td>C</td>
<td><span class="fixed">icc</span></td>
<td><span class="fixed">mpicc</span></td>
</tr><tr><td>C++</td>
<td><span class="fixed">icpc</span></td>
<td><span class="fixed">mpicxx, mpic++</span></td>
</tr><tr><td>Fortran</td>
<td><span class="fixed">ifort</span></td>
<td><span class="fixed">mpif77, mpif90, mpifort</span></td>
</tr><tr><td colspan="1" rowspan="3">GNU</td>
<td>C</td>
<td><span class="fixed">gcc</span></td>
<td><span class="fixed">mpicc</span></td>
</tr><tr><td>C++</td>
<td><span class="fixed">g++</span></td>
<td><span class="fixed">mpicxx, mpic++</span></td>
</tr><tr><td>Fortran</td>
<td><span class="fixed">gfortran</span></td>
<td><span class="fixed">mpif77, mpif90, mpifort</span></td>
</tr><tr><td colspan="1" rowspan="3">PGI</td>
<td>C</td>
<td><span class="fixed">pgcc</span></td>
<td><span class="fixed">mpipgcc, mpicc</span></td>
</tr><tr><td>C++</td>
<td><span class="fixed">pgc++</span></td>
<td><span class="fixed">mpicxx, mpic++</span></td>
</tr><tr><td>Fortran</td>
<td><span class="fixed">pgf77,Â pgf90, pgfortran</span></td>
<td><span class="fixed">mpif77, mpif90, mpifort</span></td>
</tr><tr><td colspan="1" rowspan="2">LLVM/Clang</td>
<td>C</td>
<td><span class="fixed">clang</span></td>
<td><span class="fixed">mpicc</span></td>
</tr><tr><td>C++</td>
<td><span class="fixed">clang++</span></td>
<td><span class="fixed">mpicxx, mpic++</span></td>
</tr></table><h3><a name="compilers-versions" id="compilers-versions"></a>Compiler Versions and Defaults</h3>
<ul><li>LC maintains multiple versions of each compiler.</li>
</ul><ul><li>The <a href="#modules">Modules</a> <span class="fixed">module avail</span> command is used to list available compilers and versions:</li>
</ul><p><span class="fixed">Â Â Â  module avail intel<br />Â Â Â  module avail gcc<br />Â Â Â  module avail pgi<br />Â Â Â  module avail clang</span></p>
<ul><li>Versions: to determine the actual version you are using, issue the compiler invocation command with its "version" option. For example:</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">Compiler</th>
<th scope="col">Option</th>
<th scope="col">Example</th>
</tr><tr><td>Intel</td>
<td><span class="fixed">version</span></td>
<td><span class="fixed">ifort --version</span></td>
</tr><tr><td>GNU</td>
<td><span class="fixed">version</span></td>
<td><span class="fixed">g++ --version</span></td>
</tr><tr><td>PGI</td>
<td><span class="fixed">-V</span></td>
<td><span class="fixed">pgf90 -V</span></td>
</tr><tr><td>Clang</td>
<td><span class="fixed">--version</span></td>
<td><span class="fixed">clang --version</span></td>
</tr></table><ul><li>To use an alternate version issue the Modules command: <span class="fixed">module load <em>module-name</em></span></li>
</ul><h3><a name="compilers-options" id="compilers-options"></a>Compiler Options</h3>
<ul><li>Each compiler has hundreds of options that determine what the compiler does and how it behaves.</li>
<li>The options used by one compiler mostly differ from other compilers.</li>
<li>Additionally, compilers have different default options.</li>
<li>An in-depth discussion of compiler options is beyond the scope of this tutorial.</li>
<li>See the compiler's documentation, man pages, and/or <span class="fixed">-help</span> or <span class="fixed">--help</span> option for details.</li>
</ul><h3><a name="compilers-docs" id="compilers-docs"></a>Compiler Documentation</h3>
<ul><li>Intel and PGI: compiler docs are included in the <span class="fixed">/opt/</span><em><span class="fixed">compilername</span></em><span> directory</span>. Otherwise, see Intel or PGI web pages.</li>
<li>GNU: see the web pages at <a href="https://gcc.gnu.org/" target="_blank">gcc.gnu.org/</a></li>
<li>LLVM/Clang: see the web pages at <a href="http://clang.llvm.org/docs/" target="_blank">clang.llvm.org/docs/</a></li>
<li>Man pages may/may not be available</li>
</ul><h3><a name="compilers-opt" id="compilers-opt"></a>Optimizations</h3>
<ul><li>All compilers are able to perform optimizations, though they will differ between compilers even though the compiler flags appear to be the same.</li>
<li>Optimizations are intended to make codes run faster, though this isn't guaranteed.</li>
<li>Some optimizations "rewrite" your code, and can make debugging difficult, since the source may not match the executable.</li>
<li>Optimizations can also produce wrong results, reduced precision, increased compile times and increased executable size.</li>
<li>The table below summarizes common compiler optimization options. See the compiler documentation for details and other optimization options.</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">Optimization</th>
<th scope="col">Intel</th>
<th scope="col">GNU</th>
<th scope="col">PGI</th>
</tr><tr><td><strong>-O</strong></td>
<td>Same as O2</td>
<td>Same as O1</td>
<td>O1 + global optimizations. No SIMD.</td>
</tr><tr><td><strong>-O0</strong></td>
<td>No optimization</td>
<td>DEFAULT. No optimization. Same as omitting any -O flag.</td>
<td>No optimization</td>
</tr><tr><td><strong>-O1</strong></td>
<td>Optimize for size: basic optimizations to create smallest code</td>
<td>Reduce code size and execution time, without performing any optimizations that take a great deal of compilation time.</td>
<td>Local optimizations, block scheduling and register allocation.</td>
</tr><tr><td><strong>-O2</strong></td>
<td>DEFAULT. Optimize for speed: O1 + additional optimizations such as basic loop and vectorization</td>
<td>Optimize even more. O1 + nearly all supported optimizations that do not involve a space-speed tradeoff.</td>
<td>DEFAULT. O1 + global optimizations + advanced optimizations including SIMD.</td>
</tr><tr><td><strong>-O3</strong></td>
<td>O2 + aggressive loop optimizations. Recommended for loop dominated codes.</td>
<td>O2 + further optimizations</td>
<td>O2 + aggressive global optimizations</td>
</tr><tr><td><strong>-O4</strong></td>
<td>n/a</td>
<td>n/a</td>
<td>O3 + hoisting of guarded invariant floating point expressions</td>
</tr><tr><td><strong>-Ofast</strong></td>
<td>Same as O3 (mostly)</td>
<td>Same as O3 + optimizations that disregard strict standards compliance.</td>
<td>n/a</td>
</tr><tr><td><strong>-fast</strong></td>
<td>O3 + several additional optimizations</td>
<td>n/a</td>
<td>Generally specifies global optimization. Actual optimizations vary from release to release.</td>
</tr><tr><td><strong>-Og</strong></td>
<td>n/a</td>
<td>Enables optimizations that do not interfere with debugging.</td>
<td>n/a</td>
</tr><tr><td><strong>Optimization / Vectorization report</strong></td>
<td>-opt-report<br />-vec-report</td>
<td>-ftree-vectorizer-verbose=[<em>1-7</em>]<br />-ftree-vectorizer-verbose=7</td>
<td>-Minfo=[<em>option</em>]<br />-Minfo=all</td>
</tr></table><h3><a name="compilers-flop" id="compilers-flop"></a>Floating-point Exceptions</h3>
<ul><li>The IEEE floating point standard defines several exceptions (FPEs) that occur when the result of a floating point operation is unclear or undesirable:
<ul><li><strong>overflow:</strong> an operation's result is too large to be represented as a float. Can be trapped, or else returned as a +/- infinity.</li>
<li><strong>underflow:</strong> an operation's result is too small to be represented as a normalized float. Can be trapped, or else represented as as a denormalized float (zero exponent w/ non-zero fraction) or zero.</li>
<li><strong>divide-by-zero:</strong> attempting to divide a float by zero. Can be trapped, or else returned as a +/- infinity.</li>
<li><strong>inexact:</strong> result was rounded off. Can be trapped or returned as rounded result.</li>
<li><strong>invalid:</strong> an operation's result is ill-defined, such as 0/0 or the sqrt of a negative number. Can be trapped or returned as NaN (not a number).</li>
</ul></li>
<li>By default, the Xeon processors used at LC mask/ignore FPEs. Programs that encounter FPEs will not terminate abnormally, but instead, will continue execution with the potential of producing wrong results.</li>
<li>Compilers differ in their ability to handle FPEs. See the relevant compiler documentation for details.</li>
</ul><h3><a name="compilers-precision" id="compilers-precision"></a>Precision, Performance, and IEEE 754 Compliance</h3>
<ul><li>Typically, most compilers do not guarantee IEEE 754 compliance for floating-point arithmetic unless it is explicitly specified by a compiler flag. This is because compiler optimizations are performed at the possible expense of precision.</li>
<li>Unfortunately for most programs, adhering to IEEE floating-point arithmetic adversely affects performance.</li>
<li>If you are not sure whether your application needs this, try compiling and running your program both with and without it to evaluate the effects on both performance and precision.</li>
<li>See the relevant compiler documentation for details.</li>
</ul><h3><a name="compilers-c-fortran" id="compilers-c-fortran"></a>Mixing C and Fortran</h3>
<ul><li>If you are linking C/C++ and FORTRAN code together, and need to explicitly specify the FORTRAN or C/C++ libraries on the link line.</li>
<li>All of the other issues involved with mixed language programming apply, such as:
<ul><li>Column-major vs. row-major array ordering</li>
<li>Routine name differences - appended underscores</li>
<li>Arguments passed by reference versus by value</li>
<li>Common blocks vs. extern structs</li>
<li>Memory alignment differences</li>
<li>File I/O - Fortran unit numbers vs. C/C++ file pointers</li>
<li>C++ name mangling</li>
<li>Data type differences</li>
</ul></li>
<li>Some useful references:
<ul><li>Mixed language programming using C++ and FORTRAN 77 from <a href="http://arnholm.org/software/cppf77/cppf77.htm" target="_blank">arnholm.org/software/cppf77/cppf77.htm</a></li>
<li>Using C/C++ and Fortran Together from <a href="http://www.yolinux.com/TUTORIALS/LinuxTutorialMixingFortranAndC.html" target="_blank">www.yolinux.com/TUTORIALS/LinuxTutorialMixingFortranAndC.html</a></li>
</ul></li>
</ul><h2><a name="debuggers" id="debuggers"></a>Debuggers</h2>
<ul><li><span class="note-red">Note:</span> This section only touches on selected highlights. For more information users will definitely need to consult the relevant documentation mentioned below. Also, please consult the "Supported Software and Computing Tools" web page located at <a href="/software">hpc.llnl.gov/software</a>.</li>
</ul><h3><a name="total-view" id="total-view"></a>TotalView</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-951" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/totalview-small-gif">totalview.small_.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/totalview.small_.gif"><img alt="TotalView Debugger screenshot" height="328" width="350" style="width: 350px; height: 328px;" class="media-element file-default" data-delta="81" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/totalview.small_-350x328.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>TotalView is probably the most widely used debugger for parallel programs. It can be used with C/C++ and Fortran programs and supports all common forms of parallelism, including pthreads, openMP, MPI, accelerators and GPUs.</li>
<li>Starting TotalView for serial codes: simply issue the command:</li>
</ul><p><span class="fixed">Â Â Â Â Â Â Â  totalview <em>myprog</em></span></p>
<ul><li>Starting TotalView for interactive parallel jobs:
<ul><li>Some special command line options are required to run a parallel job through TotalView under SLURM. You need to run <span class="fixed">srun</span> under TotalView, and then specify the <span class="fixed">-a</span> flag followed by 1)srun options, 2)your program, and 3)your program flags (in that order). The general syntax is: <span class="fixed">totalview srun -a -n #processes -p pdebug myprog [prog args]</span></li>
<li>To debug an already running interactive parallel job, simply issue the <span class="fixed">totalview</span> command and then attach to the srun process that started the job.</li>
<li>Debugging batch jobs is covered in LC's <a href="/totalview/">TotalView tutorial</a> and in the "Debugging in Batch" section below.</li>
</ul></li>
<li>Documentation:
<ul><li><a href="/totalview/">LC Tutorial</a></li>
<li>Vendor website: <a href="http://www.roguewave.com/" target="_blank">www.roguewave.com/</a></li>
</ul></li>
</ul><h3><a name="ddt" id="ddt"></a>DDT</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-952" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/ddt-small-gif">ddt.small_.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/ddt.small_.gif"><img alt="DDT screenshot" height="270" width="350" style="width: 350px; height: 270px;" class="media-element file-default" data-delta="82" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/ddt.small_-350x270.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>DDT stands for "Distributed Debugging Tool", a product of Allinea Software Ltd.</li>
<li>DDT is a comprehensive graphical debugger designed specifically for debugging complex parallel codes. It is supported on a variety of platforms for C/C++ and Fortran. It is able to be used to debug multi-process MPI programs, and multi-threaded programs, including OpenMP.</li>
<li>Currently, LC has a limited number of fixed and floating licenses for OCF and SCF Linux machines.</li>
<li>Usage information: see LC's DDT Quick Start information located at: <a href="/software/development-environment-software/allinea-ddt">hpc.llnl.gov/software/development-environment-software/allinea-ddt</a></li>
<li>Documentation: see the vendor website: <a href="http://www.allinea.com" target="_blank">www.allinea.com</a></li>
</ul><h3><a name="stat" id="stat"></a>STAT - Stack Trace Analysis Tool</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-953" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/stat1-gif">STAT1.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/STAT1.gif"><img alt="STAT" height="252" width="350" style="width: 350px; height: 252px;" class="media-element file-default" data-delta="83" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/STAT1-350x252.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>The Stack Trace Analysis Tool gathers and merges stack traces from a parallel application's processes.</li>
<li>STAT is particularly useful for debugging hung programs.</li>
<li>It produces call graphs: 2D spatial and 3D spatial-temporal
<ul><li>The 2D spatial call prefix tree represents a single snapshot of the entire application (see image).</li>
<li>The 3D spatial-temporal call prefix tree represents a series of snapshots from the application taken over time.</li>
</ul></li>
<li>In these graphs, the nodes are labeled by function names. The directed edges, showing the calling sequence from caller to callee, are labeled by the set of tasks that follow that call path. Nodes that are visited by the same set of tasks are assigned the same color, giving a visual reference to the various equivalence classes.</li>
<li>This tool should be in your default path as:
<ul><li><span class="fixed">/usr/local/bin/stat-gui</span> - GUI</li>
<li><span class="fixed">/usr/local/bin/stat-cl</span> - command line</li>
<li><span class="fixed">/usr/local/bin/stat-view</span> - viewer for DOT format output files</li>
<li><span class="fixed">/usr/local/tools/stat</span> - install directory, documentation</li>
</ul></li>
<li>More information: see the STAT web page at: <a href="/software/development-environment-software/stat-stack-trace-analysis-tool">hpc.llnl.gov/software/development-environment-software/stat-stack-trace-analysis-tool</a></li>
</ul><h3><a name="debugging-batch" id="debugging-batch"></a>Debugging in Batch: mxterm / sxterm</h3>
<ul><li>Debugging batch parallel jobs on LC production clusters is fairly straightforward. The main idea is that you need to submit a batch job that gets your partition allocated and running.</li>
<li>Once you have your partition, you can <span class="fixed">rsh</span> to any of the nodes within it, and then starting running as though youâ€™re in the interactive pdebug partition.</li>
<li>For convenience, LC has developed the <span class="fixed">mxtermÂ / sxterm</span> utilitiesÂ which makes the process even easier.</li>
<li>How to use <span class="fixed">mxterm / sxterm</span>:
<ul><li>If you are on a Windows PC, start your X11 application (such as X-Win32)</li>
<li>Make sure you enable X11 tunneling for your ssh session</li>
<li>ssh and login to your cluster</li>
<li>Issue the command as follows:<br /><pre>mxterm #nodes #tasks #minutes
sxterm #nodes #tasks #minutes</pre><p><br />Where: #nodes = number of nodes your job requires<br />#tasks = number of tasks your job requires<br />#minutes = how long you need to keep your partition for debugging</p></li>
<li>This will submit a batch job for you that will open an xterm when it starts to run.</li>
<li>After the xterm appears, <span class="fixed">cd</span> to the directory with your source code and begin your debug session.</li>
<li>This utility does not have a man page, however you can view the usage information by simple typing the name of the command.</li>
</ul></li>
</ul><h3><a name="debuggers-other" id="debuggers-other"></a>Other Debuggers</h3>
<ul><li>Several other common debuggers are available on LC Linux clusters, though they are not recommended for parallel programs when compared to TotalView and DDT.</li>
<li>PGDBG: the Portland Group Compiler debugger. Documentation: <a href="https://www.pgroup.com/products/pgdbg.htm" target="_blank">www.pgroup.com/products/pgdbg.htm</a></li>
<li>GDB: GNU GDB debugger, a command-line, text-based, single process debugger. Documentation: <a href="http://www.gnu.org/software/gdb" target="_blank">www.gnu.org/software/gdb</a></li>
<li>DDD: GNU DDD debugger is a graphical front-end for command-line debuggers such as GDB, DBX, WDB, Ladebug, JDB, XDB, the Perl debugger, the bash debugger, or the Python debugger. Documentation: <a href="http://www.gnu.org/software/ddd" target="_blank">www.gnu.org/software/ddd</a></li>
</ul><h3><a name="debuggers-hints" id="debuggers-hints"></a>A Few Additional Useful Debugging Hints</h3>
<ul><li><strong>Core Files:</strong>
<ul><li>It is quite likely that your shell's core file size setting may limit the size of a core file so that it is inadequate for debugging, especially with TotalView.</li>
<li>To check your shell's limit settings, use either the <span class="fixed">limit</span> (csh/tcsh) or <span class="fixed">ulimit -a</span> (sh/ksh/bash) command. For example:</li>
</ul></li>
</ul><table class="table table-bordered"><tr><td>
<p><span class="fixed">Â  Â  Â  Â  <span class="text-danger">limit</span><br />Â Â Â Â Â Â Â  cputimeÂ Â Â Â Â  unlimited<br />Â Â Â Â Â Â Â  filesizeÂ Â Â Â  unlimited<br />Â Â Â Â Â Â Â  datasizeÂ Â Â Â  unlimited<br />Â Â Â Â Â Â Â  stacksizeÂ Â Â  unlimited<br />Â Â Â Â Â Â Â  coredumpsize 16 kbytes<br />Â Â Â Â Â Â Â  memoryuseÂ Â Â  unlimited<br />Â Â Â Â Â Â Â  vmemoryuseÂ Â  unlimited<br />Â Â Â Â Â Â Â  descriptorsÂ  1024<br />Â Â Â Â Â Â Â  memorylocked 7168 kbytes<br />Â Â Â Â Â Â Â  maxprocÂ Â Â Â Â  1024</span></p>
<p><span class="fixed">Â Â Â Â Â Â Â  <span class="text-danger">ulimit -a</span><br />Â Â Â Â Â Â Â  address space limit (kbytes)Â Â  (-M)Â  unlimited<br />Â Â Â Â Â Â Â  core file size (blocks)Â Â Â Â Â Â Â  (-c)Â  32<br />Â Â Â Â Â Â Â  cpu time (seconds)Â Â Â Â Â Â Â Â Â Â Â Â  (-t)Â  unlimited<br />Â Â Â Â Â Â Â  data size (kbytes)Â Â Â Â Â Â Â Â Â Â Â Â  (-d)Â  unlimited<br />Â Â Â Â Â Â Â  file size (blocks)Â Â Â Â Â Â Â Â Â Â Â Â  (-f)Â  unlimited<br />Â Â Â Â Â Â Â  locksÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (-L)Â  unlimited<br />Â Â Â Â Â Â Â  locked address space (kbytes)Â  (-l)Â  7168<br />Â Â Â Â Â Â Â  nofileÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (-n)Â  1024<br />Â Â Â Â Â Â Â  nprocÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (-u)Â  1024<br />Â Â Â Â Â Â Â  pipe buffer size (bytes)Â Â Â Â Â Â  (-p)Â  4096<br />Â Â Â Â Â Â Â  resident set size (kbytes)Â Â Â Â  (-m)Â  unlimited<br />Â Â Â Â Â Â Â  socket buffer size (bytes)Â Â Â Â  (-b)Â  4096<br />Â Â Â Â Â Â Â  stack size (kbytes)Â Â Â Â Â Â Â Â Â Â Â  (-s)Â  unlimited<br />Â Â Â Â Â Â Â  threadsÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (-T)Â  not supported<br />Â Â Â Â Â Â Â  process size (kbytes)Â Â Â Â Â Â Â Â Â  (-v)Â  unlimited</span></p>
</td>
</tr></table><ul><li>To override your default core file size setting, use one of the following commands:</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="row">csh/tcsh</th>
<td><span class="fixed">unlimit</span><br />-or-<br /><span class="fixed">limit coredumpsize 64</span></td>
</tr><tr><th scope="row">sh/ksh/bash</th>
<td><span class="fixed">ulimit -c 64</span></td>
</tr></table><ul><li>Some users have complained that for many-process jobs, they actually don't want core files or only want small core files because normal core files can fill up their disk space. The <span class="fixed">limit</span> (csh/tcsh) or <span class="fixed">ulimit -c</span> (sh/ksh/bash) commands can be used as shown above to set smaller / zero sizes.</li>
<li>Add the <span class="fixed">sinfo</span> and <span class="fixed">squeue</span> commands to your batch scripts to assist in diagnosing problems. In particular, these commands will document which nodes your job is using.</li>
<li>Also add the <span class="fixed">-l</span> option to your <span class="fixed">srun</span> command so that output statements are prepended with the task number that created them.</li>
<li>Be sure to check the exit status of all I/O operations when reading or writing files in Lustre. This will allow you to detect any I/O problems with the underlying OST servers.</li>
<li>If you know/suspect that there are problems with particular nodes, you can use the srun <span class="fixed">-x</span> option to skip these nodes. For example: <span class="fixed">srun -N12 -x "cab40 cab41" -ppdebug myjob</span></li>
</ul><h2><a name="performance-analysis" id="performance-analysis"></a>Performance Analysis Tools</h2>
<h3><a name="book" id="book"></a>We Need a Book!</h3>
<ul><li>The subject of Performance Analysis Tools is far too broad and deep to cover here. Instead, a few pointers are being provided for those who are interested in further research.</li>
<li>The first place to check are the "Development Environment Software" web pages at:Â  <a href="/software/development-environment-software">hpc.llnl.gov/software/development-environment-software</a> for what may be available here. Some example tools are listed below.</li>
</ul><h3><a name="mem-correctness" id="mem-correctness"></a>Memory Correctness Tools</h3>
<p><strong>Memcheck: </strong>Valgrind's Memcheck tool detects a comprehensive set of memory errors, including reads and writes of unallocated or freed memory and memory leaks.</p>
<p><strong>TotalView: </strong>Allows you to stop execution when heap API problems occur, list memory leaks, paint allocated and deallocated blocks, identify dangling pointers, hold onto deallocated memory, graphically browse the heap, identify the source line and stack backtrace of an allocation or deallocation, summarize heap use by routine, filter and dump heap information, and review memory usage by process or by library.</p>
<p><strong>memP: </strong>The memP tool provides heap profiling through the generation of two reports: a summary of the heap high-water-mark across all processes in a parallel job as well as a detailed task-specific report that provides a snapshot of the heap memory currently in use, including the amount allocated at specific call sites.</p>
<p><strong>Intel Inspector: </strong>Primarily a thread correctness tool, but memory debugging features are included.</p>
<h3><a name="profiling" id="profiling"></a>Profiling, Tracing, and Performance Analysis</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-954" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/perfanalysis-jpg">perfAnalysis.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/perfAnalysis.jpg"><img alt="perf Analysis" height="256" width="400" style="width: 400px; height: 256px;" class="media-element file-default" data-delta="84" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/perfAnalysis-400x256.jpg" /></a>  </div>

  
</div>
</div></div>
<p><strong>Open|SpeedShop: </strong>Open|SpeedShop is a comprehensive performance tool set with a unified look and feel that covers most important performance analysis steps. It offers various different interfaces, including a flexible GUI, a scripting interface, and a Python class. Supported experiments include profiling using PC sampling, inclusive and exclusive execution time analysis, hardware counter support, as well as MPI, I/O, and floating point exception tracing. All analysis is applied on unmodified binaries and can be used on codes with MPI and/or thread parallelism.</p>
<p><strong>TAU: </strong>TAU is a robust profiling and tracing tool from the University of Oregon that includes support for MPI and OpenMP. TAU provides an instrumentation API, but source code can also be automatically instrumented and there is support for dynamic instrumentation as well. TAU is generally viewed as having a steep learning curve, but experienced users have applying the tool with good results at LLNL. TAU can be configured with many feature combinations. If the features you are interested in are not available in the public installation, please request the appropriate configuration through the hotline. TAU developer response is excellent, so if you are encountering a problem with TAU, there is a good chance it can be quickly addressed.</p>
<p><strong>HPCToolkit: </strong>HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the largest supercomputers. It uses low overhead statistical sampling of timers and hardware performance counters to collect accurate measurements of a program's work, resource consumption, and inefficiency and attributes them to the full calling context in which they occur. HPCToolkit works with C/C++/Fortran applications that are either statically or dynamically linked. It supports measurement and analysis of serial codes, threaded codes (pthreads, OpenMP), MPI, and hybrid (MPI + threads) parallel codes.</p>
<p><strong>mpiP:</strong> A lightweight MPI profiling library that provides time spent in MPI functions by callsite and stacktrace. This tool is developed and maintained at LLNL, so support and modifications can be quickly addressed. New run-time functionality can be used to generate mpiP data without relinking through the srun-mpip and poe-mpip scripts on Linux and AIX systems.</p>
<p><strong>gprof: </strong> Displays call graph profile data. The gprof command is useful in identifying how a program consumes CPU resources. Gprof does simple function profiling and requires that the code be built and linked with -pg. For parallel programs, in order to get a unique output file for each process, you will need to set the undocumented environment variable GMON_OUT_PREFIX to some non-null string. For example: <span class="fixed">setenv GMON_OUT_PREFIX 'gmon.out.'`/bin/uname -n`</span></p>
<p><strong>pgprof</strong>: PGI profiler - pgprof is a tool which analyzes data generated during execution of specially compiled programs. This information can be used to identify which portions of a program will benefit the most from performance tuning.</p>
<p><strong>PAPI: </strong>Portable hardware performance counter library.</p>
<p><strong>PapiEx: </strong>A PAPI-based performance profiler that measures hardware performance events of an application without having to instrument the application.</p>
<p><strong>VTune Amplifier: </strong>The Intel VTune Amplifier tool is a performance analysis tool for finding hotspots in serial and multithreaded codes. Note the installation on LC machines does not include the advanced hardware analysis capabilities.</p>
<p><strong>Intel Profiler: </strong>The Intel Profiler tool is built into the Intel compiler along with a simple GUI to display the collected results.</p>
<p><strong>Vampir / Vampirtrace: </strong>Full featured trace file visualizer and library for generating trace files for parallel programs.</p>
<h3><a name="beyond-lc" id="beyond-lc"></a>Beyond LC</h3>
<ul><li>Beyond LC, the web offers endless opportunities for discovering tools that aren't available here.</li>
<li>In many cases, users can install tools in their own directories if LC doesn't have/support them.</li>
</ul><h2><a name="graphics-software-resources" id="graphics-software-resources"></a>Graphics Software and Resources</h2>
<ul><li>LC's Information Management and Graphics Group (IMGG) supports a wide range of graphics related software packages.</li>
<li>For a complete list and getting started instructions see: <a href="/data-vis/vis-software">hpc.llnl.gov/data-vis/vis-software</a></li>
<li>Contacts and more information: <a href="/data-vis">hpc.llnl.gov/data-vis</a></li>
</ul><h3><a name="consulting" id="consulting"></a>Consulting</h3>
<ul><li>Consulting on scientific visualization issues (demos, classes, getting started, visualization advice)</li>
<li>Maintaining a graphics environment that is current and consistent across LC computing platforms</li>
<li>Graphics software support</li>
<li>Troubleshooting graphics related problems</li>
<li>Also available to work with customers to develop custom data visualization applications</li>
</ul><h3><a name="video-prod" id="video-prod"></a>Video Production</h3>
<ul><li>State-of-the-art unclassified and classified facilities</li>
<li>3D modeling and computer animation</li>
<li>Video and audio editing; quick turnaround of videos</li>
<li>Also, photo quality output and DVD/CDROM creation</li>
</ul><h3><a name="viz-resources" id="viz-resources"></a>Visualization Machine Resources</h3>
<ul><li>LC provides dedicated clusters for visualization work.</li>
<li>Accounts must be requested through the LC Hotline for use of the visualization machines.</li>
</ul><h3><a id="power-walls" name="power-walls"></a>Power Walls</h3>
<p>Large screen displays for viewing high-resolution images and data comparisons</p>
<table><tr><td><div class="media media-element-container media-default"><div id="file-955" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/powerwall-jpg">powerwall.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/powerwall.jpg"><img alt="Power wall" height="250" width="395" style="width: 395px; height: 250px;" class="media-element file-default" data-delta="159" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/powerwall-395x250.jpg" /></a>  </div>

  
</div>
</div></td>
<td><div class="media media-element-container media-default"><div id="file-957" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/theaters-jpg-0">theaters.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/theaters_0.jpg"><img alt="collage of different power wall theaters at the lab" height="250" width="406" style="width: 406px; height: 250px;" class="media-element file-default" data-delta="160" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/theaters_0-406x250.jpg" /></a>  </div>

  
</div>
</div></td>
</tr></table><h2><a name="running-jobs" id="running-jobs"></a>Running Jobs</h2>
<h3><a name="where" id="where"></a>Where to Run?</h3>
<p><span class="note-red">Note:</span>This section only provides a general overview on running jobs on LC systems. Details associated with running jobs are covered in depth in other <a href="/training/tutorials">LC tutorials</a> (Slurm and Moab, MPI, BG/Q, OpenMP, Pthreads, etc.)</p>
<h4>Determining Your Job's Requirements</h4>
<ul><li>The first thing that needs to be done before deciding where to run your jobs is to determine your jobs' requirements.</li>
<li>There are several important factors to consider, because not all LC clusters may match your jobs' requirements:</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">Requirement</th>
<th scope="col">Questions?</th>
</tr><tr><td>Machine architecture</td>
<td>
<ul><li>Sierra? Linux?</li>
<li>GPU?</li>
<li>Specific processor model?</li>
</ul></td>
</tr><tr><td>Number of nodes/cores</td>
<td>
<ul><li>How many nodes/cores will your jobs require?</li>
<li>What job queues are available on a cluster and will your jobs fit into their node limits?</li>
<li>Are you running a serial application?</li>
</ul></td>
</tr><tr><td>Wall clock run time</td>
<td>
<ul><li>How much time will your jobs require to complete?</li>
<li>What job queues are available on a cluster and will your jobs fit into their time limits?</li>
</ul></td>
</tr><tr><td>Memory</td>
<td>
<ul><li>Will your jobs need more memory than is available on a cluster?</li>
<li>May need to use a memory profiling tool to accurately assess your application's requirements.</li>
</ul></td>
</tr><tr><td>Communication/network</td>
<td>
<ul><li>Will your jobs require the ability to communicate between nodes?</li>
</ul></td>
</tr><tr><td>Accounts/banks</td>
<td>
<ul><li>Which machines are you able to get an account on?</li>
<li>Which banks are configured and available for you to use on a given machine?</li>
</ul></td>
</tr></table><h4>Getting Machine Configuration Information</h4>
<ul><li>After determining your jobs' requirements, the next step is to see which LC machines match those requirements.</li>
<li>LC's production clusters vary widely in their configurations.</li>
<li>Fortunately, getting configuration details for any production LC machine is easy to do.</li>
<li>See the previous <a href="#system-status">System Status and Configuration Information</a> section for details.</li>
</ul><h4>Job Limits</h4>
<ul><li>For all production clusters, there are defined job limits which vary from cluster to cluster. The primary job limits apply to:
<ul><li>How many nodes/cores a job may use</li>
<li>How long a job may run</li>
<li>How many simultaneous jobs can a user run?</li>
<li>What time of the day/week can jobs run?</li>
</ul></li>
<li>Most job limits are enforced by the batch system</li>
<li>Some job limits are enforced by a <strong>"good neighbor"</strong> policy</li>
<li>An easy way to determine the job limits for a machine where you are logged in is to use the command: <span class="fixed">news job.lim.machinename</span> where <em><span class="fixed">machinename</span></em> is the name of the machine you are logged into.</li>
<li>Job limits are also documented on the "MyLC" web pages:<br />OCF-CZ: <a href="https://mylc.llnl.gov" target="_blank">mylc.llnl.gov</a><br />OCF-RZ: <a href="http://rzmylc.llnl.gov" target="_blank">rzmylc.llnl.gov</a><br />SCF: lc.llnl.gov/lorenz<br />Just click on any machine name in the "machine status" or "my accounts" portlets. Then select the "job limits" tab.</li>
<li>Further discussion, and a summary table of job limits for all production machines are available in the <a href="/training/tutorials/slurm-and-moab#QueueLimits">Queue Limits</a> section of the Slurm and Moab tutorial.</li>
</ul><h4>Accounts and Banks</h4>
<ul><li>In order to run on a machine, you need a valid login account, and at least one "bank" to charge your usage against.</li>
<li>Getting information about your assigned bank(s) is covered in the <a href="/training/tutorials/slurm-and-moab#Banks">Banks</a> and <a href="/training/tutorials/slurm-and-moab#BanksUsage">Banks and Usage Information</a> sections of the Slurm and Moab tutorial.</li>
</ul><h4>Serial vs Parallel</h4>
<ul><li>Parallel jobs can range in size from a single node (multi-core) to the full system.</li>
<li>All but a few of LC's production clusters are intended to be used for parallel jobs.</li>
<li>Serial jobs by definition require only one core on a node.</li>
<li>Because running serial jobs on parallel clusters would waste compute resources, LC provides several clusters where serial jobs can be run without wasting compute resources.</li>
<li>Running serials jobs is discussed here:Â <a href="/training/tutorials/slurm-and-moab#Serial">/training/tutorials/slurm-and-moab#Serial</a></li>
</ul><h4>Dedicated Application Time (DAT)</h4>
<ul><li>Some LC systems allow users to request dedicated machine use.</li>
<li>DAT is primarily for "big runs" that require a larger number of nodes and/or longer time than the normal batch limits permit.</li>
<li>During DATs, other user jobs are drained/removed and the batch queue is suspended while the dedicated job runs.</li>
<li>DATs are posted to the relevant machine status news list.</li>
<li>DATs are requested through the LC Hotline. Forms for requesting "Expedited Priority Runs" are available at: <a href="/accounts/forms">hpc.llnl.gov/accounts/forms</a>.</li>
</ul><h3><a name="batch-vs-interactive" id="batch-vs-interactive"></a>Batch Versus Interactive</h3>
<h4>Interactive Jobs</h4>
<ul><li>Refers to jobs that you launch and interact with real-time from the command line.</li>
<li>Don't use Login Nodes (in most cases):
<ul><li>Login nodes are shared, oftentimes by many users.</li>
<li>Intended for non-cpu intensive interactive tasks, such as editing, working with files, running GUIs, browsers, debuggers, etc.</li>
<li>Not intended for running cpu-intensive production jobs, which can negatively impact other users.</li>
</ul></li>
<li>Most clusters have a <strong>pdebug </strong>partition of compute nodes dedicated to small, short, interactive jobs. Another common interactive partition isÂ <strong>pvis</strong>.</li>
<li>There are several ways to run interactive jobs on LC's Linux clusters, as described below.
<ul><li>For interactive jobs on CORAL clusters, see:Â <a href="/training/tutorials/using-lcs-sierra-system#Interactive">hpc.llnl.gov/training/tutorials/using-lcs-sierra-system#Interactive</a>.</li>
</ul></li>
<li>Method 1: Use <span class="fixed">srun</span> on a login node specifying compute nodes in the <strong>pdebug</strong> or other interactive partition.
<ul><li>To view the partitions on a cluster use the <span class="fixed">sinfo -s</span> command. It shows which partitions are configured, their time limit and how the nodes are currently being used (Allocated / Idle / Other / Total). For example, on Quartz note the pdebug partition:
<pre>% <span class="text-danger">sinfo -s</span>
PARTITION AVAIL  TIMELIMIT   NODES(A/I/O/T)  NODELIST
pdebug       up      30:00       20/26/0/46  quartz[1-46]
pbatch*      up 1-00:00:00  2865/52/13/2930  quartz[47-186,193-378,...,2887-3072]
pall       down   infinite  2885/78/13/2976  quartz[1-186,193-378,...,2887-3072]
</pre></li>
<li>Then use the <span class="fixed">srun</span> command to run the job on compute nodes in pdebug. For example, to run your executable on 2 pdebug nodes using 16 tasks:<br /><pre>% <span class="text-danger">srun -p pdebug -N 2 -n 16 myexe</span></pre></li>
</ul></li>
<li>Method 2: Use <span class="fixed">salloc</span> to acquire compute nodes for interactive use.
<ul><li>On a login node, request an allocation of compute nodes, and optionally the partition to acquire them from. The default is usually pbatch, but small short jobs usually get better turnaround time in pdebug. For example, to request 2 pdebug compute nodes:
<pre>% <span class="text-danger">salloc -p pdebug -N 2</span>
salloc: Pending job allocation 3891090
salloc: job 3891090 queued and waiting for resources
salloc: job 3891090 has been allocated resources
salloc: Granted job allocation 3891090</pre></li>
<li>After your allocation is granted, you will be placed on the first node of the allocation. You can verify this as follows:<br /><pre>% <span class="text-danger">squeue -u joeuser</span>
  JOBID PARTITION     NAME      USER ST       TIME  NODES NODELIST(REASON)
3891090    pdebug       sh   joeuser  R       0:07      2 quartz[1-2]
% <span class="text-danger">hostname</span>
quartz1</pre></li>
<li>You can now run whatever you'd like interactively on your job's compute nodes.</li>
</ul></li>
<li>Method 3: Use <span class="fixed">sxterm / mxterm</span> to acquire compute nodes for interactive use.
<ul><li>Similar to <span class="fixed">salloc</span>, but different because it will actually "behind the scenes" create a batch job for your request, queue it and then when it starts to run on a compute node, pop open an xterm window on your desktop.</li>
<li>For example requesting 1 node / 1 task / for 30 minutes in the pdebug partition:<br /><pre>% <span class="text-danger">sxterm 1 1 30 -p pdebug</span>
Submitted batch job 3048379</pre></li>
<li>Wait until the xterm window appears on your desktop. You will be on the first node of your job's allocation and can run whatever you'd like interactively.</li>
<li>There's no man page - just invoke "sxterm" with no options and a usage message will display.</li>
<li>The <span class="fixed">mxterm</span> command is similar, with different options. Just invoke "mxterm" for usage information.</li>
</ul></li>
<li>Method 4: Submit a batch job and then <span class="fixed">rsh/ssh</span> to the compute nodes where it is running.
<ul><li>You can submit a batch job (covered next) that really doesn't do anything except "sleep".</li>
<li>The <span class="fixed">squeue</span> command can be used to show when and where your job is running.</li>
<li>After it starts running, you can <span class="fixed">rsh</span> or <span class="fixed">ssh</span> to any of its compute nodes.</li>
<li>Once on a compute node, you can run whatever you'd like interactively.</li>
</ul></li>
<li>Important usage notes about the <strong>pdebug</strong> partition:
<ul><li>As the name pdebug implies, interactive jobs should be short, small debugging jobs, not production runs.</li>
<li>Shorter time limit</li>
<li>Fewer number of nodes permitted</li>
<li>There is usually a "good neighbor" policy in effect - don't monopolize the queue or setup streams of jobs</li>
</ul></li>
</ul><h4>Batch Jobs</h4>
<p><span class="note-red">Note:</span>This section only provides a quick summary of batch usage on LC's Linux clusters. For details, see the <a href="/training/tutorials/slurm-and-moab">Slurm and Moab</a> tutorial. For CORAL/Sierra systems see the <a href="/training/tutorials/using-lcs-sierra-system">Sierra tutorial</a>.</p>
<ul><li>Typically, most of a cluster's compute nodes are configured into a <strong>pbatch</strong> partition.</li>
<li>The pbatch partition is intended for production work:
<ul><li>Longer time limits</li>
<li>Larger number of nodes per job</li>
<li>Limits enforced by batch system rather than "good neighbor" policy</li>
</ul></li>
<li>The pbatch partition is managed by the workload manager</li>
<li>Batch jobs must be submitted in the form of a job control script. Example batch job script:</li>
</ul><pre>#!/bin/tcsh
##### These lines are for Slurm
#SBATCH -N 16
#SBATCH -J parSolve34
#SBATCH -t 2:00:00
#SBATCH -p pbatch
#SBATCH --mail-type=ALL
#SBATCH -A myAccount
#SBATCH -o /p/lustre1/joeuser/par_solve/myjob.out

##### These are shell commands
date
cd /p/lustre1/joeuser/par_solve
##### Launch parallel job using srun - assumes 36 cores/node
srun -n576 a.out
echo 'Done'</pre><ul><li>Example batch job script submission commands:</li>
</ul><pre>sbatch myjobscript
sbatch myjobscript -p ppdebug -A mic
sbatch myjobscript -t 45:00</pre><ul><li>After successfully submitting a job, you may then check its progress and interact with it (hold, release, alter, terminate) by means of other batch commands - discussed in the <a href="#interacting">Interacting with Jobs</a> section.</li>
<li>Some clusters have additional partitions permitting batch jobs.</li>
<li>Interactive use of pbatch nodes is facilitated by using the <span class="fixed">mxterm</span> command - discussed in the <a href="#login">Where to Login</a> section.</li>
<li>Interactive debugging of batch jobs is possible - covered in the <a href="#debuggers">Debuggers</a> section.</li>
</ul><h3><a name="starting-jobs" id="starting-jobs"></a>Starting Jobs - srun</h3>
<p><span class="note-red">Note:</span>This section only provides a quick summary of batch usage on LC's Linux clusters. For details, see the <a href="/training/tutorials/slurm-and-moab" target="_blank">Slurm and Moab tutorial</a>.</p>
<h4>The srun Command</h4>
<ul><li>The SLURM <span class="fixed">srun</span> command is required to launch <em>parallel</em> jobs - both batch and interactive.</li>
<li>It should also be used to launch <em>serial</em> jobs in the <strong>pdebug</strong> and other interactive queues.</li>
<li>Syntax:<br /><br /><strong><a href="https://computing.llnl.gov/tutorials/moab/man/srun.txt" target="_blank">srun</a>Â </strong>Â  <strong>[option list]Â Â  [executable]Â Â  [args]</strong><br /><br />Note that srun options must precede your executable.</li>
</ul><ul><li>Interactive use example, from the login node command line. Specifies 2 nodes (<strong>-N</strong>), 16 tasks (<strong>-n</strong>) and the interactive pdebug partition (<strong>-p</strong>):</li>
</ul><pre>% srun -N2 -n16 -ppdebug myexe</pre><ul><li>Batch use example requesting 16 nodes and 256 tasks (assumes nodes have 16 cores):
<ul><li>First create a job script that requests nodes via <span class="fixed">#SBATCH -N</span> and uses <span class="fixed">srun</span> to specify the number of tasks and launch the job.</li>
</ul></li>
</ul><pre>#!/bin/csh
#SBATCH -N 16
#SBATCH -t 2:00:00
#SBATCH -p pbatch
# Run info and srun job launch
cd /p/lscratch3/joeuser/par_solve
srun -n256 a.out
echo 'Done'</pre><ul><li>Then submit the job script from the login node command line:</li>
</ul><pre>% sbatch myjobscript</pre><ul><li>Primary differences between batch and interactive usage:</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="col">Difference</th>
<th scope="col">Interactive</th>
<th scope="col">Batch</th>
</tr><tr><td>Where used</td>
<td>From login node command line</td>
<td>In batch script</td>
</tr><tr><td>Partition</td>
<td>Requires specification of an interactive partition, such as <strong>pdebug</strong> with the <span class="fixed">-p</span> flag</td>
<td><strong>pbatch</strong> is default</td>
</tr><tr><td>Scheduling</td>
<td>If there are available interactive nodes, job will run immediately. Otherwise, it will queue up (fifo) and wait until there are enough free nodes to run it.</td>
<td>The batch scheduler handles when to run your job regardless of the number of nodes available.</td>
</tr></table><ul><li>More Examples:</li>
</ul><table class="table table-striped table-bordered"><tr><td><span class="fixed">srun -n64 -ppdebug my_app</span></td>
<td>64 process job run interactively in pdebug partition</td>
</tr><tr><td><span class="fixed">srun -N64 -n512 my_threaded_app</span></td>
<td>512 process job using 64 nodes. Assumes pbatch partition.</td>
</tr><tr><td><span class="fixed">srun -N4 -n32 -c2 my_threaded_app</span></td>
<td>4 node, 32 process job with 2 cores (threads) per process. Assumes pbatch partition.</td>
</tr><tr><td><span class="fixed">srun -N8 my_app</span></td>
<td>8 node job with a default value of one task per node (8 tasks). Assumes pbatch partition.</td>
</tr><tr><td><span class="fixed">srun -n128 -o my_app.out my_app</span></td>
<td>128 process job that redirects stdout to file my_app.out. Assumes pbatch partition.</td>
</tr><tr><td><span class="fixed">srun -n32 -ppdebug -i my.inp my_app</span></td>
<td>32 process interactive job; each process accepts input from a file called my.inp instead of stdin</td>
</tr></table><ul><li>Behavior of <span class="fixed">srun</span> <strong>-N</strong> and<strong> -n</strong> flags - using 4 nodes in batch, each of which has 16 cores:</li>
</ul><p></p><div class="media media-element-container media-default"><div id="file-958" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/srunnn-gif">srunNn.gif</a></h2>
    
  
  <div class="content">
    <img alt="srun core behavior" height="288" width="810" class="media-element file-default" data-delta="88" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/srunNn.gif" /></div>

  
</div>
</div>
<h4>srun Options</h4>
<ul><li><span class="fixed">srun</span> is a powerful command with @100 options affecting a wide range of job parameters.</li>
<li>For example:
<ul><li>Accounting</li>
<li>Number and placement of processes/threads</li>
<li>Process/thread binding</li>
<li>Job resource requirements; dependencies</li>
<li>Mail notification options</li>
<li>Input, output options</li>
<li>Time limits</li>
<li>Checkpoint, restart options</li>
<li>and much more....</li>
</ul></li>
<li>Some <span class="fixed">srun</span> options may be set via @60 SLURM environment variables. For example, SLURM_NNODES behaves like the <span class="fixed">-N</span> option.</li>
<li>See the <a href="https://computing.llnl.gov/tutorials/moab/man/srun.txt" target="_blank">srun man</a> page for details.</li>
</ul><h3><a name="interacting" id="interacting"></a>Interacting with Jobs</h3>
<p><span class="note-red">Note:</span>This section only provides a quick summary of batch usage on LC's Linux clusters. For details, see the <a href="/training/tutorials/slurm-and-moab" target="_blank">Slurm and Moab tutorial</a>. For CORAL/Sierra systems see the <a href="/training/tutorials/using-lcs-sierra-system">Sierra tutorial</a>.</p>
<h4>Monitoring Jobs and Displaying Job Information</h4>
<ul><li>There are several different job monitoring commands. Some are based on Moab, some on Slurm, and some on other sources.</li>
<li>The more commonly used job monitoring commands are summarized in the table below with links to additional information and examples.</li>
</ul><table class="table table-bordered table-striped"><tr><th scope="col">Command</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">squeue</span></td>
<td>Displays one line of information per job by default. Numerous options.</td>
</tr><tr><td><span class="fixed">showq</span></td>
<td>Displays one line of information per job. Similar to squeue. Several options.</td>
</tr><tr><td><span class="fixed">mdiag -j</span></td>
<td>Displays one line of information per job. Similar to squeue.</td>
</tr><tr><td><span class="fixed">mjstat</span></td>
<td>Summarizes queue usage and displays one line of information for active jobs.</td>
</tr><tr><td><span class="fixed">checkjob jobid</span></td>
<td>Provides detailed information about a specific job.</td>
</tr><tr><td><span class="fixed">sprio -l<br />mdiag -p -v</span></td>
<td>Displays a list of queued jobs, their priority, and the primary factors used to calculate job priority.</td>
</tr><tr><td><span class="fixed">sview</span></td>
<td>Provides a graphical view of a cluster and all job information.</td>
</tr><tr><td><span class="fixed">sinfo</span></td>
<td>Displays state information about a cluster's queues and nodes</td>
</tr></table><h4>Holding / Releasing Jobs</h4>
<ul><li>Jobs can be placed "on hold" when they are submitted.</li>
<li>They can also be placed on hold while they are waiting to run.</li>
<li>Held jobs can then be released to run later</li>
<li>More information/examples: see <a href="https://computing.llnl.gov/tutorials/moab/index.html#Hold/Release">Holding and Releasing Jobs</a> in the Slurm and Moab tutorial.</li>
</ul><table class="table table-bordered table-striped"><tr><th scope="col">Command</th>
<th scope="col">Description</th>
</tr><tr><td><span class="fixed">sbatch -H <em>jobscript</em><br />msub -h <em>jobscript</em></span></td>
<td>Put job on hold when it is submitted</td>
</tr><tr><td><span class="fixed">scontrol hole <em>jobid</em><br />mjobctl -h <em>jobid</em></span></td>
<td>Place a specific idle <em>jobid</em> on hold</td>
</tr><tr><td><span class="fixed">scontrol release <em>jobid</em><br />mjobctl -u <em>jobid</em></span></td>
<td>Release a specific held <em>jobid</em></td>
</tr></table><h4>Modifying Jobs</h4>
<ul><li>After a job has been submitted, certain attributes may be modified using the <span class="fixed">scontrol update</span> and <span class="fixed">mjobctl -m</span> commands.</li>
<li>Examples of parameters that can be changed: account, queue, job name, wall clock limit...</li>
<li>More information/examples: see <a href="https://computing.llnl.gov/tutorials/moab/index.html#ChangingParameters">Changing Job Parameters</a> in the Slurm and Moab tutorial.</li>
</ul><h4>Terminating / Canceling Jobs</h4>
<ul><li>Interactive <span class="fixed">srun</span> jobs launched from the command line should normally be terminated with a SIGINT (CTRL-C):
<ul><li>The first CTRL-C will report the state of the tasks</li>
<li>A second CTRL-C within one second will terminate the tasks</li>
</ul></li>
<li>For batch jobs, the <span class="fixed">mjobctl -c</span> and <span class="fixed">canceljob</span> commands can be used.</li>
<li>More information/examples: see <a href="https://computing.llnl.gov/tutorials/moab/index.html#Cancel">Canceling Jobs</a> in the Slurm and Moab tutorial.</li>
</ul><h3><a name="other-topics" id="other-topics"></a>Other Topics of Interest</h3>
<h4>Optimizing Core Usage</h4>
<ul><li>Fully utilizing the cores on a node requires that you use the right combination of <span class="fixed">srun</span> and Slurm/Moab options, depending upon what you want to do and which type of machine you are using.</li>
<li><strong>MPI only:</strong> for example, if you are running on a cluster that has 36 cores per node, and you want your job to use all 36 cores on 4 nodes (36 MPI tasks per node), then you would do something like:</li>
</ul><table class="table table-bordered table-striped"><tr><th scope="col">Interactive</th>
<th scope="col">Batch</th>
</tr><tr><td><span class="fixed">srun -n144 -ppdebug a.out</span></td>
<td><span class="fixed">#SBATCH -N 4<br />srun -n144 a.out</span></td>
</tr></table><ul><li><strong>MPI with Threads:</strong> If your MPI job uses POSIX or OpenMP threads within each node, you will need to calculate how many cores will be required in addition to the number of tasks. For example, running on a cluster having 16 cores per node, an 8-task job where each task creates 4 OpenMP threads, would need a total of 32 cores, or 2 nodes:</li>
</ul><pre>8 tasks * 4 threads / 16 cores/node = 2 nodes</pre><table class="table table-bordered table-striped"><tr><th scope="col">Interactive</th>
<th scope="col">Batch</th>
</tr><tr><td><span class="fixed">srun -N4 -n8 -ppdebug a.out</span></td>
<td><span class="fixed">#SBATCH -N 4<br />srun -N4 -n8 a.out</span></td>
</tr></table><ul><li>You can include multiple <span class="fixed">srun</span> commands within your batch job command script. For example, suppose that you were conducting a scalability run on a 36 core/node Linux cluster. You could allocate the maximum number of nodes that you would use with <span class="fixed">#SBATCH -N</span> and then have a series of srun commands that use varying numbers of nodes:</li>
</ul><pre>#SBATCH -N 8

srun -N1 -n36 myjob
srun -N2 -n72 myjob
srun -N3 -n108 myjob
....
srun -N8 -n228 myjob</pre><h4>Diskless Nodes</h4>
<ul><li>Because LC's Linux clusters employ a 64-bit architecture, 16 exabytes of memory can be addressed - which is about 4 billion times more than 4 GB limit of 32-bit architectures. By current standards, this is virtually unlimited memory.</li>
<li>In reality, machines are usually configured with only GBs of memory, so any address access that exceeds physical memory will result (on most systems) with paging and degraded performance.</li>
<li><em><strong>However, LC machines are diskless and have no paging space.</strong></em></li>
<li>This has very important implications for programs that exceed physical memory. For example, most compute nodes have only 16-64 GB of physical memory.</li>
<li>Programs that exceed physical memory will terminate with an OOM (out of memory) error and/or segmentation fault.</li>
</ul><h4>Process/Thread Binding to Cores</h4>
<ul><li>By default, jobs run on LC's Linux clusters have their processes bound to the available cores on a node. For example:</li>
</ul><pre>% <span class="text-danger">srun -l -n2 numactl -s | grep physc | sort</span>
0: physcpubind: 0 1 2 3 4 5 6 7
1: physcpubind: 8 9 10 11 12 13 14 15

% <span class="text-danger">srun -l -n4 numactl -s | grep physc | sort</span>
0: physcpubind: 0 1 2 3
1: physcpubind: 4 5 6 7
2: physcpubind: 8 9 10 11
3: physcpubind: 12 13 14 15

% <span class="text-danger">srun -l -n8 numactl -s | grep physc |sort</span>
0: physcpubind: 0 1
1: physcpubind: 2 3
2: physcpubind: 4 5
3: physcpubind: 6 7
4: physcpubind: 8 9
5: physcpubind: 10 11
6: physcpubind: 12 13
7: physcpubind: 14 15</pre><ul><li>Binding processes to cores may improve performance by keeping cache local to cores.</li>
<li>If a process is multi-threaded (such as with OpenMP), the threads will run on any of the cores bound to their process.</li>
<li>To bind an OpenMP thread to a single core, the <strong>OMP_PROC_BIND</strong> environment variable can be used (set to "TRUE").</li>
<li>Additionally, LC provides a couple useful utilities for binding processes and threads to cores:
<ul><li><strong>mpibind</strong> - automatically binds processes and threads to cores. Documentation at <a href="https://lc.llnl.gov/confluence/display/TLCC2/mpibind" target="_blank">lc.llnl.gov/confluence/display/TLCC2/mpibind</a> (requires authentication)</li>
<li><strong>mpifind</strong> - reports on how processes and threads are bound to cores. Documentation at <a href="https://lc.llnl.gov/confluence/display/TLCC2/mpifind" target="_blank">lc.llnl.gov/confluence/display/TLCC2/mpifind</a> (requires authentication)</li>
</ul></li>
</ul><h4>Vectorization</h4>
<ul><li>Historically, the Xeon architecture has provided support for SIMD (Single Instruction Multiple Data) vector instructions through Intel's Streaming SIMD Extensions (SSE, SSE2, SSE3, SSE4) instruction sets.</li>
<li>AVX - Advanced Vector Extension instruction set (2011) improved on SSE instructions by increasing the size of the vector registers from 128-bits to 256-bits. AVX2 (2013) offered further improvements, such as fused multiply-add (FMA) instructions to double FLOPS.</li>
<li>The primary purpose of these instructions is to increase CPU throughput by performing operations on vectors of data elements, rather than on single data elements. For example:</li>
</ul><div class="float-left"><div class="media media-element-container media-default"><div id="file-961" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/simd2-gif-1">simd2.gif</a></h2>
    
  
  <div class="content">
    <img alt="SIMD" height="147" width="400" class="media-element file-default" data-delta="89" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/simd2_2.gif" /></div>

  
</div>
</div></div>
<table class="table table-striped table-bordered"><tr><th scope="col">Instruction Set</th>
<th scope="col">Single-precision<br />Flops/Clock</th>
<th scope="col">Double-precision<br />Flops/Clock</th>
</tr><tr><td>SSE4</td>
<td>8</td>
<td>4</td>
</tr><tr><td>AVX</td>
<td>16</td>
<td>8</td>
</tr><tr><td>AVX2</td>
<td>32</td>
<td>16</td>
</tr></table><ul><li>Sandy Bridge-EP (TLCC2) processors support SSE and AVX instructions.</li>
<li>Broadwell (CTS-1) processors support SSE, AVX and AVX2.</li>
<li>To take advantage of the potential performance improvements offered by vectorization, all you need to do is compile with the appropriate compiler flags. Some recommendations are shown in the table below.</li>
</ul><table class="table table-striped table-bordered"><tr><th scope="row">Compiler</th>
<th scope="col">SSE Flag</th>
<th scope="col">AVX Flag</th>
<th scope="col">Reporting</th>
</tr><tr><th scope="row">Intel</th>
<td><span class="fixed">-vec</span> (default)</td>
<td><span class="fixed">-axAVX<br />-axCORE-AVX2</span></td>
<td><span class="fixed">-qopt_report</span></td>
</tr><tr><th scope="row">PGI</th>
<td><span class="fixed">-Mvect=simd:128</span></td>
<td><span class="fixed">-Mvect=simd:256</span></td>
<td><span class="fixed">-Minfo=all</span></td>
</tr><tr><th scope="row">GNU</th>
<td><span class="fixed">-O3</span></td>
<td><span class="fixed">-mavx</span><br /><span class="fixed">-mavx2</span></td>
<td><span class="fixed">-fopt-info-all</span></td>
</tr></table><ul><li>For additional information see the <a href="https://computing.llnl.gov/tutorials/linux_clusters/index.html#Misc">Vectorization section</a> of the Linux Clusters tutorial.</li>
</ul><h4>Hyper-threading</h4>
<ul><li>On Intel processors, hyper-threading enables 2 hardware threads per core.</li>
<li>Hyper-threading benefits some codes more than others. Tests performed on some LC codes (pF3D, IMC, Ares) showed improvements in the 10-30% range. Your mileage may vary.</li>
<li>On TOSS 3 systems, hyper-threading is turned on by default. Details are available at: <a href="https://lc.llnl.gov/confluence/display/TCE/Hyper-Threading" target="_blank">lc.llnl.gov/confluence/display/TCE/Hyper-Threading</a> (authentication required).</li>
</ul><h4>Clusters Without an Interconnect (serial and single-node jobs)</h4>
<ul><li>The <strong>agate</strong>, <strong>borax</strong> and <strong>rztrona</strong> clusters fall into this category.</li>
<li>Limited to serial and single-node parallel jobs.</li>
<li>Multiple users and jobs can run on a single node (shared node).</li>
<li>Jobs are allocated ONE core by default. For this reason, it is <em><strong>very</strong></em> important that you tell the scheduler how many cores your job actually requires:
<ul><li>For batch jobs, be sure to include in your job script:
<ul><li>Slurm: <span class="fixed">#SBATCH -n #cores</span></li>
<li>Moab: <span class="fixed">#MSUB -l ttc=#cores</span></li>
</ul></li>
<li>For interactive jobs, be sure to use the <span class="fixed">srun -c</span> flag.</li>
</ul></li>
<li>MPI jobs: these are started with the srun command as described previously, but because there is no interconnect, jobs are limited to a single node and communications are done in shared memory.</li>
<li>Pthreads and OpenMP jobs can be run as usual - keeping in mind the required number of cores per node, and that a node may be shared with other users.</li>
<li><strong>Important:</strong> Please see theÂ <a href="https://computing.llnl.gov/tutorials/moab/index.html#Serial">Running on Serial Clusters</a>Â section of the Moab and SLURM tutorial for further discussion on running jobs on these clusters.</li>
</ul><h2><a name="batch-systems" id="batch-systems"></a>Batch Systems</h2>
<ul><li>The batch system used depends upon the cluster:
<ul><li>Linux TOSS 3 clusters: Run Slurm stand-alone with Moab wrappers</li>
<li>CORAL Early Access clusters: Run Spectrum LSF with no wrappers</li>
</ul></li>
<li>Because there is so much to discuss, we won't say much more here.</li>
<li>The Slurm and Moab batch systems are covered in-depth in LC's <a href="/training/tutorials/slurm-and-moab">Slurm and Moab tutorial</a> and <a href="/banks-jobs/running-jobs">Running Jobs</a>.</li>
<li>Spectrum LSF is covered under <a href="/banks-jobs/running-jobs">Running Jobs</a>.</li>
</ul><h2><a name="misc-topics" id="misc-topics"></a>Miscellaneous Topics</h2>
<h3><a name="clusters-gpus" id="clusters-gpus"></a>Clusters with GPUs</h3>
<ul><li>Several LC Linux clusters have GPUs:
<ul><li>Max (SCF)</li>
<li>Pascal, Surface (CZ)</li>
<li>RZHasgpu (RZ)</li>
</ul></li>
<li>Additionally, the Sierra and CORAL Early Access clusters have GPUs:
<ul><li>Sierra, Shark (SCF)</li>
<li>Lassen, Ray (CZ)</li>
<li>RZAnsel, RZManta (RZ)</li>
</ul></li>
<li>These are discussed on the respective webpages:
<ul><li>Linux clusters: <a href="https://computing.llnl.gov/tutorials/linux_clusters/#GPU">computing.llnl.gov/tutorials/linux_clusters/#GPU</a></li>
<li>Sierra and CORAL EA clusters:Â <a href="/training/tutorials/using-lcs-sierra-system">hpc.llnl.gov/training/tutorials/using-lcs-sierra-system</a></li>
</ul></li>
</ul><h3><a name="big-data" id="big-data"></a>Big Data at LC</h3>
<ul><li>Recently "Big Data" cluster computing frameworks have become popular. There has been interest in running some frameworks, such as Hadoop and Spark, on LC clusters.</li>
<li>LC clusters differ from most "Big Data" cluster architectures:
<ul><li>LC uses a different scheduler/resource manager (Moab/Slurm)</li>
<li>LC uses a networked file system (Lustre)</li>
<li>Most LC systems have no local disk</li>
</ul></li>
<li>LC provides Magpie, a set of scripts to provide a framework to run several popular "Big Data" frameworks on LC systems.
<ul><li>Setup a Hadoop, Spark, etc. environment to run Big Data jobs</li>
<li>Work with LC file systems and scheduler/resource manager</li>
<li>Hide a number of LC-isms from users</li>
<li>Provide secure mechanisms to run within LC environment</li>
<li>Magpie instructions and information can be found on LC Confluence: <a href="https://lc.llnl.gov/confluence/display/BigData/Magpie+Guide" target="_blank">lc.llnl.gov/confluence/display/BigData/Magpie+Guide</a></li>
</ul></li>
<li>Catalyst is a special LC cluster dedicated to Big Data research and testing.
<ul><li>324 node Intel Xeon E5-2695 v2 nodes; compute nodes have 24 cores/node</li>
<li>Compute nodes have 800 NVRAM SSD storage</li>
<li>Catalyst information available at: <a href="https://lc.llnl.gov/confluence/display/BigData/Catalyst+Information+and+Documentation" target="_blank">lc.llnl.gov/confluence/display/BigData/Catalyst+Information+and+Documentation</a></li>
</ul></li>
</ul><h3><a name="gdo" id="gdo"></a>Green Data Oasis (GDO)</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-962" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/gdologo-gif">gdoLogo.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/gdoLogo.gif"><img alt="Green Data Oasis screenshot" height="31" width="250" style="width: 250px; height: 31px;" class="media-element file-default" data-delta="90" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/gdoLogo-250x31.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>Green Data Oasis (GDO) is a large data store (1.4 PB as of Nov 2019) on the LLNL open network.</li>
<li>Purpose:
<ul><li>Facilitate the sharing of scientific data with external collaborators by providing an easy way to share data</li>
<li>Designed as a data portal and is not intended for doing data analysis or other CPU-intensive activities.</li>
</ul></li>
<li>LC managed program for the Multiprogrammatic and Institutional Computing (M&amp;IC) Initiative and Laboratory Science and Technology Office (LSTO).</li>
<li>More information: <a href="/data-vis/green-data-oasis">hpc.llnl.gov/data-vis/green-data-oasis</a></li>
</ul><h3><a name="security" id="security"></a>Security Reminders</h3>
<ul><li>Follow procedures when escorting into limited areas</li>
<li>Restrictions on cell phones, cameras, lap-tops and other items</li>
<li>Separation between classified and unclassified equipment, including media</li>
<li>Never share your password/account with anyone (duh!!). SCF passwords should be regarded as Secret Restricted Data (SRD)</li>
<li>Keep classified information off unclassified systems - including email</li>
<li>Ask an Authorized Derivative Classifier (ADC) if you're not sure</li>
<li>LC can assist with the disposal of classified data</li>
<li>For the full story, visit the LLNL security web page located at:Â <a href="https://security-r.llnl.gov/" target="_blank">security-r.llnl.gov</a>Â (LLNL internal)</li>
</ul><h2><a name="help" id="help"></a>Where to Get Information &amp; Help</h2>
<h3><a name="hotline" id="hotline"></a>LC Hotline</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-1842" class="file file-image file-image-jpeg">

        <h2 class="element-invisible"><a href="/files/hotlinestaff2-jpg">hotlineStaff2.jpg</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/hotlineStaff2.jpg"><img alt="LC team standing in front of Sierra" height="197" width="300" style="width: 300px; height: 197px;" class="media-element file-default" data-delta="134" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/hotlineStaff2-300x197.jpg" /></a>  </div>

  
</div>
</div></div>
<ul><li>The LC Hotline staff provide walk-in, phone and email assistance weekdays 8:00am - noon, 1:00pm - 4:45pm.</li>
<li>Walk-in Consulting:
<ul><li>On-site users can visit the LC help desk consultants in Building 453, Room 1103.</li>
<li>Note that this is a Q-clearance area.</li>
<li><a href="/files/llnlmap2gif">Need a map?</a></li>
</ul></li>
<li>Phone:
<ul><li>(925) 422-4531 - Main number</li>
<li>422-4532 - Direct phone line for technical consulting help</li>
<li>422-4533 - Direct phone line for support help (accounts, passwords, forms, etc)</li>
</ul></li>
<li>Email:
<ul><li>Technical Help: <a href="mailto:lc-hotline@llnl.gov">lc-hotline@llnl.gov</a></li>
<li>Accounts &amp; Passwords: <a href="mailto:lc-support@llnl.gov">lc-support@llnl.gov</a></li>
</ul></li>
</ul><div class="clear-floats">Â </div>
<h3><a name="user-home" id="user-home"></a>LC Users Home Page: hpc.llnl.gov</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-964" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/lcwebpage2-gif">LCwebpage2.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/LCwebpage2.gif"><img alt="LC Webpage" height="231" width="300" style="height: 231px; width: 300px;" class="media-element file-default" data-delta="92" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/LCwebpage2-300x231.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li><a href="https://hpc.llnl.gov">hpc.llnl.gov</a>: LC maintains extensive web documentation for all systems and also for computing in the LC environment:</li>
<li>A few highlights:
<ul><li><strong>Accounts</strong> - how to request an account; forms</li>
<li><strong>Access Information</strong> - how to access and login to LC systems</li>
<li><strong>Training</strong> - online tutorials and workshops</li>
<li><strong>Compute Platforms</strong> - complete list with details for all LC systems</li>
<li><strong>Machine Status</strong> - shows current OCF machines status with links to detailed information such as MOTD, currently running jobs, configuration, announcements, etc.</li>
<li><strong>Software</strong> - including compilers, tools, debuggers, visualization, math libs</li>
<li><strong>Running Jobs</strong> - including using LC's workload managers</li>
<li><strong>Documentation</strong> - user manuals for a range of topics, technical bulletins, user meetings slides</li>
<li><strong>Getting Help</strong> - how to contact the LC Hotline</li>
</ul></li>
<li>Some web pages are password protected. If prompted to enter a userid/password, use your OTP login.</li>
<li>Some web pages may only be accessed from LLNL machines or by using one of the LC <a href="#remote-access">Remote Access Services</a> covered previously.</li>
</ul><h3><a name="user-dash" id="user-dash"></a>Lorenz User Dashboard: mylc.llnl.gov</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-965" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/lorenz1-gif">lorenz1.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/lorenz1.gif"><img alt="MyLC dashboard" height="259" width="300" style="height: 259px; width: 300px;" class="media-element file-default" data-delta="93" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/lorenz1-300x259.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>Provides a wealth of real-time information in a user-friendly dashboard</li>
<li>Simply enter "mylc" into your browser's address bar. The actual URL is: <a href="https://lc.llnl.gov/lorenz/mylc/mylc.cgi" target="_blank">lc.llnl.gov/lorenz/mylc/mylc.cgi</a></li>
<li>Click on the screenshot at right to see a larger version. Note: If your browser squeezes the large image into a single window, try zooming to get more detail.</li>
</ul><h3><a name="login-banner" id="login-banner"></a>Login Banner</h3>
<ul><li>Login banner / MOTD may be very important!
<ul><li>News topics for LC, for the login system</li>
<li>Some configuration information</li>
<li>Useful references and contact information</li>
<li>System status information</li>
<li>Quota and password expiration warnings also appear when you login</li>
</ul></li>
</ul><h3><a name="news-items" id="news-items"></a>News Items</h3>
<div class="float-right"><div class="media media-element-container media-default"><div id="file-966" class="file file-image file-image-gif">

        <h2 class="element-invisible"><a href="/files/motd-gif">MOTD.gif</a></h2>
    
  
  <div class="content">
    <a href="https://hpc.llnl.gov/sites/default/files/MOTD.gif"><img alt="System login messages" height="386" width="300" style="height: 386px; width: 300px;" class="media-element file-default" data-delta="94" typeof="foaf:Image" src="https://hpc.llnl.gov/sites/default/files/resize/MOTD-300x386.gif" /></a>  </div>

  
</div>
</div></div>
<ul><li>News postings on each LC System:
<ul><li>Unread news items appear with login messages</li>
<li><span class="fixed">news -l</span> - list all news items</li>
<li><span class="fixed">news -a</span> - display content of all news messages</li>
<li><span class="fixed">news -n</span> - lists unread messages</li>
<li><span class="fixed">news -s</span> - shows number of unread items</li>
<li><span class="fixed">news <em>item</em></span><em> </em>- shows specified news item</li>
<li>You can also list/read the files in /var/news on any system. This is useful when your searching for a topic you've already read and can't remember the news item name. You can also "grep" on these files.</li>
</ul></li>
</ul><ul><li>Also accessible from <a href="https://hpc.llnl.gov/">hpc.llnl.gov</a> and <a href="https://lc.llnl.gov/lorenz/mylc/mylc.cgi" target="_blank">Lorenz</a>.</li>
</ul><h3><a name="email-list" id="email-list"></a>Machine Email Lists</h3>
<ul><li>Machine status email lists exist for all LC machines</li>
<li>Provide important, timely information not necessarily announced elsewhere</li>
<li><a href="mailto:ocf-status@llnl.gov">ocf-status@llnl.gov</a> and <a href="mailto:scf-status@llnl.gov">scf-status@llnl.gov</a> are general lists for all users</li>
<li>Plus each machine has its own list, for example: <a href="mailto:zin-status@llnl.gov">zin-status@llnl.gov</a>.</li>
<li>The LC Hotline initially populates a list with subscribers, but you can subscribe/unsubscribe yourself anytime using the <a href="https://listserv.llnl.gov/" target="_blank">listserv.llnl.gov</a> website.</li>
</ul><h3><a name="user-meeting" id="user-meeting"></a>LC User Meetings</h3>
<ul><li>When held, is usually scheduled for the first Tuesday of the month at 9:30 am</li>
<li>Building 132 Auditorium (or as otherwise announced)</li>
<li>Agenda and viewgraphs on LC Home Page (<a href="https://hpc.llnl.gov/">hpc.llnl.gov</a>) See "Documentation" and look for "User Meeting Viewgraphs". Note that these are LLNL internal web pages.</li>
</ul><h2><a name="exercise-2" id="exercise-2"></a>Exercise 2</h2>
<p>Compiling, running, job and system status information:</p>
<ul><li>Get information about running and queued jobs</li>
<li>Get compiler information</li>
<li>Compile and run serial programs</li>
<li>Compile and run parallel MPI and OpenMP programs, both interactively and in batch</li>
<li>Check hyper-threading</li>
<li>Get online system status information (and more)</li>
</ul><h4>Â </h4>
</div>
<p>Â </p>
</div></div></div>    </div>
  </div>
</div>


<!-- Needed to activate display suite support on forms -->
  </div>
  
</div> <!-- /.block --></div>
 <!-- /.region -->
                   		</div>
                  </main>
                </div>
      		</div>
    	</div>
	</div>
  	
	

    <footer id="colophon" class="site-footer">
        <div class="container">
            <div class="row">
                <div class="col-sm-12 footer-top">

                    <a class="llnl" href="https://www.llnl.gov/" target="_blank"><img src="/sites/all/themes/tid/images/llnl.png" alt="LLNL"></a>
                    <p>
                        Lawrence Livermore National Laboratory
                        <br>7000 East Avenue â€¢ Livermore, CA 94550
                    </p>
                    <p>
                        Operated by Lawrence Livermore National Security, LLC, for the
                        <br>Department of Energy's National Nuclear Security Administration.
                    </p>
                    <div class="footer-top-logos">
                        <a class="nnsa" href="https://www.energy.gov/nnsa/national-nuclear-security-administration" target="_blank"><img src="/sites/all/themes/tid/images/nnsa2.png" alt="NNSA"></a>
                        <a class="doe" href="https://www.energy.gov/" target="_blank"><img src="/sites/all/themes/tid/images/doe_small.png" alt="U.S. DOE"></a>
                        <a class="llns" href="https://www.llnsllc.com/" target="_blank"><img src="/sites/all/themes/tid/images/llns.png" alt="LLNS"></a>
                	</div>



                </div>
                <div class="col-sm-12 footer-bottom">
                	

                    <span>UCRL-MI-131558  &nbsp;|&nbsp;&nbsp;</span><a href="https://www.llnl.gov/disclaimer" target="_blank">Privacy &amp; Legal Notice</a>	 &nbsp;|&nbsp;&nbsp; <a href="mailto:webmaster-comp@llnl.gov">Website Query</a> &nbsp;|&nbsp;&nbsp;<a href="/about-us/contact-us" >Contact Us</a>
                </div>
            </div>
        </div>
    </footer>
</div>
  </body>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/jquery_update/replace/jquery/2.1/jquery.min.js?v=2.1.4"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/jquery-extend-3.4.0.js?v=2.1.4"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/jquery-html-prefilter-3.5.0-backport.js?v=2.1.4"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/jquery.once.js?v=1.2"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/misc/drupal.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/extlink/extlink.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/jquery.flexslider.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/slide.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/lightbox2/js/lightbox.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/modules/contrib/matomo/matomo.js?qsohrw"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
var _paq = _paq || [];(function(){var u=(("https:" == document.location.protocol) ? "https://analytics.llnl.gov/" : "http://analytics.llnl.gov/");_paq.push(["setSiteId", "149"]);_paq.push(["setTrackerUrl", u+"piwik.php"]);_paq.push(["setDoNotTrack", 1]);_paq.push(["trackPageView"]);_paq.push(["setIgnoreClasses", ["no-tracking","colorbox"]]);_paq.push(["enableLinkTracking"]);var d=document,g=d.createElement("script"),s=d.getElementsByTagName("script")[0];g.type="text/javascript";g.defer=true;g.async=true;g.src="https://hpc.llnl.gov/sites/default/files/matomo/piwik.js?qsohrw";s.parentNode.insertBefore(g,s);})();
//--><!]]>
</script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/bootstrap.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/mobilemenu.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/custom.js?qsohrw"></script>
<script type="text/javascript" src="https://hpc.llnl.gov/sites/all/themes/tid/js/mods.js?qsohrw"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"tid","theme_token":"VZbPFE7kouPetih6Bd21vNqdh34UtSbd3v5C5eVEeE4","js":{"sites\/all\/modules\/contrib\/jquery_update\/replace\/jquery\/2.1\/jquery.min.js":1,"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/modules\/contrib\/extlink\/extlink.js":1,"sites\/all\/themes\/tid\/js\/jquery.flexslider.js":1,"sites\/all\/themes\/tid\/js\/slide.js":1,"sites\/all\/modules\/contrib\/lightbox2\/js\/lightbox.js":1,"sites\/all\/modules\/contrib\/matomo\/matomo.js":1,"0":1,"sites\/all\/themes\/tid\/js\/bootstrap.js":1,"sites\/all\/themes\/tid\/js\/mobilemenu.js":1,"sites\/all\/themes\/tid\/js\/custom.js":1,"sites\/all\/themes\/tid\/js\/mods.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/system\/system.theme.css":1,"modules\/book\/book.css":1,"sites\/all\/modules\/contrib\/date\/date_api\/date.css":1,"sites\/all\/modules\/contrib\/date\/date_popup\/themes\/datepicker.1.7.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/contrib\/extlink\/extlink.css":1,"sites\/all\/modules\/contrib\/views\/css\/views.css":1,"sites\/all\/modules\/contrib\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/contrib\/lightbox2\/css\/lightbox.css":1,"sites\/all\/modules\/contrib\/print\/print_ui\/css\/print_ui.theme.css":1,"sites\/all\/themes\/tid\/css\/bootstrap.css":1,"sites\/all\/themes\/tid\/css\/flexslider.css":1,"sites\/all\/themes\/tid\/css\/system.menus.css":1,"sites\/all\/themes\/tid\/css\/style.css":1,"sites\/all\/themes\/tid\/font-awesome\/css\/font-awesome.css":1,"sites\/all\/themes\/tid\/css\/treewalk.css":1,"sites\/all\/themes\/tid\/css\/popup.css":1,"sites\/all\/themes\/tid\/css\/mods.css":1}},"lightbox2":{"rtl":0,"file_path":"\/(\\w\\w\/)public:\/","default_image":"\/sites\/all\/modules\/contrib\/lightbox2\/images\/brokenimage.jpg","border_size":10,"font_color":"000","box_color":"fff","top_position":"","overlay_opacity":"0.8","overlay_color":"000","disable_close_click":true,"resize_sequence":0,"resize_speed":400,"fade_in_speed":400,"slide_down_speed":600,"use_alt_layout":false,"disable_resize":false,"disable_zoom":false,"force_show_nav":false,"show_caption":true,"loop_items":false,"node_link_text":"View Image Details","node_link_target":false,"image_count":"Image !current of !total","video_count":"Video !current of !total","page_count":"Page !current of !total","lite_press_x_close":"press \u003Ca href=\u0022#\u0022 onclick=\u0022hideLightbox(); return FALSE;\u0022\u003E\u003Ckbd\u003Ex\u003C\/kbd\u003E\u003C\/a\u003E to close","download_link_text":"","enable_login":false,"enable_contact":false,"keys_close":"c x 27","keys_previous":"p 37","keys_next":"n 39","keys_zoom":"z","keys_play_pause":"32","display_image_size":"original","image_node_sizes":"()","trigger_lightbox_classes":"","trigger_lightbox_group_classes":"","trigger_slideshow_classes":"","trigger_lightframe_classes":"","trigger_lightframe_group_classes":"","custom_class_handler":0,"custom_trigger_classes":"","disable_for_gallery_lists":true,"disable_for_acidfree_gallery_lists":true,"enable_acidfree_videos":true,"slideshow_interval":5000,"slideshow_automatic_start":true,"slideshow_automatic_exit":true,"show_play_pause":true,"pause_on_next_click":false,"pause_on_previous_click":true,"loop_slides":false,"iframe_width":600,"iframe_height":400,"iframe_border":1,"enable_video":false,"useragent":"Mozilla\/5.0 (Windows NT 10.0; Win64; x64; rv:87.0;secssobrowser) Gecko\/20100101 Firefox\/87.0"},"extlink":{"extTarget":0,"extClass":"ext","extLabel":"(link is external)","extImgClass":0,"extIconPlacement":0,"extSubdomains":1,"extExclude":".gov|.com|.org|.io|.be|.us|.edu","extInclude":"-int.llnl.gov|lc.llnl.gov|caas.llnl.gov|exchangetools.llnl.gov","extCssExclude":"","extCssExplicit":"","extAlert":"_blank","extAlertText":"This page is routing you to a page which requires extra authentication. You must have on-site or VPN access.\r\n\r\nPress OK to continue or cancel to return.\r\n\r\nIf this fails or times-out, you are not allowed access to the internal page or the server may be temporarily unavailable.\r\n\r\nIf you have an on-site or VPN account and are still having trouble, please send e-mail to lc-hotline@llnl.gov or call 925-422-4531 for further assistance.","mailtoClass":"mailto","mailtoLabel":"(link sends e-mail)"},"matomo":{"trackMailto":1},"urlIsAjaxTrusted":{"\/training\/tutorials\/livermore-computing-resources-and-environment":true}});
//--><!]]>
</script>
</html>
